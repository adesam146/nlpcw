{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of NLP_CW.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "xGMVF5KTg-He",
        "t9Zt3py7E1ep"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adesam146/nlpcw/blob/rest_of_tasks_playground/NLP_CW_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "_i_qSkEMxlkg"
      },
      "cell_type": "markdown",
      "source": [
        "## Check GPU memory"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5-XwNX-831V6",
        "outputId": "52b7acf0-3d21-42f3-915f-d85036b3b79b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "#Check GPU Memory allocation\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NOXcwqriwFsu",
        "outputId": "1d5deedf-60a7-4fcd-e244-adbb74702512",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 12.9 GB  | Proc size: 143.1 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ecWOCoFgxS_j",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#run this if GPU utilization is not 0%\n",
        "# !kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "wTfeo8tcxhwC"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ePuqIHSPf554",
        "outputId": "53e81fd2-d0c0-4d66-be1a-27a4ccf2cba6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1007
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U spacy ftfy torchtext\n",
        "!python -m spacy download en\n",
        "!pip install -U textblob #Sentiment analysis\n",
        "!python -m textblob.download_corpora"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spacy in /usr/local/lib/python3.6/dist-packages (2.0.18)\n",
            "Requirement already up-to-date: ftfy in /usr/local/lib/python3.6/dist-packages (5.5.1)\n",
            "Requirement already up-to-date: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied, skipping upgrade: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.9)\n",
            "Requirement already satisfied, skipping upgrade: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied, skipping upgrade: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (6.12.1)\n",
            "Requirement already satisfied, skipping upgrade: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy) (2018.1.10)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied, skipping upgrade: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.35)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.18.4)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.16.2)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy) (0.1.7)\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.0.1.post2)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.5.6)\n",
            "Requirement already satisfied, skipping upgrade: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.4.3.2)\n",
            "Requirement already satisfied, skipping upgrade: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.9.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.10.11)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.11.29)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
            "Requirement already satisfied, skipping upgrade: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy) (0.9.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n",
            "Collecting textblob\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/f0/1d9bfcc8ee6b83472ec571406bd0dd51c0e6330ff1a51b2d29861d389e85/textblob-0.15.3-py2.py3-none-any.whl (636kB)\n",
            "\u001b[K    100% |████████████████████████████████| 645kB 23.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: nltk>=3.1 in /usr/local/lib/python3.6/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.1->textblob) (1.11.0)\n",
            "Installing collected packages: textblob\n",
            "  Found existing installation: textblob 0.15.2\n",
            "    Uninstalling textblob-0.15.2:\n",
            "      Successfully uninstalled textblob-0.15.2\n",
            "Successfully installed textblob-0.15.3\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Srpq8hYt4whg",
        "outputId": "42738691-e29b-474c-b54f-32f5252a9788",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data import sampler\n",
        "from torch import autograd\n",
        "import spacy\n",
        "from torchtext import data\n",
        "from torchtext import datasets as nlp_dset\n",
        "import random\n",
        "from sklearn.utils import resample\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "\n",
        "import torchvision.transforms as T\n",
        "\n",
        "nlp_spaCy = spacy.load('en', disable=[\"tagger\", \"parser\", \"ner\"])\n",
        "\n",
        "#stopwords\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stops_nltk = list(stopwords.words('english'))\n",
        "stops_sklearn = list(ENGLISH_STOP_WORDS)\n",
        "STOPWORDS = list(set(stops_nltk + stops_sklearn))\n",
        "\n",
        "#GPU\n",
        "GPU = True\n",
        "device_idx = 0\n",
        "if GPU:\n",
        "    device = torch.device(\"cuda:\"+str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "#Fix all seeds\n",
        "SEED = 0\n",
        "\n",
        "def set_seed(seed=SEED):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "set_seed()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Using device: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qtiwRhtm3s87",
        "outputId": "36977689-e9e1-487f-e4b7-9a73c10d658f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "# Load datafiles from own google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "train_fp = \"\"\"/content/drive/My Drive/colab_data/offenseval-training-v1.tsv\"\"\"\n",
        "train_df = pd.read_csv(train_fp, delimiter=\"\\t\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "xGMVF5KTg-He"
      },
      "cell_type": "markdown",
      "source": [
        "## ELMO"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wxCJbS2h4jfG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3329
        },
        "outputId": "b4c39fee-9722-402b-fc59-14e3c8411146"
      },
      "cell_type": "code",
      "source": [
        "!pip install torchvision torch allennlp\n",
        "from allennlp.modules.elmo import Elmo, batch_to_ids"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1.post2)\n",
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/32/d6d0a93a23763f366df2dbd4e007e45ce4d2ad97e6315506db9da8af7731/allennlp-0.8.2-py3-none-any.whl (5.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.6MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/5e/e91792f198bbc5a0d7d3055ad552bc4062942d27eaf75c3e2783cf64eae5/Pillow-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 14.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.16.2)\n",
            "Requirement already satisfied: spacy<2.1,>=2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.0.18)\n",
            "Collecting moto==1.3.4 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/8f/7b36e81ff067d0e7bf90f7210b351c0cfe6657f79fa4dcb0cb4787462e05/moto-1.3.4-py2.py3-none-any.whl (548kB)\n",
            "\u001b[K    100% |████████████████████████████████| 552kB 24.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask==1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.0.2)\n",
            "Collecting conllu==0.11 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/d4/2c/856344d9b69baf5b374c395b4286626181a80f0c2b2f704914d18a1cea47/conllu-0.11-py2.py3-none-any.whl\n",
            "Collecting flaky (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/02/42/cca66659a786567c8af98587d66d75e7d2b6e65662f8daab75db708ac35b/flaky-3.5.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.2)\n",
            "Collecting overrides (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/de/55/3100c6d14c1ed177492fcf8f07c4a7d2d6c996c0a7fc6a9a0a41308e7eec/overrides-1.9.tar.gz\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Collecting flask-cors==3.0.7 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/65/cb/683f71ff8daa3aea0a5cbb276074de39f9ab66d3fbb8ad5efb5bb83e90d2/Flask_Cors-3.0.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.0.23)\n",
            "Collecting matplotlib==2.2.3 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/59/f235ab21bbe7b7c6570c4abf17ffb893071f4fa3b9cf557b09b60359ad9a/matplotlib-2.2.3-cp36-cp36m-manylinux1_x86_64.whl (12.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 12.6MB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlparse==0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.2.4)\n",
            "Collecting awscli>=1.11.91 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/cf/9935187b758dccb7a85ee697d3008fe9ea50c5f0e7ae56a76caec0a46954/awscli-1.16.113-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.5MB 19.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.20.2)\n",
            "Collecting numpydoc==0.8.0 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/95/a8/b4706a6270f0475541c5c1ee3373c7a3b793936ec1f517f1a1dab4f896c0/numpydoc-0.8.0.tar.gz\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.10.1)\n",
            "Collecting pytorch-pretrained-bert>=0.6.0 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/3c/d5fa084dd3a82ffc645aba78c417e6072ff48552e3301b1fa3bd711e03d4/pytorch_pretrained_bert-0.6.1-py3-none-any.whl (114kB)\n",
            "\u001b[K    100% |████████████████████████████████| 122kB 31.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (from allennlp) (5.5.1)\n",
            "Collecting pytz==2017.3 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/7f/e7d1acbd433b929168a4fb4182a2ff3c33653717195a26c1de099ad1ef29/pytz-2017.3-py2.py3-none-any.whl (511kB)\n",
            "\u001b[K    100% |████████████████████████████████| 512kB 21.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.18.4)\n",
            "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.6)\n",
            "Collecting jsonnet==0.10.0; sys_platform != \"win32\" (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/83/d49904ee98dd4fbba6a003938e30e76251951c4bdb49628b4f92e5009a42/jsonnet-0.10.0.tar.gz (124kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 34.9MB/s \n",
            "\u001b[?25hCollecting tensorboardX==1.2 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/22/43f4f0318f7c68a1000dbb700a353b745584bc2397437832d15ba69ea5f1/tensorboardX-1.2-py2.py3-none-any.whl (44kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 21.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.9.100)\n",
            "Collecting responses>=0.7 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/d8/a77cbd4cb8366ad0e275f2c642b50b401da22a2f5714e003e499fddca106/responses-0.10.5-py2.py3-none-any.whl\n",
            "Collecting gevent==1.3.6 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/3d/a19fece28ba1b5133cf74bd22a229d77b4d9cc4b24aa8f263cca2845c555/gevent-1.3.6-cp36-cp36m-manylinux1_x86_64.whl (4.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 4.5MB 8.9MB/s \n",
            "\u001b[?25hCollecting parsimonious==0.8.0 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/89/32c55944cd30dff856f16859ee325b13c83c260d0c56c0eed511e8063c87/parsimonious-0.8.0.tar.gz\n",
            "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (6.12.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (2018.1.10)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (0.2.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (2.0.2)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (1.35)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (0.9.6)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (2.0.1)\n",
            "Collecting python-jose<3.0.0 (from moto==1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/5c/5fa238c0c5b0656994b52721dd8b1d7bf52ebd8786518dde794f44de86b6/python_jose-2.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: werkzeug in /usr/local/lib/python3.6/dist-packages (from moto==1.3.4->allennlp) (0.14.1)\n",
            "Collecting cryptography>=2.0.0 (from moto==1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/71/e632e222f34632e0527dd41799f7847305e701f38f512d81bdf96009bca4/cryptography-2.5-cp34-abi3-manylinux1_x86_64.whl (2.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.4MB 11.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from moto==1.3.4->allennlp) (2.5.3)\n",
            "Collecting pyaml (from moto==1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/c5/e1/1523fb1dab744e2c6b1f02446f2139a78726c18c062a8ddd53875abb20f8/pyaml-18.11.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: boto>=2.36.0 in /usr/local/lib/python3.6/dist-packages (from moto==1.3.4->allennlp) (2.49.0)\n",
            "Requirement already satisfied: botocore>=1.9.16 in /usr/local/lib/python3.6/dist-packages (from moto==1.3.4->allennlp) (1.12.100)\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.6/dist-packages (from moto==1.3.4->allennlp) (2.0.0)\n",
            "Collecting docker>=2.5.1 (from moto==1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/3c/b610f22b170b0f8fe4d8f78974878e116562389f666f99e6549567eb9d87/docker-3.7.0-py2.py3-none-any.whl (133kB)\n",
            "\u001b[K    100% |████████████████████████████████| 143kB 24.0MB/s \n",
            "\u001b[?25hCollecting jsondiff==1.1.1 (from moto==1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/bd/5f/13e28a2f9abeda2ffb3f44f2f809b01b52bc02cdb63816e05b8c9cbbdfc5/jsondiff-1.1.1.tar.gz\n",
            "Requirement already satisfied: Jinja2>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from moto==1.3.4->allennlp) (2.10)\n",
            "Collecting aws-xray-sdk<0.96,>=0.93 (from moto==1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/a5/da7887285564f9e0ae5cd25a453cca36e2cd43d8ccc9effde260b4d80904/aws_xray_sdk-0.95-py2.py3-none-any.whl (52kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 24.1MB/s \n",
            "\u001b[?25hCollecting cookies (from moto==1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6a/60/557f84aa2db629e5124aa05408b975b1b5d0e1cec16cde0bfa06aae097d3/cookies-2.2.1-py2.py3-none-any.whl (44kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 21.2MB/s \n",
            "\u001b[?25hCollecting xmltodict (from moto==1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/28/fd/30d5c1d3ac29ce229f6bdc40bbc20b28f716e8b363140c26eff19122d8a5/xmltodict-0.12.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->allennlp) (7.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->allennlp) (1.1.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.2.3->allennlp) (1.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.2.3->allennlp) (2.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: PyYAML<=3.13,>=3.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (3.13)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (0.2.0)\n",
            "Collecting rsa<=3.5.0,>=3.1.2 (from awscli>=1.11.91->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/ae/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e/rsa-3.4.2-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 5.9MB/s \n",
            "\u001b[?25hCollecting colorama<=0.3.9,>=0.2.5 (from awscli>=1.11.91->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/db/c8/7dcf9dbcb22429512708fe3a547f8b6101c0d02137acbd892505aee57adf/colorama-0.3.9-py2.py3-none-any.whl\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (0.14)\n",
            "Requirement already satisfied: sphinx>=1.2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc==0.8.0->allennlp) (1.8.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (40.8.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (18.2.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (6.0.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.0)\n",
            "Requirement already satisfied: pluggy>=0.7 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.8.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.7)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2018.11.29)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: protobuf>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.2->allennlp) (3.6.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.3)\n",
            "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent==1.3.6->allennlp) (0.4.15)\n",
            "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy<2.1,>=2.0->allennlp) (1.10.11)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy<2.1,>=2.0->allennlp) (0.9.0.1)\n",
            "Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy<2.1,>=2.0->allennlp) (0.4.3.2)\n",
            "Requirement already satisfied: future<1.0 in /usr/local/lib/python3.6/dist-packages (from python-jose<3.0.0->moto==1.3.4->allennlp) (0.16.0)\n",
            "Collecting pycryptodome<4.0.0,>=3.3.1 (from python-jose<3.0.0->moto==1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/cf/4b66bf1ac2484ca39599b4576d681186b61b543c2d2c29f9aa4ba3cc53b5/pycryptodome-3.7.3-cp36-cp36m-manylinux1_x86_64.whl (7.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 7.5MB 4.6MB/s \n",
            "\u001b[?25hCollecting ecdsa<1.0 (from python-jose<3.0.0->moto==1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/f4/73669d51825516ce8c43b816c0a6b64cd6eb71d08b99820c00792cb42222/ecdsa-0.13-py2.py3-none-any.whl (86kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 25.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.0.0->moto==1.3.4->allennlp) (1.12.1)\n",
            "Collecting asn1crypto>=0.21.0 (from cryptography>=2.0.0->moto==1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/cd/35485615f45f30a510576f1a56d1e0a7ad7bd8ab5ed7cdc600ef7cd06222/asn1crypto-0.24.0-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K    100% |████████████████████████████████| 102kB 27.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock->moto==1.3.4->allennlp) (5.1.2)\n",
            "Collecting websocket-client>=0.32.0 (from docker>=2.5.1->moto==1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/54/684db2ba1b7a203602808446b8686ee786f93b4a7e080cdc440cc7e06e56/websocket_client-0.55.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K    100% |████████████████████████████████| 204kB 22.5MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from docker>=2.5.1->moto==1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7.3->moto==1.3.4->allennlp) (1.1.0)\n",
            "Collecting jsonpickle (from aws-xray-sdk<0.96,>=0.93->moto==1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/dc/12/8c44eabb501e2bc0aec0dd152b328074d98a50968d3a02be28f6037f0c6a/jsonpickle-1.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<=3.5.0,>=3.1.2->awscli>=1.11.91->allennlp) (0.4.5)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (1.2.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (19.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (1.1.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (2.6.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (1.1.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy<2.1,>=2.0->allennlp) (0.9.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0.0->moto==1.3.4->allennlp) (2.19)\n",
            "Building wheels for collected packages: overrides, numpydoc, jsonnet, parsimonious, jsondiff\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/8d/52/86/e5a83b1797e7d263b458d2334edd2704c78508b3eea9323718\n",
            "  Building wheel for numpydoc (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ea/55/7f/3e25d754760ccd62d6796e5b2cfe25629346f52ea00753d549\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a0/aa/f0/b4ab8854cf00f922a87787425cfbb789aac01ab2c2cd1b4ca4\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/bb/51/82/ae9b22a790f11e7be918939d01aa397c545ebb3723453c5fb4\n",
            "  Building wheel for jsondiff (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/68/08/07/69d839606fb7fdc778fa86476abc0a864693d45969a0c1936c\n",
            "Successfully built overrides numpydoc jsonnet parsimonious jsondiff\n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "\u001b[31mawscli 1.16.113 has requirement botocore==1.12.103, but you'll have botocore 1.12.100 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pycryptodome, ecdsa, python-jose, asn1crypto, cryptography, pyaml, pytz, websocket-client, docker-pycreds, docker, jsondiff, jsonpickle, aws-xray-sdk, cookies, xmltodict, responses, moto, conllu, flaky, overrides, flask-cors, matplotlib, rsa, colorama, awscli, numpydoc, pytorch-pretrained-bert, jsonnet, tensorboardX, gevent, parsimonious, allennlp, pillow\n",
            "  Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "  Found existing installation: matplotlib 3.0.2\n",
            "    Uninstalling matplotlib-3.0.2:\n",
            "      Successfully uninstalled matplotlib-3.0.2\n",
            "  Found existing installation: rsa 4.0\n",
            "    Uninstalling rsa-4.0:\n",
            "      Successfully uninstalled rsa-4.0\n",
            "  Found existing installation: gevent 1.4.0\n",
            "    Uninstalling gevent-1.4.0:\n",
            "      Successfully uninstalled gevent-1.4.0\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed allennlp-0.8.2 asn1crypto-0.24.0 aws-xray-sdk-0.95 awscli-1.16.113 colorama-0.3.9 conllu-0.11 cookies-2.2.1 cryptography-2.5 docker-3.7.0 docker-pycreds-0.4.0 ecdsa-0.13 flaky-3.5.3 flask-cors-3.0.7 gevent-1.3.6 jsondiff-1.1.1 jsonnet-0.10.0 jsonpickle-1.1 matplotlib-2.2.3 moto-1.3.4 numpydoc-0.8.0 overrides-1.9 parsimonious-0.8.0 pillow-5.4.1 pyaml-18.11.0 pycryptodome-3.7.3 python-jose-2.0.2 pytorch-pretrained-bert-0.6.1 pytz-2017.3 responses-0.10.5 rsa-3.4.2 tensorboardX-1.2 websocket-client-0.55.0 xmltodict-0.12.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "matplotlib",
                  "mpl_toolkits",
                  "pytz",
                  "rsa"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8iTnEx03ZO2q",
        "outputId": "769b64d4-6d52-4e64-b892-5fe0b5d784fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "#Use pretrained ELMO weights. \n",
        "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
        "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
        "\n",
        "elmo = Elmo(options_file, weight_file, 2, dropout=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 336/336 [00:00<00:00, 55516.49B/s]\n",
            "100%|██████████| 374434792/374434792 [00:21<00:00, 17177971.67B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3LeaTI5U7x5N",
        "outputId": "f2496a58-adc5-4b05-d7a5-6ad18dd36f0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "#Elmo test\n",
        "sentences = [['First', 'sentence', '.'], ['Another', '.'], \n",
        "             [\"Oh\", \"here\", \"we\", \"Go\", \"now\", \"you\", \"fool\", \".\"], \n",
        "             [\"meaninglesswordnotinvocab\"]]\n",
        "             \n",
        "character_ids = batch_to_ids(sentences)\n",
        "\n",
        "embeddings = elmo(character_ids)\n",
        "\n",
        "print(character_ids.shape)\n",
        "embed = embeddings[\"elmo_representations\"]\n",
        "print(len(embed))\n",
        "print(embed[0].shape)\n",
        "print(embed[1].shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 8, 50])\n",
            "2\n",
            "torch.Size([4, 8, 1024])\n",
            "torch.Size([4, 8, 1024])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4N-meDamEjF7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ELMO takes a list of parsed sentences as an input\n",
        "# It generates an embedding of length 1024 per word\n",
        "# We then need to find a good method of combining the word vecs to create \n",
        "# a sentence embedding (this article is good: https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a). \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "t9Zt3py7E1ep"
      },
      "cell_type": "markdown",
      "source": [
        "## Import and preprocess Data"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aMY0mUyknLDu",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def tokenize(text, params):\n",
        "    \"\"\"tokenizes, and optionally also:\n",
        "        1) replaces words with their lemmatized forms\n",
        "        2) removes punctuation\n",
        "        3) removes english stopwords. \n",
        "    \"\"\"\n",
        "    \n",
        "    lemmatize = params.get(\"lemmatize\")\n",
        "    rem_punct = params.get(\"rem_punct\")\n",
        "    rem_stopwords = params.get(\"rem_stopwords\")\n",
        "    \n",
        "    #deal with stopwords\n",
        "    if rem_stopwords:\n",
        "        stopwords = STOPWORDS\n",
        "    else:\n",
        "        stopwords = []\n",
        "    \n",
        "    #deal with no punctuation\n",
        "    if rem_punct:\n",
        "        stoptokens = [x for x in list(string.punctuation) if x not in list(\"#$&*@\")]\n",
        "        stoptokens += stopwords\n",
        "    else:\n",
        "        stoptokens = stopwords \n",
        "    #stoptokens will be removed from tokens\n",
        "        \n",
        "    #replace each sentence with its lemmatized counterpart\n",
        "    if not lemmatize:\n",
        "        result = [tok.text for tok in nlp_spaCy.tokenizer(text) if tok.text not in stoptokens]\n",
        "    else:\n",
        "        #otherwise: lemmatize\n",
        "        tweet = nlp_spaCy(text)  #SpaCy tokenizes and does POS and lemmatization on tokens \n",
        "        tokens = []\n",
        "        for counter, token in enumerate(tweet):\n",
        "            if token.lemma_ == \"-PRON-\":         #treat pronouns differently as SpaCy replaces all of them with \"-PRON-\"\n",
        "                tokens.append(token.text)        #which therefore becomes a common token that biases results\n",
        "            #For everything else, add the lemma:\n",
        "            else:\n",
        "                tokens.append(token.lemma_)\n",
        "        result = [tok for tok in tokens if tok not in stoptokens]\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "n1TwMNFOKRSm"
      },
      "cell_type": "markdown",
      "source": [
        "## Task A"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6NVQcb0MKUCh",
        "outputId": "8aa65fb9-4c0b-4c11-dede-9fe7b29f7692",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "cell_type": "code",
      "source": [
        "# Use two GloVe trained on two different corpuses for comparison:\n",
        "    # Glove.6B\n",
        "    # glove.twitter.27B\n",
        "!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "!unzip glove.twitter.27B.zip"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-27 08:14:27--  http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.twitter.27B.zip [following]\n",
            "--2019-02-27 08:14:27--  https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1520408563 (1.4G) [application/zip]\n",
            "Saving to: ‘glove.twitter.27B.zip’\n",
            "\n",
            "glove.twitter.27B.z 100%[===================>]   1.42G  2.12MB/s    in 10m 52s \n",
            "\n",
            "2019-02-27 08:25:20 (2.22 MB/s) - ‘glove.twitter.27B.zip’ saved [1520408563/1520408563]\n",
            "\n",
            "Archive:  glove.twitter.27B.zip\n",
            "  inflating: glove.twitter.27B.25d.txt  \n",
            "  inflating: glove.twitter.27B.50d.txt  \n",
            "  inflating: glove.twitter.27B.100d.txt  \n",
            "  inflating: glove.twitter.27B.200d.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bpNZ2KEwMOyM",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenizer(text): # create a tokenizer function for gloVe\n",
        "    res = [tok.text for tok in nlp_spaCy.tokenizer(text) if tok.text not in STOPWORDS]\n",
        "    return res\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WO69uqM3LtBS",
        "outputId": "28411695-747d-4bc1-ce5e-eaaedafb4ee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "#Create fields\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "\n",
        "TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, batch_first = True)\n",
        "LABEL = data.LabelField(sequential=False, use_vocab=True, batch_first = True)\n",
        "ID = data.LabelField(sequential=False, use_vocab=False, batch_first=True)\n",
        "\n",
        "data_fields = [('id', ID), \n",
        "               ('tweet', TEXT),\n",
        "               ('subtask_a',LABEL),\n",
        "               ('subtask_b',LABEL),\n",
        "               ('subtask_c',LABEL)]\n",
        "\n",
        "\n",
        "train = data.TabularDataset(train_fp, format='TSV', fields = \n",
        "                            data_fields, skip_header=True, filter_pred=None)\n",
        "\n",
        "train, valid = train.split(split_ratio=0.8, random_state=random.seed(SEED))\n",
        "\n",
        "print(f'Train size: {len(train)}')\n",
        "print(f'Validation size: {len(valid)}')\n",
        "\n",
        "#Now build vocab (using only the training set)\n",
        "TEXT.build_vocab(train, vectors='glove.twitter.27B.200d') #USE \"glove.840B.300d\" or glove.twitter.27B.200d\n",
        "\n",
        "LABEL.build_vocab(train.subtask_a)\n",
        "\n",
        "output_dim = len(LABEL.vocab)\n",
        "\n",
        "#Create iterators\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid),\n",
        "                        batch_sizes=(BATCH_SIZE, len(valid)),  \n",
        "                        sort_key=lambda x: len(x.tweet), device=device)\n",
        "\n",
        "# For retrieving tweet text later on\n",
        "train_df = pd.read_csv(train_fp, delimiter=\"\\t\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 10592\n",
            "Validation size: 2648\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1193102/1193514 [01:47<00:00, 11072.34it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "KkGDZeI-rccB",
        "outputId": "83a18934-e4ca-46c2-e52d-fbc766b9e291",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "print('first tweet', train[0].tweet)\n",
        "print('first label', train[0].subtask_a)\n",
        "print(\"first tweet id:\", train[0].id)\n",
        "# print(TEXT.vocab.stoi) # word to index\n",
        "# print(LABEL.vocab.stoi) # word to index\n"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first tweet ['@user', '@user', 'a', 'read', '!', 'url']\n",
            "first label NOT\n",
            "first tweet id: 29719\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Q9_NCwh3C1Z4",
        "outputId": "5d6393af-c834-46b4-82bb-c53950ee8932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "#check loader\n",
        "for idx, batch in enumerate(train_iterator):\n",
        "    inputs, labels = batch.tweet, batch.subtask_a\n",
        "    print(inputs.shape)\n",
        "    print(labels.shape)\n",
        "    print(len(train_iterator))\n",
        "    \n",
        "    break"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 49])\n",
            "torch.Size([128])\n",
            "83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "k4UHz12y6L7m",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import metrics \n",
        "\n",
        "def check_accuracy(task_header, loader, model, conf=False, RNN=False):\n",
        "    \"\"\"\n",
        "    Note at the moment this function assumes the batch size is equal to the \n",
        "    number of data in the loader when calculating the confusion matrix\n",
        "    \"\"\"\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    \n",
        "    model.eval()  # set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(loader):\n",
        "            x, y = batch.tweet, getattr(batch, task_header)\n",
        "            y = y.view(-1, 1)\n",
        "                \n",
        "            x = x.to(device=device, dtype=torch.long)  # move to  GPU\n",
        "            y = y.to(device=device, dtype=torch.long)\n",
        "            \n",
        "            if RNN:\n",
        "                #Must zero all of the accumalated hidden state for the RNN\n",
        "                model.hidden = model.init_hidden(batch.batch_size)\n",
        "                \n",
        "            logits = model(x, ids = batch.id)\n",
        "            if task_header == 'subtask_c':\n",
        "                pred_prob = F.softmax(logits, dim=1)\n",
        "                pred_1 = torch.argmax(pred_prob, dim=1).view(-1, 1)\n",
        "            else:\n",
        "                pred_prob = torch.sigmoid(logits)\n",
        "                pred_1 = (pred_prob > 0.5).type(torch.long)\n",
        "              \n",
        "            num_correct += (pred_1 == y).sum()\n",
        "            num_samples += pred_prob.size(0)\n",
        "            \n",
        "            # move to CPU to prevent memory overflow and calculate metrics\n",
        "            x = x.to(device=\"cpu\", dtype=torch.long)\n",
        "            y = y.to(device=\"cpu\", dtype=torch.long).numpy()\n",
        "            pred_1 = pred_1.to(device=\"cpu\", dtype=torch.long).numpy()\n",
        "            \n",
        "            \n",
        "        acc = float(num_correct) / num_samples\n",
        "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
        "        if conf:\n",
        "            print(metrics.confusion_matrix(y, pred_1))\n",
        "            print(metrics.classification_report(y, pred_1))\n",
        "            print(\"Kappa: {:.4f}\".format(metrics.cohen_kappa_score(y, pred_1)))\n",
        "            \n",
        "def check_loss(task_header, loader, model, loss_fn, RNN=False):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        loss = 0\n",
        "        for idx, batch in enumerate(loader):\n",
        "            x, y = batch.tweet, getattr(batch, task_header)\n",
        "\n",
        "            x = x.to(device=device, dtype=torch.long) \n",
        "            y = y.to(device=device, dtype=torch.long if task_header == 'subtask_c' else torch.float)\n",
        "            \n",
        "            if RNN:\n",
        "                #Must zero all of the accumalated hidden state for the RNN\n",
        "                model.hidden = model.init_hidden(batch.batch_size)\n",
        "                \n",
        "            logits = model(x, ids= batch.id)\n",
        "\n",
        "            loss += loss_fn(logits, y.view(-1,) if isinstance(loss_fn, nn.CrossEntropyLoss) else y.view(-1, 1))\n",
        "\n",
        "    return loss/len(loader)\n",
        "      \n",
        "\n",
        "def train_helper(task_header, model, optimizer, train_loader, \n",
        "               valid_loader, epochs=1, RNN = False, loss_fn=F.binary_cross_entropy_with_logits, \n",
        "                 print_every=50):\n",
        "    \"\"\"\n",
        "    Train a model\n",
        "    \n",
        "    Inputs:\n",
        "    - model: A PyTorch Module giving the model to train.\n",
        "    - optimizer: An Optimizer object we will use to train the model\n",
        "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
        "    \n",
        "    Returns: Nothing, but prints model accuracies during training.\n",
        "    \"\"\"\n",
        "    #sets seeds to make results reproducible\n",
        "    set_seed()\n",
        "    \n",
        "    model = model.to(device=device)  # move the model parameters to GPU\n",
        "    \n",
        "    training_losses = []\n",
        "    validation_losses = []\n",
        "    try:\n",
        "        for epoch in range(epochs):\n",
        "            print(\"Epoch:\", epoch)\n",
        "            total_loss = 0\n",
        "            for batch_idx, batch in enumerate(train_loader):\n",
        "\n",
        "                model.train()  # put model to training mode\n",
        "                optimizer.zero_grad()\n",
        "                inputs, targets = batch.tweet, getattr(batch, task_header)\n",
        "                \n",
        "                if RNN:\n",
        "                    #Must zero all of the accumulated hidden state for the RNN\n",
        "                    model.hidden = model.init_hidden(batch.batch_size)\n",
        "                \n",
        "                \n",
        "                x = inputs.to(device=device, dtype=torch.long)  # move to device, e.g. GPU\n",
        "                y = targets.to(device=device, dtype=torch.long if task_header == 'subtask_c' else torch.float) #this should be a float cross entropy\n",
        "                #x = inputs\n",
        "                #y = targets\n",
        "                logits = model(x, ids = batch.id)\n",
        "                \n",
        "                # When using cross_entropy the targets need to have a shape (N,)\n",
        "                # However, for BCEWithLogits they just need\n",
        "                # to have the same shape as the logits\n",
        "                loss = loss_fn(logits, y.view(-1,) if isinstance(loss_fn, nn.CrossEntropyLoss) else y.view(-1, 1))\n",
        "                # Zero out all of the gradients for the variables which the optimizer\n",
        "                # will update.\n",
        "                \n",
        "\n",
        "                # This is the backwards pass: compute the gradient of the loss with\n",
        "                # respect to each  parameter of the model.\n",
        "                loss.backward()\n",
        "\n",
        "                # Actually update the parameters of the model using the gradients\n",
        "                # computed by the backwards pass.\n",
        "                optimizer.step()\n",
        "\n",
        "                x = x.to(device=\"cpu\", dtype=torch.long)  # move to CPU to prevent memory overflow\n",
        "                y = y.to(device=\"cpu\", dtype=torch.long)\n",
        "\n",
        "                total_loss += loss.detach().item()\n",
        "                \n",
        "                if batch_idx % print_every == 0:\n",
        "                    print('Iteration %d, loss = %.4f' % (batch_idx, loss.item()))\n",
        "            \n",
        "            training_losses.append(total_loss/len(train_iterator))\n",
        "            print()\n",
        "            print(\"Validation Accuracy:\")\n",
        "            check_accuracy(task_header, valid_loader, model, RNN=RNN, conf=True)\n",
        "            valid_loss = check_loss(task_header, valid_loader, model, loss_fn, RNN)\n",
        "            validation_losses.append(valid_loss)\n",
        "            print()\n",
        "        return training_losses, validation_losses\n",
        "    except Exception as e:\n",
        "        #Attempt to prevent GPU memory overflow by transferring model back to cpu\n",
        "        #model = model.to(device=\"cpu\")\n",
        "        raise e    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZlBy4TS6JqW1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def add_sentiment_oldmthod(embedded, ids):\n",
        "    \"\"\"Calculate tweet sentiment and concatenate this feature onto the \n",
        "    embeddings matrix. \n",
        "    The embedding matrix dimensions will be changed as follows: \n",
        "        (B, L, D) -> (B, L, D + 2)\n",
        "        where B = batch size, L = max sent length, D = embedding dim\n",
        "    \"\"\"\n",
        "\n",
        "    assert type(ids) == torch.Tensor, \"If sentiment == True, ids must be of type tensor\"\n",
        "\n",
        "    max_sent_len = embedded.shape[1]\n",
        "\n",
        "    #retrieve tweets using id:\n",
        "    tweets = train_df[train_df[\"id\"].isin(ids.cpu().numpy())]\n",
        "\n",
        "    sentiments = []\n",
        "    subjectivities = []\n",
        "    #extract \"sentiment\" and \"subjectivity\" according to TextBlob \n",
        "    #(use a loop for now - optimize later if this is too slow)\n",
        "\n",
        "    for tweet_text in tweets[\"tweet\"].values:\n",
        "        blob = TextBlob(tweet_text)\n",
        "        sentiment = blob.sentiment.polarity\n",
        "        subjectivity = blob.sentiment.subjectivity\n",
        "\n",
        "        sentiments.append(sentiment)\n",
        "        subjectivities.append(subjectivity)\n",
        "\n",
        "    # Each token in a tweet has a seperate embedding, but we have just obtained \n",
        "    # the sentiment per tweet so we will concat the same sentiment (and subjectivity)\n",
        "    # to all tokens in the given sentence. \n",
        "    sentiments = torch.tensor(sentiments, device=device).unsqueeze(1)\n",
        "    sentiments = sentiments.repeat(1, max_sent_len).unsqueeze(2)\n",
        "    subjectivities = torch.tensor(subjectivities, device=device).unsqueeze(1)\n",
        "    subjectivities = subjectivities.repeat(1, max_sent_len).unsqueeze(2)\n",
        "\n",
        "\n",
        "    embedded = torch.cat([embedded, sentiments, subjectivities], dim=2)\n",
        "    #(batch size, max sent length, embedding dim + 2)\n",
        "\n",
        "    return embedded\n",
        "\n",
        "\n",
        "def add_sentiment(h, ids):\n",
        "    \"\"\"Calculate tweet sentiment and concatenate this feature onto the \n",
        "    feature matrix (typically just before the fully connected layers). \n",
        "    The feature matrix dimensions will be changed as follows: \n",
        "        (B, O) -> (B, O + 2)\n",
        "        where B = batch size, O = out channels\n",
        "    \"\"\"\n",
        "\n",
        "    assert type(ids) == torch.Tensor, \"If sentiment == True, ids must be of type tensor\"\n",
        "\n",
        "    #retrieve tweets using id:\n",
        "    tweets = train_df[train_df[\"id\"].isin(ids.cpu().numpy())]\n",
        "\n",
        "    sentiments = []\n",
        "    subjectivities = []\n",
        "    \n",
        "    #extract \"sentiment\" and \"subjectivity\" according to TextBlob:\n",
        "    sentiments, subjectivities = get_sentiment_v(tweets[\"tweet\"].values)\n",
        "\n",
        "    sentiments = torch.tensor(sentiments, device=device).type(torch.float32).unsqueeze(1)\n",
        "    subjectivities = torch.tensor(subjectivities, device=device).type(torch.float32).unsqueeze(1)\n",
        "    \n",
        "    \n",
        "    h = torch.cat([h, sentiments, subjectivities], dim=1)\n",
        "    #(batch size, out channels + 2)\n",
        "\n",
        "    \n",
        "    return h\n",
        "\n",
        "def get_sentiment(text):\n",
        "    \"\"\"Gets sentiment and subjectivity of text\"\"\"\n",
        "    blob = TextBlob(text)\n",
        "    sentiment = blob.sentiment.polarity\n",
        "    subjectivity = blob.sentiment.subjectivity\n",
        "    return sentiment, subjectivity\n",
        "\n",
        "#create vectorized implementation for speed\n",
        "get_sentiment_v = np.vectorize(get_sentiment, otypes = [\"float\", \"float\"], doc= \"vectorized version of get_sentiment()\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PeisH53s6cfR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#embedding (lookup layer) layer\n",
        "class SimpleClassifierGloVe(nn.Module):\n",
        "    \"\"\"Glove w. 2d conv\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab, embedding_dim, window_size, out_channels, dropout, num_classes=2, sentiment=False):\n",
        "        \n",
        "        super(SimpleClassifierGloVe, self).__init__()\n",
        "        self.sentiment = sentiment\n",
        "        self.vocab = vocab\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
        "        self.embedding.weight.data.copy_(vocab.vectors) # copies pre-trained word vectors\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(window_size, embedding_dim))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc = nn.Linear(out_channels + 2, 1 if num_classes == 2 else num_classes)\n",
        "        self.lReLU = nn.LeakyReLU(negative_slope=0.05, inplace=False)\n",
        "        \n",
        "        #Kaming normalization\n",
        "        nn.init.kaiming_normal_(self.conv.weight)\n",
        "        nn.init.kaiming_normal_(self.fc.weight)\n",
        "\n",
        "        \n",
        "        \n",
        "    def forward(self, x, ids = None):\n",
        "        \n",
        "        embedded = self.embedding(x)\n",
        "            \n",
        "        #(batch size, max sent length, embedding dim)\n",
        "        \n",
        "        \n",
        "        #images have 3 RGB channels \n",
        "        #for the text we add 1 channel\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        #(batch size, 1, max sent length, embedding dim)\n",
        "        \n",
        "        feature_maps =  self.lReLU(self.conv(embedded).squeeze(3))\n",
        "        # (batch size, out_channels, max sent length - window size +1, 1)\n",
        "        # -> (batch size, out_channels, max sent length - window size +1)\n",
        "           \n",
        "        #the max pooling layer\n",
        "        h = F.max_pool1d(feature_maps, feature_maps.shape[2]).squeeze(2)\n",
        "        # (batch size, out_channels)     \n",
        "        if self.sentiment:\n",
        "            h = add_sentiment(h, ids)\n",
        "            \n",
        "        # Do batch normalize pooled then at sentiment\n",
        "        \n",
        "        return self.fc( self.dropout(h))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1hrLG6N2Ex17",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7962
        },
        "outputId": "a3f70ec5-19c3-411a-b024-a8cde997cd3e"
      },
      "cell_type": "code",
      "source": [
        "#Simple Conv with Glove\n",
        "embedding_dim = 200\n",
        "window_size = 3\n",
        "lr = 0.00025\n",
        "out_channels = 128\n",
        "dropout = 0.5\n",
        "pos_weight = torch.tensor([1.], device = device) #deals with unbalanced classes\n",
        "\n",
        "set_seed()\n",
        "\n",
        "model = SimpleClassifierGloVe(TEXT.vocab, embedding_dim, window_size, out_channels, dropout, sentiment = True)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight = pos_weight)\n",
        "\n",
        "\n",
        "t_losses, v_losses = train_helper('subtask_a', model, optimizer, loss_fn = loss_fn, epochs = 20, train_loader=train_iterator, valid_loader=valid_iterator)\n"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Iteration 0, loss = 0.9101\n",
            "Iteration 50, loss = 0.7993\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1846 / 2648 correct (69.71)\n",
            "[[1709   64]\n",
            " [ 738  137]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.96      0.81      1773\n",
            "           1       0.68      0.16      0.25       875\n",
            "\n",
            "   micro avg       0.70      0.70      0.70      2648\n",
            "   macro avg       0.69      0.56      0.53      2648\n",
            "weighted avg       0.69      0.70      0.63      2648\n",
            "\n",
            "Kappa: 0.1497\n",
            "\n",
            "Epoch: 1\n",
            "Iteration 0, loss = 0.6978\n",
            "Iteration 50, loss = 0.6655\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1929 / 2648 correct (72.85)\n",
            "[[1673  100]\n",
            " [ 619  256]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.94      0.82      1773\n",
            "           1       0.72      0.29      0.42       875\n",
            "\n",
            "   micro avg       0.73      0.73      0.73      2648\n",
            "   macro avg       0.72      0.62      0.62      2648\n",
            "weighted avg       0.73      0.73      0.69      2648\n",
            "\n",
            "Kappa: 0.2779\n",
            "\n",
            "Epoch: 2\n",
            "Iteration 0, loss = 0.6548\n",
            "Iteration 50, loss = 0.5178\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1984 / 2648 correct (74.92)\n",
            "[[1645  128]\n",
            " [ 536  339]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.93      0.83      1773\n",
            "           1       0.73      0.39      0.51       875\n",
            "\n",
            "   micro avg       0.75      0.75      0.75      2648\n",
            "   macro avg       0.74      0.66      0.67      2648\n",
            "weighted avg       0.74      0.75      0.72      2648\n",
            "\n",
            "Kappa: 0.3574\n",
            "\n",
            "Epoch: 3\n",
            "Iteration 0, loss = 0.4620\n",
            "Iteration 50, loss = 0.5533\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2014 / 2648 correct (76.06)\n",
            "[[1626  147]\n",
            " [ 487  388]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.92      0.84      1773\n",
            "           1       0.73      0.44      0.55       875\n",
            "\n",
            "   micro avg       0.76      0.76      0.76      2648\n",
            "   macro avg       0.75      0.68      0.69      2648\n",
            "weighted avg       0.75      0.76      0.74      2648\n",
            "\n",
            "Kappa: 0.3999\n",
            "\n",
            "Epoch: 4\n",
            "Iteration 0, loss = 0.4428\n",
            "Iteration 50, loss = 0.5616\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2032 / 2648 correct (76.74)\n",
            "[[1626  147]\n",
            " [ 469  406]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.84      1773\n",
            "           1       0.73      0.46      0.57       875\n",
            "\n",
            "   micro avg       0.77      0.77      0.77      2648\n",
            "   macro avg       0.76      0.69      0.70      2648\n",
            "weighted avg       0.76      0.77      0.75      2648\n",
            "\n",
            "Kappa: 0.4203\n",
            "\n",
            "Epoch: 5\n",
            "Iteration 0, loss = 0.4755\n",
            "Iteration 50, loss = 0.5020\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2046 / 2648 correct (77.27)\n",
            "[[1602  171]\n",
            " [ 431  444]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.90      0.84      1773\n",
            "           1       0.72      0.51      0.60       875\n",
            "\n",
            "   micro avg       0.77      0.77      0.77      2648\n",
            "   macro avg       0.75      0.71      0.72      2648\n",
            "weighted avg       0.77      0.77      0.76      2648\n",
            "\n",
            "Kappa: 0.4444\n",
            "\n",
            "Epoch: 6\n",
            "Iteration 0, loss = 0.4283\n",
            "Iteration 50, loss = 0.4009\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2056 / 2648 correct (77.64)\n",
            "[[1618  155]\n",
            " [ 437  438]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.91      0.85      1773\n",
            "           1       0.74      0.50      0.60       875\n",
            "\n",
            "   micro avg       0.78      0.78      0.78      2648\n",
            "   macro avg       0.76      0.71      0.72      2648\n",
            "weighted avg       0.77      0.78      0.76      2648\n",
            "\n",
            "Kappa: 0.4499\n",
            "\n",
            "Epoch: 7\n",
            "Iteration 0, loss = 0.4531\n",
            "Iteration 50, loss = 0.4483\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2058 / 2648 correct (77.72)\n",
            "[[1629  144]\n",
            " [ 446  429]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.92      0.85      1773\n",
            "           1       0.75      0.49      0.59       875\n",
            "\n",
            "   micro avg       0.78      0.78      0.78      2648\n",
            "   macro avg       0.77      0.70      0.72      2648\n",
            "weighted avg       0.77      0.78      0.76      2648\n",
            "\n",
            "Kappa: 0.4482\n",
            "\n",
            "Epoch: 8\n",
            "Iteration 0, loss = 0.3268\n",
            "Iteration 50, loss = 0.4622\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2066 / 2648 correct (78.02)\n",
            "[[1590  183]\n",
            " [ 399  476]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.90      0.85      1773\n",
            "           1       0.72      0.54      0.62       875\n",
            "\n",
            "   micro avg       0.78      0.78      0.78      2648\n",
            "   macro avg       0.76      0.72      0.73      2648\n",
            "weighted avg       0.77      0.78      0.77      2648\n",
            "\n",
            "Kappa: 0.4702\n",
            "\n",
            "Epoch: 9\n",
            "Iteration 0, loss = 0.3677\n",
            "Iteration 50, loss = 0.4068\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2069 / 2648 correct (78.13)\n",
            "[[1579  194]\n",
            " [ 385  490]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.89      0.85      1773\n",
            "           1       0.72      0.56      0.63       875\n",
            "\n",
            "   micro avg       0.78      0.78      0.78      2648\n",
            "   macro avg       0.76      0.73      0.74      2648\n",
            "weighted avg       0.78      0.78      0.77      2648\n",
            "\n",
            "Kappa: 0.4769\n",
            "\n",
            "Epoch: 10\n",
            "Iteration 0, loss = 0.3620\n",
            "Iteration 50, loss = 0.3867\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2073 / 2648 correct (78.29)\n",
            "[[1580  193]\n",
            " [ 382  493]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.89      0.85      1773\n",
            "           1       0.72      0.56      0.63       875\n",
            "\n",
            "   micro avg       0.78      0.78      0.78      2648\n",
            "   macro avg       0.76      0.73      0.74      2648\n",
            "weighted avg       0.78      0.78      0.78      2648\n",
            "\n",
            "Kappa: 0.4809\n",
            "\n",
            "Epoch: 11\n",
            "Iteration 0, loss = 0.4052\n",
            "Iteration 50, loss = 0.3136\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2078 / 2648 correct (78.47)\n",
            "[[1595  178]\n",
            " [ 392  483]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.90      0.85      1773\n",
            "           1       0.73      0.55      0.63       875\n",
            "\n",
            "   micro avg       0.78      0.78      0.78      2648\n",
            "   macro avg       0.77      0.73      0.74      2648\n",
            "weighted avg       0.78      0.78      0.78      2648\n",
            "\n",
            "Kappa: 0.4814\n",
            "\n",
            "Epoch: 12\n",
            "Iteration 0, loss = 0.3337\n",
            "Iteration 50, loss = 0.3122\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2079 / 2648 correct (78.51)\n",
            "[[1558  215]\n",
            " [ 354  521]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.85      1773\n",
            "           1       0.71      0.60      0.65       875\n",
            "\n",
            "   micro avg       0.79      0.79      0.79      2648\n",
            "   macro avg       0.76      0.74      0.75      2648\n",
            "weighted avg       0.78      0.79      0.78      2648\n",
            "\n",
            "Kappa: 0.4940\n",
            "\n",
            "Epoch: 13\n",
            "Iteration 0, loss = 0.2740\n",
            "Iteration 50, loss = 0.3517\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2082 / 2648 correct (78.63)\n",
            "[[1570  203]\n",
            " [ 363  512]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.89      0.85      1773\n",
            "           1       0.72      0.59      0.64       875\n",
            "\n",
            "   micro avg       0.79      0.79      0.79      2648\n",
            "   macro avg       0.76      0.74      0.75      2648\n",
            "weighted avg       0.78      0.79      0.78      2648\n",
            "\n",
            "Kappa: 0.4935\n",
            "\n",
            "Epoch: 14\n",
            "Iteration 0, loss = 0.2550\n",
            "Iteration 50, loss = 0.3350\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2089 / 2648 correct (78.89)\n",
            "[[1576  197]\n",
            " [ 362  513]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.89      0.85      1773\n",
            "           1       0.72      0.59      0.65       875\n",
            "\n",
            "   micro avg       0.79      0.79      0.79      2648\n",
            "   macro avg       0.77      0.74      0.75      2648\n",
            "weighted avg       0.78      0.79      0.78      2648\n",
            "\n",
            "Kappa: 0.4990\n",
            "\n",
            "Epoch: 15\n",
            "Iteration 0, loss = 0.3584\n",
            "Iteration 50, loss = 0.2268\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2083 / 2648 correct (78.66)\n",
            "[[1573  200]\n",
            " [ 365  510]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.89      0.85      1773\n",
            "           1       0.72      0.58      0.64       875\n",
            "\n",
            "   micro avg       0.79      0.79      0.79      2648\n",
            "   macro avg       0.76      0.74      0.75      2648\n",
            "weighted avg       0.78      0.79      0.78      2648\n",
            "\n",
            "Kappa: 0.4936\n",
            "\n",
            "Epoch: 16\n",
            "Iteration 0, loss = 0.2140\n",
            "Iteration 50, loss = 0.2308\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2077 / 2648 correct (78.44)\n",
            "[[1567  206]\n",
            " [ 365  510]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.85      1773\n",
            "           1       0.71      0.58      0.64       875\n",
            "\n",
            "   micro avg       0.78      0.78      0.78      2648\n",
            "   macro avg       0.76      0.73      0.74      2648\n",
            "weighted avg       0.78      0.78      0.78      2648\n",
            "\n",
            "Kappa: 0.4892\n",
            "\n",
            "Epoch: 17\n",
            "Iteration 0, loss = 0.2839\n",
            "Iteration 50, loss = 0.1971\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2077 / 2648 correct (78.44)\n",
            "[[1559  214]\n",
            " [ 357  518]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.85      1773\n",
            "           1       0.71      0.59      0.64       875\n",
            "\n",
            "   micro avg       0.78      0.78      0.78      2648\n",
            "   macro avg       0.76      0.74      0.74      2648\n",
            "weighted avg       0.78      0.78      0.78      2648\n",
            "\n",
            "Kappa: 0.4916\n",
            "\n",
            "Epoch: 18\n",
            "Iteration 0, loss = 0.2311\n",
            "Iteration 50, loss = 0.1846\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2072 / 2648 correct (78.25)\n",
            "[[1556  217]\n",
            " [ 359  516]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84      1773\n",
            "           1       0.70      0.59      0.64       875\n",
            "\n",
            "   micro avg       0.78      0.78      0.78      2648\n",
            "   macro avg       0.76      0.73      0.74      2648\n",
            "weighted avg       0.78      0.78      0.78      2648\n",
            "\n",
            "Kappa: 0.4874\n",
            "\n",
            "Epoch: 19\n",
            "Iteration 0, loss = 0.1960\n",
            "Iteration 50, loss = 0.2586\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2087 / 2648 correct (78.81)\n",
            "[[1586  187]\n",
            " [ 374  501]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.89      0.85      1773\n",
            "           1       0.73      0.57      0.64       875\n",
            "\n",
            "   micro avg       0.79      0.79      0.79      2648\n",
            "   macro avg       0.77      0.73      0.75      2648\n",
            "weighted avg       0.78      0.79      0.78      2648\n",
            "\n",
            "Kappa: 0.4938\n",
            "\n",
            "Epoch: 20\n",
            "Iteration 0, loss = 0.1857\n",
            "Iteration 50, loss = 0.2096\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2082 / 2648 correct (78.63)\n",
            "[[1576  197]\n",
            " [ 369  506]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.89      0.85      1773\n",
            "           1       0.72      0.58      0.64       875\n",
            "\n",
            "   micro avg       0.79      0.79      0.79      2648\n",
            "   macro avg       0.77      0.73      0.74      2648\n",
            "weighted avg       0.78      0.79      0.78      2648\n",
            "\n",
            "Kappa: 0.4916\n",
            "\n",
            "Epoch: 21\n",
            "Iteration 0, loss = 0.2308\n",
            "Iteration 50, loss = 0.2007\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2066 / 2648 correct (78.02)\n",
            "[[1550  223]\n",
            " [ 359  516]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84      1773\n",
            "           1       0.70      0.59      0.64       875\n",
            "\n",
            "   micro avg       0.78      0.78      0.78      2648\n",
            "   macro avg       0.76      0.73      0.74      2648\n",
            "weighted avg       0.77      0.78      0.78      2648\n",
            "\n",
            "Kappa: 0.4829\n",
            "\n",
            "Epoch: 22\n",
            "Iteration 0, loss = 0.2358\n",
            "Iteration 50, loss = 0.1402\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2064 / 2648 correct (77.95)\n",
            "[[1552  221]\n",
            " [ 363  512]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84      1773\n",
            "           1       0.70      0.59      0.64       875\n",
            "\n",
            "   micro avg       0.78      0.78      0.78      2648\n",
            "   macro avg       0.75      0.73      0.74      2648\n",
            "weighted avg       0.77      0.78      0.77      2648\n",
            "\n",
            "Kappa: 0.4802\n",
            "\n",
            "Epoch: 23\n",
            "Iteration 0, loss = 0.1489\n",
            "Iteration 50, loss = 0.1689\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2058 / 2648 correct (77.72)\n",
            "[[1543  230]\n",
            " [ 360  515]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84      1773\n",
            "           1       0.69      0.59      0.64       875\n",
            "\n",
            "   micro avg       0.78      0.78      0.78      2648\n",
            "   macro avg       0.75      0.73      0.74      2648\n",
            "weighted avg       0.77      0.78      0.77      2648\n",
            "\n",
            "Kappa: 0.4768\n",
            "\n",
            "Epoch: 24\n",
            "Iteration 0, loss = 0.1674\n",
            "Iteration 50, loss = 0.1523\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2051 / 2648 correct (77.45)\n",
            "[[1532  241]\n",
            " [ 356  519]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.86      0.84      1773\n",
            "           1       0.68      0.59      0.63       875\n",
            "\n",
            "   micro avg       0.77      0.77      0.77      2648\n",
            "   macro avg       0.75      0.73      0.74      2648\n",
            "weighted avg       0.77      0.77      0.77      2648\n",
            "\n",
            "Kappa: 0.4730\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sFC6hi13DGnK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#embedding (lookup layer) layer\n",
        "class ClassifierGloVeDeep(nn.Module):\n",
        "    \"\"\"Glove w. 2d conv, 2 layers\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab, embedding_dim, window_size, out_channels = 128, dropout=0.5, n_hidden = 64, num_classes=2, ids=None):\n",
        "        \n",
        "        super(ClassifierGloVeDeep, self).__init__()\n",
        "        \n",
        "        self.vocab = vocab\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
        "        self.embedding.weight.data.copy_(vocab.vectors) # copies pre-trained word vectors\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(window_size, embedding_dim))\n",
        "        \n",
        "        self.fc1 = nn.Linear(out_channels, n_hidden[0])\n",
        "        self.fc2 = nn.Linear(n_hidden[0], n_hidden[1])\n",
        "        self.fc3 = nn.Linear(n_hidden[1], n_hidden[2])\n",
        "        self.fc4 = nn.Linear(n_hidden[2], n_hidden[3])\n",
        "        self.fc5 = nn.Linear(n_hidden[3], n_hidden[4])\n",
        "        self.fc6 = nn.Linear(n_hidden[4], 1 if num_classes == 2 else num_classes)\n",
        "        \n",
        "        #Activation: use leaky relu\n",
        "        self.lReLU = nn.LeakyReLU(negative_slope=0.05, inplace=False)\n",
        "        \n",
        "        #Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        #Kaming normalization\n",
        "        nn.init.kaiming_normal_(self.conv1.weight)\n",
        "        nn.init.kaiming_normal_(self.fc1.weight)\n",
        "        nn.init.kaiming_normal_(self.fc2.weight)\n",
        "        \n",
        "        \n",
        "    def forward(self, x, ):\n",
        "        \n",
        "        embedded = self.embedding(x)\n",
        "                \n",
        "        #(batch size, max sent length, embedding dim)\n",
        "        \n",
        "        #images have 3 RGB channels \n",
        "        #for the text we add 1 channel\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        \n",
        "        #(batch size, 1, max sent length, embedding dim)\n",
        "        \n",
        "        feature_maps =  self.lReLU(self.conv1(embedded).squeeze(3))\n",
        "        # (batch size, out_channels, max sent length - window size +1, 1)\n",
        "        # -> (batch size, out_channels, max sent length - window size +1)\n",
        "        \n",
        "        #the max pooling layer\n",
        "        h = F.max_pool1d(feature_maps, feature_maps.shape[2]).squeeze(2)\n",
        "        # (batch size, out_channels)  \n",
        "        \n",
        "\n",
        "        # Do batch normalize pooled then add sentiment\n",
        "        h = self.lReLU(self.fc1( self.dropout(h)))\n",
        "        h = self.lReLU(self.fc2( self.dropout(h)))\n",
        "        h = self.lReLU(self.fc3( self.dropout(h)))\n",
        "        h = self.lReLU(self.fc4( self.dropout(h)))\n",
        "        h = self.lReLU(self.fc5( self.dropout(h)))\n",
        "        h = self.fc6( self.dropout(h))\n",
        "        \n",
        "        \n",
        "        return h\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ee6etf14Qtxz",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#embedding (lookup layer) layer\n",
        "class ClassifierGloVe(nn.Module):\n",
        "    \"\"\"Glove w. 2d conv, 2 layers\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab, embedding_dim, window_size, out_channels = 128, dropout=0.5, n_hidden = 64, num_classes=2, ids=None):\n",
        "        \n",
        "        super(ClassifierGloVe, self).__init__()\n",
        "        \n",
        "        \n",
        "        self.vocab = vocab\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
        "        self.embedding.weight.data.copy_(vocab.vectors) # copies pre-trained word vectors\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(window_size, embedding_dim))\n",
        "        \n",
        "        self.fc1 = nn.Linear(out_channels, n_hidden)\n",
        "        self.fc2 = nn.Linear(n_hidden, 1 if num_classes == 2 else num_classes)\n",
        "        \n",
        "        #Activation: use leaky relu\n",
        "        self.lReLU = nn.LeakyReLU(negative_slope=0.05, inplace=False)\n",
        "        \n",
        "        #Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        #Kaming normalization\n",
        "        nn.init.kaiming_normal_(self.conv1.weight)\n",
        "        nn.init.kaiming_normal_(self.fc1.weight)\n",
        "        nn.init.kaiming_normal_(self.fc2.weight)\n",
        "        \n",
        "        \n",
        "    def forward(self, x, ):\n",
        "        \n",
        "        embedded = self.embedding(x)\n",
        "                \n",
        "        #(batch size, max sent length, embedding dim)\n",
        "        \n",
        "        #images have 3 RGB channels \n",
        "        #for the text we add 1 channel\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        \n",
        "        #(batch size, 1, max sent length, embedding dim)\n",
        "        \n",
        "        feature_maps =  self.lReLU(self.conv1(embedded).squeeze(3))\n",
        "        # (batch size, out_channels, max sent length - window size +1, 1)\n",
        "        # -> (batch size, out_channels, max sent length - window size +1)\n",
        "        \n",
        "        #the max pooling layer\n",
        "        h = F.max_pool1d(feature_maps, feature_maps.shape[2]).squeeze(2)\n",
        "        # (batch size, out_channels)  \n",
        "        \n",
        "\n",
        "        # Do batch normalize pooled then add sentiment\n",
        "        h = self.lReLU(self.fc1( self.dropout(h)))\n",
        "        \n",
        "        h = self.fc2(self.dropout(h))\n",
        "        return h\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6-FK3OFvGMhw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#embedding (lookup layer) layer\n",
        "class BidirectionalGRU(nn.Module):\n",
        "    \"\"\"Glove w. 2d conv, 2 layers\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab, embedding_dim, window_size, out_channels = 128, \n",
        "                 dropout=0.5, n_hidden = 64, num_classes=2, batch_size = BATCH_SIZE, ids=None):\n",
        "        \n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "        self.n_hidden = n_hidden\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.num_layers = 1 #number of GRU layers\n",
        "        \n",
        "        self.vocab = vocab\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
        "        self.embedding.weight.data.copy_(vocab.vectors) # copies pre-trained word vectors\n",
        "        \n",
        "        self.bi_gru =  torch.nn.GRU(input_size=embedding_dim, hidden_size=n_hidden, \n",
        "                                    num_layers= self.num_layers, batch_first=False, \n",
        "                                    bidirectional=True)\n",
        "#         self.reverse_gru = torch.nn.GRU(input_size=embedding_dim, hidden_size=n_hidden, num_layers=1, batch_first=True, bidirectional=False)\n",
        "        self.hidden = self.init_hidden(self.batch_size)\n",
        "    \n",
        "        #Fully connected layer will convert GRU output into a label\n",
        "        # number of input features is 2 * n_hidden since GRU is bidirectional\n",
        "        self.fc1 = nn.Linear( 2 * n_hidden, 16)\n",
        "        self.fc2 = nn.Linear(16, 1 if num_classes == 2 else num_classes)\n",
        "\n",
        "        #Activation: use leaky relu\n",
        "        self.lReLU = nn.LeakyReLU(negative_slope=0.05, inplace=False)\n",
        "        \n",
        "        #Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        #Kaming normalization\n",
        "        nn.init.kaiming_normal_(self.fc1.weight)\n",
        "        \n",
        "#         #Initialize weights to same in forward and backwards direction:\n",
        "#         reverse_gru.weight_ih_l0 = bi_grus.weight_ih_l0_reverse\n",
        "#         reverse_gru.weight_hh_l0 = bi_grus.weight_hh_l0_reverse\n",
        "#         reverse_gru.bias_ih_l0 = bi_grus.bias_ih_l0_reverse\n",
        "#         reverse_gru.bias_hh_l0 = bi_grus.bias_hh_l0_reverse\n",
        "    def init_hidden(self, batch_size = BATCH_SIZE):\n",
        "        return torch.zeros((self.num_layers * 2, batch_size, self.n_hidden), device=device)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        #(batch size, max sent length, embedding dim)\n",
        "        embedded = self.embedding(x).view((embedded.shape[1], embedded.shape[0], -1))\n",
        "        #(max sent length, batch size, embedding dim)\n",
        "        \n",
        "        bi_output, self.hidden = self.bi_gru(embedded, self.hidden)\n",
        "        \n",
        "        # add sentiment?\n",
        "        \n",
        "        #Just take final value of bi_output:\n",
        "        h = self.lReLU(bi_output[-1])\n",
        "\n",
        "        h = self.fc1( self.dropout(h))\n",
        "        \n",
        "        h = self.fc2( self.dropout(h))\n",
        "        \n",
        "        #h = self.fc2(self.dropout(h))\n",
        "        return h\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "X9LseL5F9n7P",
        "outputId": "1b771598-47dc-49c3-9a62-9a3948dfa60c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6038
        }
      },
      "cell_type": "code",
      "source": [
        "# Bidirectional GRU with Glove\n",
        "# Beats baseline for minor class. Not as good as convolution\n",
        "\n",
        "embedding_dim = 200\n",
        "window_size = 3\n",
        "lr = 0.00025\n",
        "out_channels = 128\n",
        "dropout = 0\n",
        "pos_weight = torch.tensor([2.], device = device) #deals with unbalanced classes\n",
        "\n",
        "\n",
        "model = BidirectionalGRU(TEXT.vocab, embedding_dim, window_size, out_channels, dropout, n_hidden = 64)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "\n",
        "t_losses, v_losses = train_helper('subtask_a', model, optimizer, loss_fn = loss_fn,\n",
        "                                  epochs = 20, train_loader=train_iterator, \n",
        "                                  valid_loader=valid_iterator, RNN=True)\n"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Iteration 0, loss = 0.9413\n",
            "Iteration 50, loss = 0.9259\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 878 / 2648 correct (33.16)\n",
            "[[  14 1759]\n",
            " [  11  864]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.01      0.02      1773\n",
            "           1       0.33      0.99      0.49       875\n",
            "\n",
            "   micro avg       0.33      0.33      0.33      2648\n",
            "   macro avg       0.44      0.50      0.25      2648\n",
            "weighted avg       0.48      0.33      0.17      2648\n",
            "\n",
            "\n",
            "Epoch: 1\n",
            "Iteration 0, loss = 0.9316\n",
            "Iteration 50, loss = 0.9418\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 883 / 2648 correct (33.35)\n",
            "[[  22 1751]\n",
            " [  14  861]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.01      0.02      1773\n",
            "           1       0.33      0.98      0.49       875\n",
            "\n",
            "   micro avg       0.33      0.33      0.33      2648\n",
            "   macro avg       0.47      0.50      0.26      2648\n",
            "weighted avg       0.52      0.33      0.18      2648\n",
            "\n",
            "\n",
            "Epoch: 2\n",
            "Iteration 0, loss = 0.9645\n",
            "Iteration 50, loss = 0.8763\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 880 / 2648 correct (33.23)\n",
            "[[   9 1764]\n",
            " [   4  871]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.01      0.01      1773\n",
            "           1       0.33      1.00      0.50       875\n",
            "\n",
            "   micro avg       0.33      0.33      0.33      2648\n",
            "   macro avg       0.51      0.50      0.25      2648\n",
            "weighted avg       0.57      0.33      0.17      2648\n",
            "\n",
            "\n",
            "Epoch: 3\n",
            "Iteration 0, loss = 0.9718\n",
            "Iteration 50, loss = 0.8899\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1745 / 2648 correct (65.90)\n",
            "[[1712   61]\n",
            " [ 842   33]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.97      0.79      1773\n",
            "           1       0.35      0.04      0.07       875\n",
            "\n",
            "   micro avg       0.66      0.66      0.66      2648\n",
            "   macro avg       0.51      0.50      0.43      2648\n",
            "weighted avg       0.56      0.66      0.55      2648\n",
            "\n",
            "\n",
            "Epoch: 4\n",
            "Iteration 0, loss = 0.9374\n",
            "Iteration 50, loss = 0.8982\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1770 / 2648 correct (66.84)\n",
            "[[1756   17]\n",
            " [ 861   14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.99      0.80      1773\n",
            "           1       0.45      0.02      0.03       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.56      0.50      0.42      2648\n",
            "weighted avg       0.60      0.67      0.55      2648\n",
            "\n",
            "\n",
            "Epoch: 5\n",
            "Iteration 0, loss = 0.9251\n",
            "Iteration 50, loss = 0.9528\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1760 / 2648 correct (66.47)\n",
            "[[1736   37]\n",
            " [ 851   24]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.98      0.80      1773\n",
            "           1       0.39      0.03      0.05       875\n",
            "\n",
            "   micro avg       0.66      0.66      0.66      2648\n",
            "   macro avg       0.53      0.50      0.42      2648\n",
            "weighted avg       0.58      0.66      0.55      2648\n",
            "\n",
            "\n",
            "Epoch: 6\n",
            "Iteration 0, loss = 0.9574\n",
            "Iteration 50, loss = 0.9106\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1692 / 2648 correct (63.90)\n",
            "[[1609  164]\n",
            " [ 792   83]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.91      0.77      1773\n",
            "           1       0.34      0.09      0.15       875\n",
            "\n",
            "   micro avg       0.64      0.64      0.64      2648\n",
            "   macro avg       0.50      0.50      0.46      2648\n",
            "weighted avg       0.56      0.64      0.57      2648\n",
            "\n",
            "\n",
            "Epoch: 7\n",
            "Iteration 0, loss = 0.9370\n",
            "Iteration 50, loss = 0.9460\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 921 / 2648 correct (34.78)\n",
            "[[  96 1677]\n",
            " [  50  825]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.05      0.10      1773\n",
            "           1       0.33      0.94      0.49       875\n",
            "\n",
            "   micro avg       0.35      0.35      0.35      2648\n",
            "   macro avg       0.49      0.50      0.29      2648\n",
            "weighted avg       0.55      0.35      0.23      2648\n",
            "\n",
            "\n",
            "Epoch: 8\n",
            "Iteration 0, loss = 0.8599\n",
            "Iteration 50, loss = 0.9439\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 899 / 2648 correct (33.95)\n",
            "[[  42 1731]\n",
            " [  18  857]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.02      0.05      1773\n",
            "           1       0.33      0.98      0.49       875\n",
            "\n",
            "   micro avg       0.34      0.34      0.34      2648\n",
            "   macro avg       0.52      0.50      0.27      2648\n",
            "weighted avg       0.58      0.34      0.19      2648\n",
            "\n",
            "\n",
            "Epoch: 9\n",
            "Iteration 0, loss = 0.9110\n",
            "Iteration 50, loss = 0.9971\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 892 / 2648 correct (33.69)\n",
            "[[  30 1743]\n",
            " [  13  862]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.02      0.03      1773\n",
            "           1       0.33      0.99      0.50       875\n",
            "\n",
            "   micro avg       0.34      0.34      0.34      2648\n",
            "   macro avg       0.51      0.50      0.26      2648\n",
            "weighted avg       0.58      0.34      0.19      2648\n",
            "\n",
            "\n",
            "Epoch: 10\n",
            "Iteration 0, loss = 0.9400\n",
            "Iteration 50, loss = 0.8638\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 962 / 2648 correct (36.33)\n",
            "[[ 168 1605]\n",
            " [  81  794]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.09      0.17      1773\n",
            "           1       0.33      0.91      0.49       875\n",
            "\n",
            "   micro avg       0.36      0.36      0.36      2648\n",
            "   macro avg       0.50      0.50      0.33      2648\n",
            "weighted avg       0.56      0.36      0.27      2648\n",
            "\n",
            "\n",
            "Epoch: 11\n",
            "Iteration 0, loss = 0.9231\n",
            "Iteration 50, loss = 0.9146\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 886 / 2648 correct (33.46)\n",
            "[[  21 1752]\n",
            " [  10  865]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.01      0.02      1773\n",
            "           1       0.33      0.99      0.50       875\n",
            "\n",
            "   micro avg       0.33      0.33      0.33      2648\n",
            "   macro avg       0.50      0.50      0.26      2648\n",
            "weighted avg       0.56      0.33      0.18      2648\n",
            "\n",
            "\n",
            "Epoch: 12\n",
            "Iteration 0, loss = 0.9192\n",
            "Iteration 50, loss = 0.9104\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 943 / 2648 correct (35.61)\n",
            "[[ 138 1635]\n",
            " [  70  805]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.08      0.14      1773\n",
            "           1       0.33      0.92      0.49       875\n",
            "\n",
            "   micro avg       0.36      0.36      0.36      2648\n",
            "   macro avg       0.50      0.50      0.31      2648\n",
            "weighted avg       0.55      0.36      0.25      2648\n",
            "\n",
            "\n",
            "Epoch: 13\n",
            "Iteration 0, loss = 0.9217\n",
            "Iteration 50, loss = 0.9478\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 889 / 2648 correct (33.57)\n",
            "[[  28 1745]\n",
            " [  14  861]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.02      0.03      1773\n",
            "           1       0.33      0.98      0.49       875\n",
            "\n",
            "   micro avg       0.34      0.34      0.34      2648\n",
            "   macro avg       0.50      0.50      0.26      2648\n",
            "weighted avg       0.56      0.34      0.18      2648\n",
            "\n",
            "\n",
            "Epoch: 14\n",
            "Iteration 0, loss = 0.8883\n",
            "Iteration 50, loss = 0.8793\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1741 / 2648 correct (65.75)\n",
            "[[1712   61]\n",
            " [ 846   29]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.97      0.79      1773\n",
            "           1       0.32      0.03      0.06       875\n",
            "\n",
            "   micro avg       0.66      0.66      0.66      2648\n",
            "   macro avg       0.50      0.50      0.43      2648\n",
            "weighted avg       0.55      0.66      0.55      2648\n",
            "\n",
            "\n",
            "Epoch: 15\n",
            "Iteration 0, loss = 0.9649\n",
            "Iteration 50, loss = 0.9513\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1744 / 2648 correct (65.86)\n",
            "[[1722   51]\n",
            " [ 853   22]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.97      0.79      1773\n",
            "           1       0.30      0.03      0.05       875\n",
            "\n",
            "   micro avg       0.66      0.66      0.66      2648\n",
            "   macro avg       0.49      0.50      0.42      2648\n",
            "weighted avg       0.55      0.66      0.55      2648\n",
            "\n",
            "\n",
            "Epoch: 16\n",
            "Iteration 0, loss = 0.9092\n",
            "Iteration 50, loss = 0.9315\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1678 / 2648 correct (63.37)\n",
            "[[1598  175]\n",
            " [ 795   80]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.90      0.77      1773\n",
            "           1       0.31      0.09      0.14       875\n",
            "\n",
            "   micro avg       0.63      0.63      0.63      2648\n",
            "   macro avg       0.49      0.50      0.45      2648\n",
            "weighted avg       0.55      0.63      0.56      2648\n",
            "\n",
            "\n",
            "Epoch: 17\n",
            "Iteration 0, loss = 0.9328\n",
            "Iteration 50, loss = 0.9522\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1730 / 2648 correct (65.33)\n",
            "[[1696   77]\n",
            " [ 841   34]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79      1773\n",
            "           1       0.31      0.04      0.07       875\n",
            "\n",
            "   micro avg       0.65      0.65      0.65      2648\n",
            "   macro avg       0.49      0.50      0.43      2648\n",
            "weighted avg       0.55      0.65      0.55      2648\n",
            "\n",
            "\n",
            "Epoch: 18\n",
            "Iteration 0, loss = 0.9302\n",
            "Iteration 50, loss = 0.9396\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 896 / 2648 correct (33.84)\n",
            "[[  54 1719]\n",
            " [  33  842]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.03      0.06      1773\n",
            "           1       0.33      0.96      0.49       875\n",
            "\n",
            "   micro avg       0.34      0.34      0.34      2648\n",
            "   macro avg       0.47      0.50      0.27      2648\n",
            "weighted avg       0.52      0.34      0.20      2648\n",
            "\n",
            "\n",
            "Epoch: 19\n",
            "Iteration 0, loss = 0.9115\n",
            "Iteration 50, loss = 0.9955\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 881 / 2648 correct (33.27)\n",
            "[[  18 1755]\n",
            " [  12  863]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.01      0.02      1773\n",
            "           1       0.33      0.99      0.49       875\n",
            "\n",
            "   micro avg       0.33      0.33      0.33      2648\n",
            "   macro avg       0.46      0.50      0.26      2648\n",
            "weighted avg       0.51      0.33      0.18      2648\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "a4wCjiR5O5Qf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9605
        },
        "outputId": "477dfa6c-6ab5-4fef-cf1a-830cb484b48c"
      },
      "cell_type": "code",
      "source": [
        "#Conv with Glove Deep\n",
        "embedding_dim = 200\n",
        "window_size = 3\n",
        "lr = 0.00025\n",
        "out_channels = 128\n",
        "dropout = 0.5\n",
        "pos_weight = torch.tensor([0.6], device = device) #deals with unbalanced classes\n",
        "n_hidden = (64, 32, 16, 8, 4)\n",
        "\n",
        "set_seed()\n",
        "\n",
        "model = ClassifierGloVeDeep(TEXT.vocab, embedding_dim, window_size, out_channels, dropout, n_hidden = n_hidden)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight = pos_weight)\n",
        "\n",
        "\n",
        "t_losses, v_losses = train_helper('subtask_a', model, optimizer, loss_fn = loss_fn, epochs = 30, train_loader=train_iterator, valid_loader=valid_iterator)\n"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Iteration 0, loss = 0.6479\n",
            "Iteration 50, loss = 0.7072\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 875 / 2648 correct (33.04)\n",
            "[[   0 1773]\n",
            " [   0  875]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1773\n",
            "           1       0.33      1.00      0.50       875\n",
            "\n",
            "   micro avg       0.33      0.33      0.33      2648\n",
            "   macro avg       0.17      0.50      0.25      2648\n",
            "weighted avg       0.11      0.33      0.16      2648\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 0, loss = 0.6512\n",
            "Iteration 50, loss = 0.5838\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 879 / 2648 correct (33.19)\n",
            "[[   7 1766]\n",
            " [   3  872]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.00      0.01      1773\n",
            "           1       0.33      1.00      0.50       875\n",
            "\n",
            "   micro avg       0.33      0.33      0.33      2648\n",
            "   macro avg       0.52      0.50      0.25      2648\n",
            "weighted avg       0.58      0.33      0.17      2648\n",
            "\n",
            "Kappa 0.0003\n",
            "\n",
            "Epoch: 2\n",
            "Iteration 0, loss = 0.6778\n",
            "Iteration 50, loss = 0.6506\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 890 / 2648 correct (33.61)\n",
            "[[  24 1749]\n",
            " [   9  866]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.01      0.03      1773\n",
            "           1       0.33      0.99      0.50       875\n",
            "\n",
            "   micro avg       0.34      0.34      0.34      2648\n",
            "   macro avg       0.53      0.50      0.26      2648\n",
            "weighted avg       0.60      0.34      0.18      2648\n",
            "\n",
            "Kappa 0.0022\n",
            "\n",
            "Epoch: 3\n",
            "Iteration 0, loss = 0.6812\n",
            "Iteration 50, loss = 0.6490\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1001 / 2648 correct (37.80)\n",
            "[[ 225 1548]\n",
            " [  99  776]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.13      0.21      1773\n",
            "           1       0.33      0.89      0.49       875\n",
            "\n",
            "   micro avg       0.38      0.38      0.38      2648\n",
            "   macro avg       0.51      0.51      0.35      2648\n",
            "weighted avg       0.58      0.38      0.30      2648\n",
            "\n",
            "Kappa 0.0097\n",
            "\n",
            "Epoch: 4\n",
            "Iteration 0, loss = 0.6856\n",
            "Iteration 50, loss = 0.6642\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1421 / 2648 correct (53.66)\n",
            "[[931 842]\n",
            " [385 490]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.53      0.60      1773\n",
            "           1       0.37      0.56      0.44       875\n",
            "\n",
            "   micro avg       0.54      0.54      0.54      2648\n",
            "   macro avg       0.54      0.54      0.52      2648\n",
            "weighted avg       0.60      0.54      0.55      2648\n",
            "\n",
            "Kappa 0.0752\n",
            "\n",
            "Epoch: 5\n",
            "Iteration 0, loss = 0.6172\n",
            "Iteration 50, loss = 0.5744\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1705 / 2648 correct (64.39)\n",
            "[[1428  345]\n",
            " [ 598  277]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.81      0.75      1773\n",
            "           1       0.45      0.32      0.37       875\n",
            "\n",
            "   micro avg       0.64      0.64      0.64      2648\n",
            "   macro avg       0.58      0.56      0.56      2648\n",
            "weighted avg       0.62      0.64      0.63      2648\n",
            "\n",
            "Kappa 0.1316\n",
            "\n",
            "Epoch: 6\n",
            "Iteration 0, loss = 0.5983\n",
            "Iteration 50, loss = 0.5998\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1765 / 2648 correct (66.65)\n",
            "[[1504  269]\n",
            " [ 614  261]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.85      0.77      1773\n",
            "           1       0.49      0.30      0.37       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.60      0.57      0.57      2648\n",
            "weighted avg       0.64      0.67      0.64      2648\n",
            "\n",
            "Kappa 0.1628\n",
            "\n",
            "Epoch: 7\n",
            "Iteration 0, loss = 0.6017\n",
            "Iteration 50, loss = 0.5787\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1821 / 2648 correct (68.77)\n",
            "[[1627  146]\n",
            " [ 681  194]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.92      0.80      1773\n",
            "           1       0.57      0.22      0.32       875\n",
            "\n",
            "   micro avg       0.69      0.69      0.69      2648\n",
            "   macro avg       0.64      0.57      0.56      2648\n",
            "weighted avg       0.66      0.69      0.64      2648\n",
            "\n",
            "Kappa 0.1649\n",
            "\n",
            "Epoch: 8\n",
            "Iteration 0, loss = 0.6026\n",
            "Iteration 50, loss = 0.5366\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1854 / 2648 correct (70.02)\n",
            "[[1721   52]\n",
            " [ 742  133]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.97      0.81      1773\n",
            "           1       0.72      0.15      0.25       875\n",
            "\n",
            "   micro avg       0.70      0.70      0.70      2648\n",
            "   macro avg       0.71      0.56      0.53      2648\n",
            "weighted avg       0.71      0.70      0.63      2648\n",
            "\n",
            "Kappa 0.1533\n",
            "\n",
            "Epoch: 9\n",
            "Iteration 0, loss = 0.5827\n",
            "Iteration 50, loss = 0.5228\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1849 / 2648 correct (69.83)\n",
            "[[1744   29]\n",
            " [ 770  105]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.98      0.81      1773\n",
            "           1       0.78      0.12      0.21       875\n",
            "\n",
            "   micro avg       0.70      0.70      0.70      2648\n",
            "   macro avg       0.74      0.55      0.51      2648\n",
            "weighted avg       0.72      0.70      0.61      2648\n",
            "\n",
            "Kappa 0.1319\n",
            "\n",
            "Epoch: 10\n",
            "Iteration 0, loss = 0.5982\n",
            "Iteration 50, loss = 0.5274\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1850 / 2648 correct (69.86)\n",
            "[[1753   20]\n",
            " [ 778   97]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.99      0.81      1773\n",
            "           1       0.83      0.11      0.20       875\n",
            "\n",
            "   micro avg       0.70      0.70      0.70      2648\n",
            "   macro avg       0.76      0.55      0.51      2648\n",
            "weighted avg       0.74      0.70      0.61      2648\n",
            "\n",
            "Kappa 0.1276\n",
            "\n",
            "Epoch: 11\n",
            "Iteration 0, loss = 0.5148\n",
            "Iteration 50, loss = 0.5826\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1831 / 2648 correct (69.15)\n",
            "[[1762   11]\n",
            " [ 806   69]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.99      0.81      1773\n",
            "           1       0.86      0.08      0.14       875\n",
            "\n",
            "   micro avg       0.69      0.69      0.69      2648\n",
            "   macro avg       0.77      0.54      0.48      2648\n",
            "weighted avg       0.74      0.69      0.59      2648\n",
            "\n",
            "Kappa 0.0944\n",
            "\n",
            "Epoch: 12\n",
            "Iteration 0, loss = 0.5619\n",
            "Iteration 50, loss = 0.5780\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1775 / 2648 correct (67.03)\n",
            "[[1773    0]\n",
            " [ 873    2]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      1773\n",
            "           1       1.00      0.00      0.00       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.84      0.50      0.40      2648\n",
            "weighted avg       0.78      0.67      0.54      2648\n",
            "\n",
            "Kappa 0.0031\n",
            "\n",
            "Epoch: 13\n",
            "Iteration 0, loss = 0.5608\n",
            "Iteration 50, loss = 0.4945\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1773 / 2648 correct (66.96)\n",
            "[[1773    0]\n",
            " [ 875    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      1773\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.33      0.50      0.40      2648\n",
            "weighted avg       0.45      0.67      0.54      2648\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 14\n",
            "Iteration 0, loss = 0.4938\n",
            "Iteration 50, loss = 0.5065\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1773 / 2648 correct (66.96)\n",
            "[[1773    0]\n",
            " [ 875    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      1773\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.33      0.50      0.40      2648\n",
            "weighted avg       0.45      0.67      0.54      2648\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 15\n",
            "Iteration 0, loss = 0.4948\n",
            "Iteration 50, loss = 0.4651\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1773 / 2648 correct (66.96)\n",
            "[[1773    0]\n",
            " [ 875    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      1773\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.33      0.50      0.40      2648\n",
            "weighted avg       0.45      0.67      0.54      2648\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 16\n",
            "Iteration 0, loss = 0.4661\n",
            "Iteration 50, loss = 0.4758\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1773 / 2648 correct (66.96)\n",
            "[[1773    0]\n",
            " [ 875    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      1773\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.33      0.50      0.40      2648\n",
            "weighted avg       0.45      0.67      0.54      2648\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 17\n",
            "Iteration 0, loss = 0.5305\n",
            "Iteration 50, loss = 0.4536\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1774 / 2648 correct (66.99)\n",
            "[[1772    1]\n",
            " [ 873    2]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      1773\n",
            "           1       0.67      0.00      0.00       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.67      0.50      0.40      2648\n",
            "weighted avg       0.67      0.67      0.54      2648\n",
            "\n",
            "Kappa 0.0023\n",
            "\n",
            "Epoch: 18\n",
            "Iteration 0, loss = 0.4442\n",
            "Iteration 50, loss = 0.4362\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1895 / 2648 correct (71.56)\n",
            "[[1752   21]\n",
            " [ 732  143]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.99      0.82      1773\n",
            "           1       0.87      0.16      0.28       875\n",
            "\n",
            "   micro avg       0.72      0.72      0.72      2648\n",
            "   macro avg       0.79      0.58      0.55      2648\n",
            "weighted avg       0.76      0.72      0.64      2648\n",
            "\n",
            "Kappa 0.1909\n",
            "\n",
            "Epoch: 19\n",
            "Iteration 0, loss = 0.4041\n",
            "Iteration 50, loss = 0.4989\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2027 / 2648 correct (76.55)\n",
            "[[1669  104]\n",
            " [ 517  358]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.94      0.84      1773\n",
            "           1       0.77      0.41      0.54       875\n",
            "\n",
            "   micro avg       0.77      0.77      0.77      2648\n",
            "   macro avg       0.77      0.68      0.69      2648\n",
            "weighted avg       0.77      0.77      0.74      2648\n",
            "\n",
            "Kappa 0.3981\n",
            "\n",
            "Epoch: 20\n",
            "Iteration 0, loss = 0.4519\n",
            "Iteration 50, loss = 0.4613\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2068 / 2648 correct (78.10)\n",
            "[[1605  168]\n",
            " [ 412  463]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.91      0.85      1773\n",
            "           1       0.73      0.53      0.61       875\n",
            "\n",
            "   micro avg       0.78      0.78      0.78      2648\n",
            "   macro avg       0.76      0.72      0.73      2648\n",
            "weighted avg       0.78      0.78      0.77      2648\n",
            "\n",
            "Kappa 0.4674\n",
            "\n",
            "Epoch: 21\n",
            "Iteration 0, loss = 0.4216\n",
            "Iteration 50, loss = 0.4578\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2065 / 2648 correct (77.98)\n",
            "[[1551  222]\n",
            " [ 361  514]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84      1773\n",
            "           1       0.70      0.59      0.64       875\n",
            "\n",
            "   micro avg       0.78      0.78      0.78      2648\n",
            "   macro avg       0.75      0.73      0.74      2648\n",
            "weighted avg       0.77      0.78      0.77      2648\n",
            "\n",
            "Kappa 0.4816\n",
            "\n",
            "Epoch: 22\n",
            "Iteration 0, loss = 0.3359\n",
            "Iteration 50, loss = 0.3880\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2043 / 2648 correct (77.15)\n",
            "[[1467  306]\n",
            " [ 299  576]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.83      0.83      1773\n",
            "           1       0.65      0.66      0.66       875\n",
            "\n",
            "   micro avg       0.77      0.77      0.77      2648\n",
            "   macro avg       0.74      0.74      0.74      2648\n",
            "weighted avg       0.77      0.77      0.77      2648\n",
            "\n",
            "Kappa 0.4847\n",
            "\n",
            "Epoch: 23\n",
            "Iteration 0, loss = 0.4136\n",
            "Iteration 50, loss = 0.3400\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2036 / 2648 correct (76.89)\n",
            "[[1445  328]\n",
            " [ 284  591]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.82      0.83      1773\n",
            "           1       0.64      0.68      0.66       875\n",
            "\n",
            "   micro avg       0.77      0.77      0.77      2648\n",
            "   macro avg       0.74      0.75      0.74      2648\n",
            "weighted avg       0.77      0.77      0.77      2648\n",
            "\n",
            "Kappa 0.4843\n",
            "\n",
            "Epoch: 24\n",
            "Iteration 0, loss = 0.3466\n",
            "Iteration 50, loss = 0.3578\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2026 / 2648 correct (76.51)\n",
            "[[1412  361]\n",
            " [ 261  614]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.80      0.82      1773\n",
            "           1       0.63      0.70      0.66       875\n",
            "\n",
            "   micro avg       0.77      0.77      0.77      2648\n",
            "   macro avg       0.74      0.75      0.74      2648\n",
            "weighted avg       0.77      0.77      0.77      2648\n",
            "\n",
            "Kappa 0.4841\n",
            "\n",
            "Epoch: 25\n",
            "Iteration 0, loss = 0.3140\n",
            "Iteration 50, loss = 0.4038\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2040 / 2648 correct (77.04)\n",
            "[[1437  336]\n",
            " [ 272  603]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.81      0.83      1773\n",
            "           1       0.64      0.69      0.66       875\n",
            "\n",
            "   micro avg       0.77      0.77      0.77      2648\n",
            "   macro avg       0.74      0.75      0.75      2648\n",
            "weighted avg       0.78      0.77      0.77      2648\n",
            "\n",
            "Kappa 0.4905\n",
            "\n",
            "Epoch: 26\n",
            "Iteration 0, loss = 0.3474\n",
            "Iteration 50, loss = 0.4298\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2026 / 2648 correct (76.51)\n",
            "[[1402  371]\n",
            " [ 251  624]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.79      0.82      1773\n",
            "           1       0.63      0.71      0.67       875\n",
            "\n",
            "   micro avg       0.77      0.77      0.77      2648\n",
            "   macro avg       0.74      0.75      0.74      2648\n",
            "weighted avg       0.78      0.77      0.77      2648\n",
            "\n",
            "Kappa 0.4870\n",
            "\n",
            "Epoch: 27\n",
            "Iteration 0, loss = 0.3858\n",
            "Iteration 50, loss = 0.3133\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2037 / 2648 correct (76.93)\n",
            "[[1427  346]\n",
            " [ 265  610]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.80      0.82      1773\n",
            "           1       0.64      0.70      0.67       875\n",
            "\n",
            "   micro avg       0.77      0.77      0.77      2648\n",
            "   macro avg       0.74      0.75      0.74      2648\n",
            "weighted avg       0.78      0.77      0.77      2648\n",
            "\n",
            "Kappa 0.4905\n",
            "\n",
            "Epoch: 28\n",
            "Iteration 0, loss = 0.3012\n",
            "Iteration 50, loss = 0.4676\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2031 / 2648 correct (76.70)\n",
            "[[1426  347]\n",
            " [ 270  605]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.80      0.82      1773\n",
            "           1       0.64      0.69      0.66       875\n",
            "\n",
            "   micro avg       0.77      0.77      0.77      2648\n",
            "   macro avg       0.74      0.75      0.74      2648\n",
            "weighted avg       0.77      0.77      0.77      2648\n",
            "\n",
            "Kappa 0.4849\n",
            "\n",
            "Epoch: 29\n",
            "Iteration 0, loss = 0.2906\n",
            "Iteration 50, loss = 0.3178\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2027 / 2648 correct (76.55)\n",
            "[[1398  375]\n",
            " [ 246  629]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.79      0.82      1773\n",
            "           1       0.63      0.72      0.67       875\n",
            "\n",
            "   micro avg       0.77      0.77      0.77      2648\n",
            "   macro avg       0.74      0.75      0.74      2648\n",
            "weighted avg       0.78      0.77      0.77      2648\n",
            "\n",
            "Kappa 0.4891\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S_q8EEjwwLf3",
        "colab_type": "code",
        "outputId": "372405bc-ea64-49f3-c73d-7b3dee987e69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "fig1, ax1 = plt.subplots()\n",
        "\n",
        "ax1.plot(t_losses, label='Training')\n",
        "ax1.plot(v_losses, label='Validation')\n",
        "\n",
        "ax1.set_title('Losses')\n",
        "ax1.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFZCAYAAACizedRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "VFiG4aRQPpN5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task B"
      ]
    },
    {
      "metadata": {
        "id": "2YbsX0DyP1B0",
        "colab_type": "code",
        "outputId": "33054b1d-3c9b-4cc3-df83-ee4a9f0821bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "#Select data that does not have subtask_a == \"OFF\":\n",
        "train = data.TabularDataset(train_fp, format='TSV', fields = \n",
        "                            data_fields, skip_header=True, filter_pred=lambda d: d.subtask_a == 'OFF')\n",
        "\n",
        "train, valid = train.split(split_ratio=0.8, random_state=random.seed(SEED))\n",
        "\n",
        "print(f'Train size: {len(train)}')\n",
        "print(f'Validation size: {len(valid)}')\n",
        "\n",
        "#Now build vocab (using only the training set)\n",
        "TEXT.build_vocab(train, vectors='glove.twitter.27B.200d') #USE \"glove.840B.300d\" or glove.twitter.27B.200d\n",
        "\n",
        "LABEL.build_vocab(train.subtask_b)\n",
        "\n",
        "output_dim = len(LABEL.vocab)\n",
        "\n",
        "print(LABEL.vocab.stoi)\n",
        "\n",
        "#Create iterators\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid),\n",
        "                        batch_sizes=(BATCH_SIZE, len(valid)),  \n",
        "                        sort_key=lambda x: len(x.tweet), device=device)"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 3520\n",
            "Validation size: 880\n",
            "defaultdict(<function _default_unk_index at 0x7fa37d683840>, {'TIN': 0, 'UNT': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uww_bdubSnf3",
        "colab_type": "code",
        "outputId": "a0c3a488-6a63-478a-d06a-0933df2a2e4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9103
        }
      },
      "cell_type": "code",
      "source": [
        "#CONV with Glove\n",
        "embedding_dim = 200\n",
        "window_size = 3\n",
        "lr = 0.00025\n",
        "out_channels = 512\n",
        "dropout = 0.5\n",
        "n_hidden = (64, 32, 16, 8, 4)\n",
        "\n",
        "pos_weight = torch.tensor([6.8], device = device) #deals with unbalanced classes\n",
        "\n",
        "model = ClassifierGloVe(TEXT.vocab, embedding_dim, window_size, out_channels, dropout, n_hidden = n_hidden)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "t_losses, v_losses = train_helper('subtask_b', model, optimizer, loss_fn = loss_fn, epochs = 30, \n",
        "                                  train_loader=train_iterator, valid_loader=valid_iterator)"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Iteration 0, loss = 1.3340\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 1\n",
            "Iteration 0, loss = 1.3140\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 2\n",
            "Iteration 0, loss = 1.2032\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 3\n",
            "Iteration 0, loss = 0.9265\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 4\n",
            "Iteration 0, loss = 1.1293\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 5\n",
            "Iteration 0, loss = 1.0648\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 6\n",
            "Iteration 0, loss = 1.2478\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 7\n",
            "Iteration 0, loss = 1.0509\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 8\n",
            "Iteration 0, loss = 1.1639\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 9\n",
            "Iteration 0, loss = 1.1975\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 10\n",
            "Iteration 0, loss = 1.1779\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 11\n",
            "Iteration 0, loss = 1.1894\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 12\n",
            "Iteration 0, loss = 1.1272\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 13\n",
            "Iteration 0, loss = 0.9544\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 14\n",
            "Iteration 0, loss = 1.0931\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 15\n",
            "Iteration 0, loss = 1.2710\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 16\n",
            "Iteration 0, loss = 1.0482\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 17\n",
            "Iteration 0, loss = 1.1068\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 18\n",
            "Iteration 0, loss = 1.1468\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 152 / 880 correct (17.27)\n",
            "[[ 43 725]\n",
            " [  3 109]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.06      0.11       768\n",
            "           1       0.13      0.97      0.23       112\n",
            "\n",
            "   micro avg       0.17      0.17      0.17       880\n",
            "   macro avg       0.53      0.51      0.17       880\n",
            "weighted avg       0.83      0.17      0.12       880\n",
            "\n",
            "Kappa 0.0078\n",
            "\n",
            "Epoch: 19\n",
            "Iteration 0, loss = 1.0760\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 122 / 880 correct (13.86)\n",
            "[[ 12 756]\n",
            " [  2 110]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.02      0.03       768\n",
            "           1       0.13      0.98      0.22       112\n",
            "\n",
            "   micro avg       0.14      0.14      0.14       880\n",
            "   macro avg       0.49      0.50      0.13       880\n",
            "weighted avg       0.76      0.14      0.06       880\n",
            "\n",
            "Kappa -0.0006\n",
            "\n",
            "Epoch: 20\n",
            "Iteration 0, loss = 0.9968\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 310 / 880 correct (35.23)\n",
            "[[215 553]\n",
            " [ 17  95]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.28      0.43       768\n",
            "           1       0.15      0.85      0.25       112\n",
            "\n",
            "   micro avg       0.35      0.35      0.35       880\n",
            "   macro avg       0.54      0.56      0.34       880\n",
            "weighted avg       0.83      0.35      0.41       880\n",
            "\n",
            "Kappa 0.0421\n",
            "\n",
            "Epoch: 21\n",
            "Iteration 0, loss = 0.9404\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 651 / 880 correct (73.98)\n",
            "[[601 167]\n",
            " [ 62  50]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.78      0.84       768\n",
            "           1       0.23      0.45      0.30       112\n",
            "\n",
            "   micro avg       0.74      0.74      0.74       880\n",
            "   macro avg       0.57      0.61      0.57       880\n",
            "weighted avg       0.82      0.74      0.77       880\n",
            "\n",
            "Kappa 0.1635\n",
            "\n",
            "Epoch: 22\n",
            "Iteration 0, loss = 1.0275\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 649 / 880 correct (73.75)\n",
            "[[598 170]\n",
            " [ 61  51]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.78      0.84       768\n",
            "           1       0.23      0.46      0.31       112\n",
            "\n",
            "   micro avg       0.74      0.74      0.74       880\n",
            "   macro avg       0.57      0.62      0.57       880\n",
            "weighted avg       0.82      0.74      0.77       880\n",
            "\n",
            "Kappa 0.1653\n",
            "\n",
            "Epoch: 23\n",
            "Iteration 0, loss = 1.2659\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 683 / 880 correct (77.61)\n",
            "[[637 131]\n",
            " [ 66  46]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.83      0.87       768\n",
            "           1       0.26      0.41      0.32       112\n",
            "\n",
            "   micro avg       0.78      0.78      0.78       880\n",
            "   macro avg       0.58      0.62      0.59       880\n",
            "weighted avg       0.82      0.78      0.80       880\n",
            "\n",
            "Kappa 0.1924\n",
            "\n",
            "Epoch: 24\n",
            "Iteration 0, loss = 1.2817\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 762 / 880 correct (86.59)\n",
            "[[732  36]\n",
            " [ 82  30]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.95      0.93       768\n",
            "           1       0.45      0.27      0.34       112\n",
            "\n",
            "   micro avg       0.87      0.87      0.87       880\n",
            "   macro avg       0.68      0.61      0.63       880\n",
            "weighted avg       0.84      0.87      0.85       880\n",
            "\n",
            "Kappa 0.2680\n",
            "\n",
            "Epoch: 25\n",
            "Iteration 0, loss = 1.0795\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 769 / 880 correct (87.39)\n",
            "[[757  11]\n",
            " [100  12]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.99      0.93       768\n",
            "           1       0.52      0.11      0.18       112\n",
            "\n",
            "   micro avg       0.87      0.87      0.87       880\n",
            "   macro avg       0.70      0.55      0.55       880\n",
            "weighted avg       0.84      0.87      0.84       880\n",
            "\n",
            "Kappa 0.1405\n",
            "\n",
            "Epoch: 26\n",
            "Iteration 0, loss = 1.1683\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 769 / 880 correct (87.39)\n",
            "[[759   9]\n",
            " [102  10]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.99      0.93       768\n",
            "           1       0.53      0.09      0.15       112\n",
            "\n",
            "   micro avg       0.87      0.87      0.87       880\n",
            "   macro avg       0.70      0.54      0.54       880\n",
            "weighted avg       0.84      0.87      0.83       880\n",
            "\n",
            "Kappa 0.1202\n",
            "\n",
            "Epoch: 27\n",
            "Iteration 0, loss = 1.1371\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 767 / 880 correct (87.16)\n",
            "[[751  17]\n",
            " [ 96  16]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.98      0.93       768\n",
            "           1       0.48      0.14      0.22       112\n",
            "\n",
            "   micro avg       0.87      0.87      0.87       880\n",
            "   macro avg       0.69      0.56      0.58       880\n",
            "weighted avg       0.84      0.87      0.84       880\n",
            "\n",
            "Kappa 0.1728\n",
            "\n",
            "Epoch: 28\n",
            "Iteration 0, loss = 1.1649\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 769 / 880 correct (87.39)\n",
            "[[765   3]\n",
            " [108   4]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      1.00      0.93       768\n",
            "           1       0.57      0.04      0.07       112\n",
            "\n",
            "   micro avg       0.87      0.87      0.87       880\n",
            "   macro avg       0.72      0.52      0.50       880\n",
            "weighted avg       0.84      0.87      0.82       880\n",
            "\n",
            "Kappa 0.0530\n",
            "\n",
            "Epoch: 29\n",
            "Iteration 0, loss = 1.1245\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 767 / 880 correct (87.16)\n",
            "[[767   1]\n",
            " [112   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      1.00      0.93       768\n",
            "           1       0.00      0.00      0.00       112\n",
            "\n",
            "   micro avg       0.87      0.87      0.87       880\n",
            "   macro avg       0.44      0.50      0.47       880\n",
            "weighted avg       0.76      0.87      0.81       880\n",
            "\n",
            "Kappa -0.0023\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AiLub0TBVF_i",
        "colab_type": "code",
        "outputId": "24b14697-f0d1-4380-8f7f-fe716490e63f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "fig1, ax1 = plt.subplots()\n",
        "\n",
        "ax1.plot(t_losses, label='Training')\n",
        "ax1.plot(v_losses, label='Validation')\n",
        "\n",
        "ax1.set_title('Losses')\n",
        "ax1.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFZCAYAAACizedRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "HzDUdTT8Agxo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Task C"
      ]
    },
    {
      "metadata": {
        "id": "Wf47-PIDAlIw",
        "colab_type": "code",
        "outputId": "781397f0-02a2-402c-8a9c-daea8a0e5fef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train = data.TabularDataset(train_fp, format='TSV', fields = \n",
        "                            data_fields, skip_header=True, filter_pred=lambda d: d.subtask_a == 'OFF' and d.subtask_b == 'TIN')\n",
        "\n",
        "train, valid = train.split(split_ratio=0.8, random_state=random.seed(SEED))\n",
        "\n",
        "print(f'Train size: {len(train)}')\n",
        "print(f'Validation size: {len(valid)}')\n",
        "\n",
        "#Now build vocab (using only the training set)\n",
        "TEXT.build_vocab(train, vectors='glove.twitter.27B.200d') #USE \"glove.840B.300d\" or glove.twitter.27B.200d\n",
        "\n",
        "LABEL.build_vocab(train.subtask_c)\n",
        "\n",
        "output_dim = len(LABEL.vocab)\n",
        "\n",
        "print(LABEL.vocab.stoi)\n",
        "\n",
        "#Create iterators\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid),\n",
        "                        batch_sizes=(BATCH_SIZE, len(valid)),  \n",
        "                        sort_key=lambda x: len(x.tweet), device=device)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 3101\n",
            "Validation size: 775\n",
            "defaultdict(<function _default_unk_index at 0x7fa37d683840>, {'IND': 0, 'GRP': 1, 'OTH': 2})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UTUbzJZBBBMr",
        "colab_type": "code",
        "outputId": "90ddda19-b833-484e-ddad-78ee5fb9b7ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6708
        }
      },
      "cell_type": "code",
      "source": [
        "#CONV with Glove\n",
        "embedding_dim = 200\n",
        "window_size = 3\n",
        "lr = 0.00025\n",
        "out_channels = 512\n",
        "dropout = 0.5\n",
        "weight = torch.tensor([1.6, 3.7 ,8.4], device = device) #deals with unbalanced classes\n",
        "\n",
        "model = SimpleClassifierGloVe(TEXT.vocab, embedding_dim, window_size, out_channels, dropout, num_classes=3)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "loss_fn = nn.CrossEntropyLoss(weight = weight)\n",
        "\n",
        "\n",
        "t_losses, v_losses = train_helper('subtask_c', model, optimizer, loss_fn = loss_fn, epochs = 20, train_loader=train_iterator, valid_loader=valid_iterator)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Iteration 0, loss = 2.1998\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 371 / 775 correct (47.87)\n",
            "[[209 234  33]\n",
            " [ 38 149  20]\n",
            " [ 23  56  13]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.44      0.56       476\n",
            "           1       0.34      0.72      0.46       207\n",
            "           2       0.20      0.14      0.16        92\n",
            "\n",
            "   micro avg       0.48      0.48      0.48       775\n",
            "   macro avg       0.44      0.43      0.40       775\n",
            "weighted avg       0.59      0.48      0.49       775\n",
            "\n",
            "Kappa 0.1654\n",
            "\n",
            "Epoch: 1\n",
            "Iteration 0, loss = 1.3425\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 445 / 775 correct (57.42)\n",
            "[[288 155  33]\n",
            " [ 45 141  21]\n",
            " [ 31  45  16]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.61      0.69       476\n",
            "           1       0.41      0.68      0.51       207\n",
            "           2       0.23      0.17      0.20        92\n",
            "\n",
            "   micro avg       0.57      0.57      0.57       775\n",
            "   macro avg       0.48      0.49      0.47       775\n",
            "weighted avg       0.62      0.57      0.58       775\n",
            "\n",
            "Kappa 0.2700\n",
            "\n",
            "Epoch: 2\n",
            "Iteration 0, loss = 1.0534\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 473 / 775 correct (61.03)\n",
            "[[324 128  24]\n",
            " [ 53 140  14]\n",
            " [ 35  48   9]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.68      0.73       476\n",
            "           1       0.44      0.68      0.54       207\n",
            "           2       0.19      0.10      0.13        92\n",
            "\n",
            "   micro avg       0.61      0.61      0.61       775\n",
            "   macro avg       0.47      0.48      0.46       775\n",
            "weighted avg       0.62      0.61      0.61       775\n",
            "\n",
            "Kappa 0.3009\n",
            "\n",
            "Epoch: 3\n",
            "Iteration 0, loss = 1.4057\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 504 / 775 correct (65.03)\n",
            "[[359 110   7]\n",
            " [ 57 140  10]\n",
            " [ 38  49   5]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.75      0.77       476\n",
            "           1       0.47      0.68      0.55       207\n",
            "           2       0.23      0.05      0.09        92\n",
            "\n",
            "   micro avg       0.65      0.65      0.65       775\n",
            "   macro avg       0.50      0.49      0.47       775\n",
            "weighted avg       0.64      0.65      0.63       775\n",
            "\n",
            "Kappa 0.3449\n",
            "\n",
            "Epoch: 4\n",
            "Iteration 0, loss = 1.0962\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 485 / 775 correct (62.58)\n",
            "[[332 101  43]\n",
            " [ 47 134  26]\n",
            " [ 32  41  19]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.70      0.75       476\n",
            "           1       0.49      0.65      0.55       207\n",
            "           2       0.22      0.21      0.21        92\n",
            "\n",
            "   micro avg       0.63      0.63      0.63       775\n",
            "   macro avg       0.50      0.52      0.50       775\n",
            "weighted avg       0.65      0.63      0.63       775\n",
            "\n",
            "Kappa 0.3385\n",
            "\n",
            "Epoch: 5\n",
            "Iteration 0, loss = 0.9045\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 477 / 775 correct (61.55)\n",
            "[[305 154  17]\n",
            " [ 31 164  12]\n",
            " [ 32  52   8]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.64      0.72       476\n",
            "           1       0.44      0.79      0.57       207\n",
            "           2       0.22      0.09      0.12        92\n",
            "\n",
            "   micro avg       0.62      0.62      0.62       775\n",
            "   macro avg       0.50      0.51      0.47       775\n",
            "weighted avg       0.65      0.62      0.61       775\n",
            "\n",
            "Kappa 0.3315\n",
            "\n",
            "Epoch: 6\n",
            "Iteration 0, loss = 1.1067\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 502 / 775 correct (64.77)\n",
            "[[351  91  34]\n",
            " [ 52 130  25]\n",
            " [ 34  37  21]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.74      0.77       476\n",
            "           1       0.50      0.63      0.56       207\n",
            "           2       0.26      0.23      0.24        92\n",
            "\n",
            "   micro avg       0.65      0.65      0.65       775\n",
            "   macro avg       0.52      0.53      0.52       775\n",
            "weighted avg       0.66      0.65      0.65       775\n",
            "\n",
            "Kappa 0.3624\n",
            "\n",
            "Epoch: 7\n",
            "Iteration 0, loss = 0.9240\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 505 / 775 correct (65.16)\n",
            "[[359  79  38]\n",
            " [ 57 123  27]\n",
            " [ 37  32  23]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.75      0.77       476\n",
            "           1       0.53      0.59      0.56       207\n",
            "           2       0.26      0.25      0.26        92\n",
            "\n",
            "   micro avg       0.65      0.65      0.65       775\n",
            "   macro avg       0.53      0.53      0.53       775\n",
            "weighted avg       0.66      0.65      0.65       775\n",
            "\n",
            "Kappa 0.3629\n",
            "\n",
            "Epoch: 8\n",
            "Iteration 0, loss = 0.8219\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 516 / 775 correct (66.58)\n",
            "[[363 101  12]\n",
            " [ 52 142  13]\n",
            " [ 37  44  11]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.76      0.78       476\n",
            "           1       0.49      0.69      0.57       207\n",
            "           2       0.31      0.12      0.17        92\n",
            "\n",
            "   micro avg       0.67      0.67      0.67       775\n",
            "   macro avg       0.53      0.52      0.51       775\n",
            "weighted avg       0.66      0.67      0.65       775\n",
            "\n",
            "Kappa 0.3781\n",
            "\n",
            "Epoch: 9\n",
            "Iteration 0, loss = 0.8320\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 506 / 775 correct (65.29)\n",
            "[[339 118  19]\n",
            " [ 35 155  17]\n",
            " [ 34  46  12]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.71      0.77       476\n",
            "           1       0.49      0.75      0.59       207\n",
            "           2       0.25      0.13      0.17        92\n",
            "\n",
            "   micro avg       0.65      0.65      0.65       775\n",
            "   macro avg       0.52      0.53      0.51       775\n",
            "weighted avg       0.67      0.65      0.65       775\n",
            "\n",
            "Kappa 0.3795\n",
            "\n",
            "Epoch: 10\n",
            "Iteration 0, loss = 0.9228\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 515 / 775 correct (66.45)\n",
            "[[375  77  24]\n",
            " [ 61 124  22]\n",
            " [ 41  35  16]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.79      0.79       476\n",
            "           1       0.53      0.60      0.56       207\n",
            "           2       0.26      0.17      0.21        92\n",
            "\n",
            "   micro avg       0.66      0.66      0.66       775\n",
            "   macro avg       0.52      0.52      0.52       775\n",
            "weighted avg       0.65      0.66      0.66       775\n",
            "\n",
            "Kappa 0.3684\n",
            "\n",
            "Epoch: 11\n",
            "Iteration 0, loss = 0.6742\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 516 / 775 correct (66.58)\n",
            "[[359  97  20]\n",
            " [ 47 144  16]\n",
            " [ 36  43  13]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.75      0.78       476\n",
            "           1       0.51      0.70      0.59       207\n",
            "           2       0.27      0.14      0.18        92\n",
            "\n",
            "   micro avg       0.67      0.67      0.67       775\n",
            "   macro avg       0.53      0.53      0.52       775\n",
            "weighted avg       0.67      0.67      0.66       775\n",
            "\n",
            "Kappa 0.3860\n",
            "\n",
            "Epoch: 12\n",
            "Iteration 0, loss = 0.6467\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 509 / 775 correct (65.68)\n",
            "[[348 100  28]\n",
            " [ 42 141  24]\n",
            " [ 34  38  20]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.73      0.77       476\n",
            "           1       0.51      0.68      0.58       207\n",
            "           2       0.28      0.22      0.24        92\n",
            "\n",
            "   micro avg       0.66      0.66      0.66       775\n",
            "   macro avg       0.53      0.54      0.53       775\n",
            "weighted avg       0.67      0.66      0.66       775\n",
            "\n",
            "Kappa 0.3836\n",
            "\n",
            "Epoch: 13\n",
            "Iteration 0, loss = 0.5908\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 513 / 775 correct (66.19)\n",
            "[[371  88  17]\n",
            " [ 59 131  17]\n",
            " [ 41  40  11]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.78      0.78       476\n",
            "           1       0.51      0.63      0.56       207\n",
            "           2       0.24      0.12      0.16        92\n",
            "\n",
            "   micro avg       0.66      0.66      0.66       775\n",
            "   macro avg       0.51      0.51      0.50       775\n",
            "weighted avg       0.65      0.66      0.65       775\n",
            "\n",
            "Kappa 0.3628\n",
            "\n",
            "Epoch: 14\n",
            "Iteration 0, loss = 0.5326\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 517 / 775 correct (66.71)\n",
            "[[358  99  19]\n",
            " [ 44 146  17]\n",
            " [ 35  44  13]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.75      0.78       476\n",
            "           1       0.51      0.71      0.59       207\n",
            "           2       0.27      0.14      0.18        92\n",
            "\n",
            "   micro avg       0.67      0.67      0.67       775\n",
            "   macro avg       0.53      0.53      0.52       775\n",
            "weighted avg       0.67      0.67      0.66       775\n",
            "\n",
            "Kappa 0.3909\n",
            "\n",
            "Epoch: 15\n",
            "Iteration 0, loss = 0.4735\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 512 / 775 correct (66.06)\n",
            "[[366  89  21]\n",
            " [ 54 131  22]\n",
            " [ 36  41  15]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.77      0.79       476\n",
            "           1       0.50      0.63      0.56       207\n",
            "           2       0.26      0.16      0.20        92\n",
            "\n",
            "   micro avg       0.66      0.66      0.66       775\n",
            "   macro avg       0.52      0.52      0.52       775\n",
            "weighted avg       0.66      0.66      0.66       775\n",
            "\n",
            "Kappa 0.3713\n",
            "\n",
            "Epoch: 16\n",
            "Iteration 0, loss = 0.5040\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 518 / 775 correct (66.84)\n",
            "[[363  93  20]\n",
            " [ 50 141  16]\n",
            " [ 36  42  14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.76      0.78       476\n",
            "           1       0.51      0.68      0.58       207\n",
            "           2       0.28      0.15      0.20        92\n",
            "\n",
            "   micro avg       0.67      0.67      0.67       775\n",
            "   macro avg       0.53      0.53      0.52       775\n",
            "weighted avg       0.67      0.67      0.66       775\n",
            "\n",
            "Kappa 0.3875\n",
            "\n",
            "Epoch: 17\n",
            "Iteration 0, loss = 0.4169\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 517 / 775 correct (66.71)\n",
            "[[362  93  21]\n",
            " [ 48 140  19]\n",
            " [ 36  41  15]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.76      0.79       476\n",
            "           1       0.51      0.68      0.58       207\n",
            "           2       0.27      0.16      0.20        92\n",
            "\n",
            "   micro avg       0.67      0.67      0.67       775\n",
            "   macro avg       0.53      0.53      0.52       775\n",
            "weighted avg       0.67      0.67      0.66       775\n",
            "\n",
            "Kappa 0.3877\n",
            "\n",
            "Epoch: 18\n",
            "Iteration 0, loss = 0.4618\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 515 / 775 correct (66.45)\n",
            "[[366  90  20]\n",
            " [ 53 136  18]\n",
            " [ 37  42  13]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.77      0.79       476\n",
            "           1       0.51      0.66      0.57       207\n",
            "           2       0.25      0.14      0.18        92\n",
            "\n",
            "   micro avg       0.66      0.66      0.66       775\n",
            "   macro avg       0.52      0.52      0.51       775\n",
            "weighted avg       0.66      0.66      0.66       775\n",
            "\n",
            "Kappa 0.3769\n",
            "\n",
            "Epoch: 19\n",
            "Iteration 0, loss = 0.5058\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 524 / 775 correct (67.61)\n",
            "[[373  85  18]\n",
            " [ 53 138  16]\n",
            " [ 38  41  13]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.78      0.79       476\n",
            "           1       0.52      0.67      0.59       207\n",
            "           2       0.28      0.14      0.19        92\n",
            "\n",
            "   micro avg       0.68      0.68      0.68       775\n",
            "   macro avg       0.53      0.53      0.52       775\n",
            "weighted avg       0.67      0.68      0.67       775\n",
            "\n",
            "Kappa 0.3936\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TSbEtokDP1Dy",
        "colab_type": "code",
        "outputId": "10f39def-a9c1-4cd6-8cfa-1b3d5c781414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "fig1, ax1 = plt.subplots()\n",
        "\n",
        "ax1.plot(t_losses, label='Training')\n",
        "ax1.plot(v_losses, label='Validation')\n",
        "\n",
        "ax1.set_title('Losses')\n",
        "ax1.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFZCAYAAACv05cWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "zza6aQ1QnMyy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9v2KyvFUQlXi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}