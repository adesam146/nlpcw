{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of NLP_CW.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "xGMVF5KTg-He",
        "t9Zt3py7E1ep"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adesam146/nlpcw/blob/rest_of_tasks_playground/NLP_CW_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "_i_qSkEMxlkg"
      },
      "cell_type": "markdown",
      "source": [
        "## Check GPU memory"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5-XwNX-831V6",
        "outputId": "13a98e80-4221-4a16-9a8d-040a05940c69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "#Check GPU Memory allocation\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NOXcwqriwFsu",
        "outputId": "142c5174-73eb-481d-d282-5961b6fbe93c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() "
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 11.5 GB  | Proc size: 2.4 GB\n",
            "GPU RAM Free: 11121MB | Used: 320MB | Util   3% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ecWOCoFgxS_j",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#run this if GPU utilization is not 0%\n",
        "# !kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "wTfeo8tcxhwC"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ePuqIHSPf554",
        "outputId": "c82aeebb-4c4e-499a-dda1-e4e40833343a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U spacy ftfy torchtext\n",
        "!python -m spacy download en\n",
        "!pip install -U textblob #Sentiment analysis\n",
        "!python -m textblob.download_corpora\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spacy in /usr/local/lib/python3.6/dist-packages (2.0.18)\n",
            "Requirement already up-to-date: ftfy in /usr/local/lib/python3.6/dist-packages (5.5.1)\n",
            "Requirement already up-to-date: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.18.4)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.9)\n",
            "Requirement already satisfied, skipping upgrade: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy) (2018.1.10)\n",
            "Requirement already satisfied, skipping upgrade: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.35)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.16.2)\n",
            "Requirement already satisfied, skipping upgrade: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (6.12.1)\n",
            "Requirement already satisfied, skipping upgrade: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy) (0.1.7)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.0.1.post2)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.11.29)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
            "Requirement already satisfied, skipping upgrade: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.10.11)\n",
            "Requirement already satisfied, skipping upgrade: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.4.3.2)\n",
            "Requirement already satisfied, skipping upgrade: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.9.0.1)\n",
            "Requirement already satisfied, skipping upgrade: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.5.6)\n",
            "Requirement already satisfied, skipping upgrade: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy) (0.9.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n",
            "Requirement already up-to-date: textblob in /usr/local/lib/python3.6/dist-packages (0.15.3)\n",
            "Requirement already satisfied, skipping upgrade: nltk>=3.1 in /usr/local/lib/python3.6/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.1->textblob) (1.11.0)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "Finished.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6NVQcb0MKUCh",
        "outputId": "63b4948f-ab77-4d24-90c8-f39b563af5a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "cell_type": "code",
      "source": [
        "# Use two GloVe trained on two different corpuses for comparison:\n",
        "    # Glove.6B\n",
        "    # glove.twitter.27B\n",
        "#!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "!unzip glove.twitter.27B.zip"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.twitter.27B.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of glove.twitter.27B.zip or\n",
            "        glove.twitter.27B.zip.zip, and cannot find glove.twitter.27B.zip.ZIP, period.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Srpq8hYt4whg",
        "outputId": "4c90b3cd-e28a-4f1c-9203-7a197d92a479",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data import sampler\n",
        "from torch import autograd\n",
        "import spacy\n",
        "from torchtext import data\n",
        "from torchtext import datasets as nlp_dset\n",
        "import random\n",
        "from sklearn.utils import resample\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import string\n",
        "from sklearn import metrics\n",
        "\n",
        "import torchvision.transforms as T\n",
        "\n",
        "nlp_spaCy = spacy.load('en', disable=[\"tagger\", \"parser\", \"ner\"])\n",
        "\n",
        "#stopwords\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stops_nltk = list(stopwords.words('english'))\n",
        "stops_sklearn = list(ENGLISH_STOP_WORDS)\n",
        "STOPWORDS = list(set(stops_nltk + stops_sklearn))\n",
        "\n",
        "#GPU\n",
        "GPU = True\n",
        "device_idx = 0\n",
        "if GPU:\n",
        "    device = torch.device(\"cuda:\"+str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "#Fix all seeds\n",
        "SEED = 0\n",
        "\n",
        "def set_seed(seed=SEED):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "set_seed()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Using device: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qtiwRhtm3s87",
        "outputId": "cd59e8c9-6e9e-4241-f773-5141d9849e40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "# Load datafiles from own google drive - EDIT AS NECESSARY\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "train_fp = \"\"\"/content/drive/My Drive/colab_data/offenseval-training-v1.tsv\"\"\"\n",
        "train_df = pd.read_csv(train_fp, delimiter=\"\\t\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "t9Zt3py7E1ep"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aMY0mUyknLDu",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenize_params(text, params):\n",
        "    \"\"\"tokenizes, and optionally also:\n",
        "        1) replaces words with their lemmatized forms\n",
        "        2) removes punctuation\n",
        "        3) removes english stopwords. \n",
        "    \"\"\"\n",
        "    \n",
        "    lemmatize = params.get(\"lemmatize\")\n",
        "    rem_punct = params.get(\"rem_punct\")\n",
        "    rem_stopwords = params.get(\"rem_stopwords\")\n",
        "    \n",
        "    #deal with stopwords\n",
        "    if rem_stopwords:\n",
        "        stopwords = STOPWORDS\n",
        "    else:\n",
        "        stopwords = []\n",
        "    \n",
        "    #deal with no punctuation\n",
        "    if rem_punct:\n",
        "        stoptokens = [x for x in list(string.punctuation) if x not in list(\"#$&*@\")]\n",
        "        stoptokens += stopwords\n",
        "    else:\n",
        "        stoptokens = stopwords \n",
        "    #stoptokens will be removed from tokens\n",
        "        \n",
        "    #replace each sentence with its lemmatized counterpart\n",
        "    if not lemmatize:\n",
        "        result = [tok.text for tok in nlp_spaCy.tokenizer(text) if tok.text not in stoptokens]\n",
        "    else:\n",
        "        #otherwise: lemmatize\n",
        "        tweet = nlp_spaCy(text)  #SpaCy tokenizes and does POS and lemmatization on tokens \n",
        "        tokens = []\n",
        "        for counter, token in enumerate(tweet):\n",
        "            if token.lemma_ == \"-PRON-\":         #treat pronouns differently as SpaCy replaces all of them with \"-PRON-\"\n",
        "                tokens.append(token.text)        #which therefore becomes a common token that biases results\n",
        "            #For everything else, add the lemma:\n",
        "            else:\n",
        "                tokens.append(token.lemma_)\n",
        "        result = [tok for tok in tokens if tok not in stoptokens]\n",
        "    return result\n",
        "\n",
        "def tokenizer(text):\n",
        "    \"\"\"wrapper for tokenize so it can be used in torchtext Field creation. It takes the \n",
        "    current global varaiable TOKENIZE_PARAMS\"\"\"\n",
        "    try:\n",
        "        params = TOKENIZE_PARAMS\n",
        "    except NameError:\n",
        "        print(\"You must initialize the global variable TOKENIZE_PARAMS\")\n",
        "        raise NameError\n",
        "    return tokenize_params(text, params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZlBy4TS6JqW1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Add sentiment\n",
        "def add_sentiment(h, ids):\n",
        "    \"\"\"Calculate tweet sentiment and concatenate this feature onto the \n",
        "    feature matrix (typically just before the fully connected layers). \n",
        "    The feature matrix dimensions will be changed as follows: \n",
        "        (B, O) -> (B, O + 2)\n",
        "        where B = batch size, O = out channels\n",
        "    \"\"\"\n",
        "\n",
        "    assert type(ids) == torch.Tensor, \"If sentiment == True, ids must be of type tensor\"\n",
        "\n",
        "    #retrieve tweets using id:\n",
        "    tweets = train_df[train_df[\"id\"].isin(ids.cpu().numpy())]\n",
        "\n",
        "    sentiments = []\n",
        "    subjectivities = []\n",
        "    \n",
        "    #extract \"sentiment\" and \"subjectivity\" according to TextBlob:\n",
        "    sentiments, subjectivities = get_sentiment_v(tweets[\"tweet\"].values)\n",
        "\n",
        "    sentiments = torch.tensor(sentiments, device=device).type(torch.float32).unsqueeze(1)\n",
        "    subjectivities = torch.tensor(subjectivities, device=device).type(torch.float32).unsqueeze(1)\n",
        "    \n",
        "    \n",
        "    h = torch.cat([h, sentiments, subjectivities], dim=1)\n",
        "    #(batch size, out channels + 2)\n",
        "\n",
        "    \n",
        "    return h\n",
        "\n",
        "def get_sentiment(text):\n",
        "    \"\"\"Gets sentiment and subjectivity of text\"\"\"\n",
        "    blob = TextBlob(text)\n",
        "    sentiment = blob.sentiment.polarity\n",
        "    subjectivity = blob.sentiment.subjectivity\n",
        "    return sentiment, subjectivity\n",
        "\n",
        "#create vectorized implementation for speed\n",
        "get_sentiment_v = np.vectorize(get_sentiment, otypes = [\"float\", \"float\"], doc= \"vectorized version of get_sentiment()\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9gUDCLc_A7uR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Models"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PeisH53s6cfR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#embedding (lookup layer) layer\n",
        "class SimpleClassifierGloVe(nn.Module):\n",
        "    \"\"\"Glove CNN\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab, embedding_dim, window_size, out_channels, dropout, \n",
        "                 num_classes=2, sentiment=False, n_hidden = 64):\n",
        "        \n",
        "        super(SimpleClassifierGloVe, self).__init__()\n",
        "        self.sentiment = sentiment\n",
        "        if sentiment == True:\n",
        "            added_features = 2\n",
        "        else:\n",
        "            added_features = 0\n",
        "            \n",
        "        self.vocab = vocab\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
        "        self.embedding.weight.data.copy_(vocab.vectors) # copies pre-trained word vectors\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(window_size, embedding_dim))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.fc1 = nn.Linear(out_channels + added_features, n_hidden)\n",
        "        self.fc2 = nn.Linear(n_hidden, 1 if num_classes == 2 else num_classes)\n",
        "        \n",
        "        \n",
        "        self.lReLU = nn.LeakyReLU(negative_slope=0.05, inplace=False)\n",
        "        \n",
        "        #Kaming normalization\n",
        "        nn.init.kaiming_normal_(self.conv.weight)\n",
        "        nn.init.kaiming_normal_(self.fc1.weight)\n",
        "        nn.init.kaiming_normal_(self.fc2.weight)\n",
        "        \n",
        "        \n",
        "    def forward(self, x, ids = None):\n",
        "        \n",
        "        embedded = self.embedding(x)\n",
        "            \n",
        "        #(batch size, max sent length, embedding dim)\n",
        "        \n",
        "        \n",
        "        #images have 3 RGB channels \n",
        "        #for the text we add 1 channel\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        #(batch size, 1, max sent length, embedding dim)\n",
        "        \n",
        "        feature_maps =  self.lReLU(self.conv(embedded).squeeze(3))\n",
        "        # (batch size, out_channels, max sent length - window size +1, 1)\n",
        "        # -> (batch size, out_channels, max sent length - window size +1)\n",
        "           \n",
        "        #the max pooling layer\n",
        "        h = F.max_pool1d(feature_maps, feature_maps.shape[2]).squeeze(2)\n",
        "        # (batch size, out_channels) \n",
        "        \n",
        "        if self.sentiment: #then add sentiment\n",
        "            h = add_sentiment(h, ids)\n",
        "            \n",
        "        h = self.lReLU(self.fc1( self.dropout(h)))\n",
        "        \n",
        "        h = self.fc2(self.dropout(h))\n",
        "       \n",
        "        \n",
        "        return h\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sFC6hi13DGnK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#embedding (lookup layer) layer\n",
        "class ClassifierGloVeDeep(nn.Module):\n",
        "    \"\"\"Glove w. 2d conv, 2 layers\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab, embedding_dim, window_size, out_channels = 128, \n",
        "                 dropout=0.5, n_hidden = 64, num_classes=2, sentiment=False):\n",
        "        \n",
        "        super(ClassifierGloVeDeep, self).__init__()\n",
        "        self.sentiment = sentiment\n",
        "        if sentiment == True:\n",
        "            added_features = 2 #This will update number of fully connected layers\n",
        "        else:\n",
        "            added_features = 0\n",
        "            \n",
        "        self.vocab = vocab\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
        "        self.embedding.weight.data.copy_(vocab.vectors) # copies pre-trained word vectors\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(window_size, embedding_dim))\n",
        "        \n",
        "        self.fc1 = nn.Linear(out_channels + added_features, n_hidden[0])\n",
        "        self.fc2 = nn.Linear(n_hidden[0], n_hidden[1])\n",
        "        self.fc3 = nn.Linear(n_hidden[1], n_hidden[2])\n",
        "        self.fc4 = nn.Linear(n_hidden[2], n_hidden[3])\n",
        "        self.fc5 = nn.Linear(n_hidden[3], n_hidden[4])\n",
        "        self.fc6 = nn.Linear(n_hidden[4], 1 if num_classes == 2 else num_classes)\n",
        "        \n",
        "        #Activation: use leaky relu\n",
        "        self.lReLU = nn.LeakyReLU(negative_slope=0.05, inplace=False)\n",
        "        \n",
        "        #Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        #Kaming normalization\n",
        "        nn.init.kaiming_normal_(self.conv1.weight)\n",
        "        nn.init.kaiming_normal_(self.fc1.weight)\n",
        "        nn.init.kaiming_normal_(self.fc2.weight)\n",
        "        \n",
        "        \n",
        "    def forward(self, x, ids=None):\n",
        "        \n",
        "        embedded = self.embedding(x)\n",
        "                \n",
        "       \n",
        "        embedded = embedded.unsqueeze(1)\n",
        "       \n",
        "        \n",
        "        feature_maps =  self.lReLU(self.conv1(embedded).squeeze(3))\n",
        "        \n",
        "       \n",
        "        h = F.max_pool1d(feature_maps, feature_maps.shape[2]).squeeze(2)\n",
        "        \n",
        "        if self.sentiment: #then add sentiment\n",
        "            h = add_sentiment(h, ids)\n",
        "        \n",
        "        h = self.lReLU(self.fc1( self.dropout(h)))\n",
        "        h = self.lReLU(self.fc2( self.dropout(h)))\n",
        "        h = self.lReLU(self.fc3( self.dropout(h)))\n",
        "        h = self.lReLU(self.fc4( self.dropout(h)))\n",
        "        h = self.lReLU(self.fc5( self.dropout(h)))\n",
        "        h = self.fc6( self.dropout(h))\n",
        "        \n",
        "        \n",
        "        return h\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6-FK3OFvGMhw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#embedding (lookup layer) layer\n",
        "class BidirectionalGRU(nn.Module):\n",
        "    \"\"\"Bidirectional GRU (i.e. RNN with memory). \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab, embedding_dim, window_size, out_channels = 128, \n",
        "                 dropout=0.5, n_hidden = 64, num_classes=2, batch_size = BATCH_SIZE, \n",
        "                 sentiment= False, ids=None):\n",
        "        \n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "        \n",
        "        self.sentiment = sentiment\n",
        "        if sentiment == True:\n",
        "            added_features = 2 #This will update number of fully connected layers\n",
        "        else:\n",
        "            added_features = 0\n",
        "            \n",
        "            \n",
        "        self.n_hidden = n_hidden\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.num_layers = 1 #number of GRU layers\n",
        "        \n",
        "        self.vocab = vocab\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
        "        self.embedding.weight.data.copy_(vocab.vectors) # copies pre-trained word vectors\n",
        "        \n",
        "        self.bi_gru =  torch.nn.GRU(input_size=embedding_dim, hidden_size=n_hidden, \n",
        "                                    num_layers= self.num_layers, batch_first=False, \n",
        "                                    bidirectional=True)\n",
        "        self.hidden = self.init_hidden(self.batch_size)\n",
        "    \n",
        "        #Fully connected layer will convert GRU output into a label\n",
        "        # number of input features is 2 * n_hidden since GRU is bidirectional\n",
        "        \n",
        "        self.fc1 = nn.Linear( 2 * n_hidden + added_features, 16)\n",
        "        self.fc2 = nn.Linear(16, 1 if num_classes == 2 else num_classes)\n",
        "\n",
        "        #Activation: use leaky relu\n",
        "        self.lReLU = nn.LeakyReLU(negative_slope=0.05, inplace=False)\n",
        "        \n",
        "        #Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        #Kaming normalization\n",
        "        nn.init.kaiming_normal_(self.fc1.weight)\n",
        "        nn.init.kaiming_normal_(self.fc2.weight)\n",
        "\n",
        "    def init_hidden(self, batch_size = BATCH_SIZE):\n",
        "        return torch.zeros((self.num_layers * 2, batch_size, self.n_hidden), device=device)\n",
        "        \n",
        "        \n",
        "    def forward(self, x, ids=None):\n",
        "        embedded = self.embedding(x)\n",
        "        #(batch size, max sent length, embedding dim)\n",
        "        embedded = self.embedding(x).view((embedded.shape[1], embedded.shape[0], -1))\n",
        "        #(max sent length, batch size, embedding dim)\n",
        "        \n",
        "        bi_output, self.hidden = self.bi_gru(embedded, self.hidden)\n",
        "        \n",
        "        # add sentiment?\n",
        "        \n",
        "        #Just take final value of bi_output:\n",
        "        h = self.lReLU(bi_output[-1])\n",
        "        \n",
        "        if self.sentiment: #then add sentiment\n",
        "            h = add_sentiment(h, ids)\n",
        "            \n",
        "        h = self.fc1( self.dropout(h))\n",
        "        \n",
        "        h = self.fc2( self.dropout(h))\n",
        "        \n",
        "        #h = self.fc2(self.dropout(h))\n",
        "        return h\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "k4UHz12y6L7m",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Helper functions to run training routine, calculate metrics etc.\n",
        "def check_accuracy(task_header, loader, model, conf=False, RNN=False, verbose = True, ret_optim_metric=False):\n",
        "    \"\"\"\n",
        "    Note at the moment this function assumes the batch size is equal to the \n",
        "    number of data in the loader when calculating the confusion matrix\n",
        "    \"\"\"\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    bayesian_metric = None\n",
        "    model.eval()  # set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(loader):\n",
        "            x, y = batch.tweet, getattr(batch, task_header)\n",
        "            y = y.view(-1, 1)\n",
        "                \n",
        "            x = x.to(device=device, dtype=torch.long)  # move to  GPU\n",
        "            y = y.to(device=device, dtype=torch.long)\n",
        "            \n",
        "            if RNN:\n",
        "                #Must zero all of the accumalated hidden state for the RNN\n",
        "                model.hidden = model.init_hidden(batch.batch_size)\n",
        "                \n",
        "            logits = model(x, ids = batch.id)\n",
        "            if task_header == 'subtask_c':\n",
        "                pred_prob = F.softmax(logits, dim=1)\n",
        "                pred_1 = torch.argmax(pred_prob, dim=1).view(-1, 1)\n",
        "            else:\n",
        "                pred_prob = torch.sigmoid(logits)\n",
        "                pred_1 = (pred_prob > 0.5).type(torch.long)\n",
        "              \n",
        "            num_correct += (pred_1 == y).sum()\n",
        "            num_samples += pred_prob.size(0)\n",
        "            \n",
        "            # move to CPU to prevent memory overflow and calculate metrics\n",
        "            x = x.to(device=\"cpu\", dtype=torch.long)\n",
        "            y = y.to(device=\"cpu\", dtype=torch.long).numpy()\n",
        "            pred_1 = pred_1.to(device=\"cpu\", dtype=torch.long).numpy()\n",
        "            \n",
        "            \n",
        "        acc = float(num_correct) / num_samples\n",
        "        if conf:\n",
        "            confusion = metrics.confusion_matrix(y, pred_1)\n",
        "            clas_rep = metrics.classification_report(y, pred_1, output_dict =ret_optim_metric)\n",
        "            kappa = \"Kappa: {:.4f}\".format(metrics.cohen_kappa_score(y, pred_1))\n",
        "            if ret_optim_metric:\n",
        "                bayesian_metric = optim_metric(clas_rep)\n",
        "        else:\n",
        "            total_metric = None\n",
        "        if verbose:\n",
        "            print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
        "            print(confusion)\n",
        "            print(clas_rep)\n",
        "            print(kappa)\n",
        "    return bayesian_metric\n",
        "            \n",
        "def optim_metric(clas_rep):\n",
        "    \"\"\"calculate Bayesian Optimization metric\"\"\"\n",
        "    f1_1 = clas_rep['0'][\"f1-score\"]\n",
        "    f1_2 = clas_rep['1'][\"f1-score\"]\n",
        "    total = np.sqrt(f1_1 * f1_2)\n",
        "    return total\n",
        "\n",
        "def check_loss(task_header, loader, model, loss_fn, RNN=False):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        loss = 0\n",
        "        for idx, batch in enumerate(loader):\n",
        "            x, y = batch.tweet, getattr(batch, task_header)\n",
        "\n",
        "            x = x.to(device=device, dtype=torch.long) \n",
        "            y = y.to(device=device, dtype=torch.long if task_header == 'subtask_c' else torch.float)\n",
        "            \n",
        "            if RNN:\n",
        "                #Must zero all of the accumalated hidden state for the RNN\n",
        "                model.hidden = model.init_hidden(batch.batch_size)\n",
        "                \n",
        "            logits = model(x, ids= batch.id)\n",
        "\n",
        "            loss += loss_fn(logits, y.view(-1,) if isinstance(loss_fn, nn.CrossEntropyLoss) else y.view(-1, 1))\n",
        "\n",
        "    return loss/len(loader)\n",
        "      \n",
        "\n",
        "def train_helper(task_header, model, optimizer, train_loader, \n",
        "               valid_loader, epochs=1, RNN = False, loss_fn=F.binary_cross_entropy_with_logits, \n",
        "                 print_every=50, verbose = True, ret_optim_metric=False):\n",
        "    \"\"\"\n",
        "    Train a model\n",
        "    \n",
        "    Inputs:\n",
        "    - model: A PyTorch Module giving the model to train.\n",
        "    - optimizer: An Optimizer object we will use to train the model\n",
        "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
        "    \n",
        "    Returns: Nothing, but prints model accuracies during training.\n",
        "    \"\"\"\n",
        "    #sets seeds to make results reproducible\n",
        "    set_seed()\n",
        "    \n",
        "    model = model.to(device=device)  # move the model parameters to GPU\n",
        "    \n",
        "    training_losses = []\n",
        "    validation_losses = []\n",
        "    optimizing_metric = []\n",
        "    try:\n",
        "        for epoch in range(epochs):\n",
        "            if verbose:\n",
        "                print(\"Epoch:\", epoch)\n",
        "            total_loss = 0\n",
        "            for batch_idx, batch in enumerate(train_loader):\n",
        "\n",
        "                model.train()  # put model to training mode\n",
        "                optimizer.zero_grad()\n",
        "                inputs, targets = batch.tweet, getattr(batch, task_header)\n",
        "                \n",
        "                if RNN:\n",
        "                    #Must zero all of the accumulated hidden state for the RNN\n",
        "                    model.hidden = model.init_hidden(batch.batch_size)\n",
        "                \n",
        "                \n",
        "                x = inputs.to(device=device, dtype=torch.long)  # move to device, e.g. GPU\n",
        "                y = targets.to(device=device, dtype=torch.long if task_header == 'subtask_c' else torch.float) #this should be a float cross entropy\n",
        "                #x = inputs\n",
        "                #y = targets\n",
        "                logits = model(x, ids = batch.id)\n",
        "                \n",
        "                # When using cross_entropy the targets need to have a shape (N,)\n",
        "                # However, for BCEWithLogits they just need\n",
        "                # to have the same shape as the logits\n",
        "                loss = loss_fn(logits, y.view(-1,) if isinstance(loss_fn, nn.CrossEntropyLoss) else y.view(-1, 1))\n",
        "                # Zero out all of the gradients for the variables which the optimizer\n",
        "                # will update.\n",
        "                \n",
        "\n",
        "                # This is the backwards pass: compute the gradient of the loss with\n",
        "                # respect to each  parameter of the model.\n",
        "                loss.backward()\n",
        "\n",
        "                # Actually update the parameters of the model using the gradients\n",
        "                # computed by the backwards pass.\n",
        "                optimizer.step()\n",
        "\n",
        "                x = x.to(device=\"cpu\", dtype=torch.long)  # move to CPU to prevent memory overflow\n",
        "                y = y.to(device=\"cpu\", dtype=torch.long)\n",
        "\n",
        "                total_loss += loss.detach().item()\n",
        "                \n",
        "                if batch_idx % print_every == 0 and verbose:\n",
        "                    print('Iteration %d, loss = %.4f' % (batch_idx, loss.item()))\n",
        "            \n",
        "            training_losses.append(total_loss/len(train_iterator))\n",
        "            if verbose:\n",
        "                print()\n",
        "                print(\"Validation Accuracy:\")\n",
        "            optim_metric = check_accuracy(task_header, valid_loader, model, RNN=RNN, \n",
        "                                          conf=True, verbose=verbose, ret_optim_metric=ret_optim_metric)\n",
        "            optimizing_metric.append(optim_metric)\n",
        "            \n",
        "            valid_loss = check_loss(task_header, valid_loader, model, loss_fn, RNN)\n",
        "            validation_losses.append(valid_loss)\n",
        "        if ret_optim_metric:\n",
        "            return training_losses, validation_losses, optimizing_metric\n",
        "        else:\n",
        "            return training_losses, validation_losses,\n",
        "    except Exception as e:\n",
        "        #Attempt to prevent GPU memory overflow by transferring model back to cpu\n",
        "        #model = model.to(device=\"cpu\")\n",
        "        raise e    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fDt0pRgyHgxw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tuning\n",
        "There are a huge number of permutations of hyperparameters (particularly to do with preprocessing). Hence it will be useful to do a hyperparameter search for the best values. These are:\n",
        " - Lemmatize words $\\in [True, False]$\n",
        " - Remove punctuation $\\in [True, False]$\n",
        " - Remove stopwords $\\in [True, False]$\n",
        " - Add sentiment (and subjectivity) $\\in [True, False]$\n",
        " - Type of model $\\in [$simple_CNN, Deep_CNN, Bidirectional_GRU]\n",
        " - Model parameters (number of neurons per layers etc)\n",
        " - Window size (in CNN only) - size of kernel. \n",
        " - Learning rate $\\in [0.0001, 0.0025]$\n",
        " - weight_decay $\\in [0.0, 0.1]$\n",
        " \n",
        " We will conduct Bayesian optimization on these parameters and maximize the validation accuracy. To avoid complications due to different combinations of parameters affecting the speed at which the network learns, we will use a large number of epochs but take the maximum validation accuracy (not including the first five epochs). In an ideal world, we would conduct a full scale Bayesian optimization for all three subtasks but due to the constraints of Colab, it will not be possible in this case. Hence we will optimize the values for subtask A and then do some fine tuning methods when it comes to the other two tasks. \n",
        " \n",
        "Note: It was clear from preliminary investigation that our implementation of the Bidirectional_GRU network was not performing well. As such, it was not included in the Bayesian Optimization. "
      ]
    },
    {
      "metadata": {
        "id": "7uXTKBmINouS",
        "colab_type": "code",
        "outputId": "0f724444-1447-45b5-8e1f-bac13bb15efb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3212
        }
      },
      "cell_type": "code",
      "source": [
        "#Bayesian optimization for hyperparameters\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def hyperparam_tuning():\n",
        "    \"\"\"Run hyperparameter tuning routine using bayesian optimization.\n",
        "\n",
        "    The hyperparameters are as follows (in this order):\n",
        "         - Lemmatize words $\\in [True, False]$\n",
        "         - Remove punctuation $\\in [True, False]$\n",
        "         - Remove stopwords $\\in [True, False]$\n",
        "         - Add sentiment (and subjectivity) $\\in [True, False]$\n",
        "         - Type of model $\\in [$simple_CNN, Deep_CNN, Bidirectional_GRU]\n",
        "         - Learning rate $\\in [0.0001, 0.0025]$\n",
        "         - weight_decay $\\in [0.0, 0.1]$\n",
        "         - window_size $\\in [3, 5]$\n",
        "    \"\"\"\n",
        "\n",
        "    #define dimension lower/upper bound (or possible values) for each hyperparameter\n",
        "        #args are of the form:\n",
        "        #[lemma, rem_punct, rem_stopwords, sentiment, model_type, lr, weight_decay, window_size, dropout_on]\n",
        "\n",
        "    dimensions = [(True, False), (True, False), (True, False), (True, False),\n",
        "                  (\"simple_CNN\", \"Deep_CNN\"), (0.0001, 0.0015), (0.0, 0.1), \n",
        "                  (2, 3, 4, 5), (True, False)]\n",
        "                  \n",
        "\n",
        "    #It will check the initial points above and then use bayesian optimization\n",
        "    #to choose the next points to evaluate\n",
        "    #first define a function to minimize:\n",
        "    res = gp_minimize(fn_optim, dimensions, n_calls=50, n_random_starts=14,\n",
        "                acq_func='gp_hedge',\n",
        "                random_state=SEED, verbose=True, callback=None, n_points=1000,\n",
        "                n_restarts_optimizer=5, xi=0.01, kappa=1.96,\n",
        "                noise='gaussian')\n",
        "    print(\"Bayesian Optimization Results:\")\n",
        "    print(res)\n",
        "    return res\n",
        "\n",
        "#Define function which will call the training cycle with each set of parameters\n",
        "def fn_optim(args, epochs=25):\n",
        "    \"\"\"Helper function to run the hyperparameter optimisation. It takes the\n",
        "    hyperparameters as args and must return the quantity to be minimized\n",
        "    \"\"\"\n",
        "    [lemma, rem_punct, rem_stopwords, sentiment, model_type, lr, weight_decay, window_size, dropout_on] = args\n",
        "\n",
        "    out_channels = 128 #keep these fixed\n",
        "    embedding_dim = 200 #for glove\n",
        "    \n",
        "    if dropout_on == True:\n",
        "        dropout_rate = 0.5\n",
        "    else:\n",
        "        dropout_rate = 0\n",
        "\n",
        "    TOKENIZE_PARAMS = {\"lemmatize\": lemma, \n",
        "                       \"rem_punct\": rem_punct, #remove punctuation\n",
        "                       \"rem_stopwords\": rem_stopwords}\n",
        "\n",
        "    #Initialize vocab with these parameters:\n",
        "\n",
        "    TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, batch_first = True)\n",
        "    LABEL = data.LabelField(sequential=False, use_vocab=True, batch_first = True)\n",
        "    ID = data.LabelField(sequential=False, use_vocab=False, batch_first=True)\n",
        "\n",
        "    data_fields = [('id', ID), \n",
        "                   ('tweet', TEXT),\n",
        "                   ('subtask_a',LABEL),\n",
        "                   ('subtask_b',LABEL),\n",
        "                   ('subtask_c',LABEL)]\n",
        "\n",
        "    set_seed()\n",
        "    train = data.TabularDataset(train_fp, format='TSV', fields = \n",
        "                                data_fields, skip_header=True, filter_pred=None)\n",
        "\n",
        "    train, valid = train.split(split_ratio=0.8, random_state=random.seed(SEED))\n",
        "\n",
        "    #Now build vocab (using only the training set)\n",
        "    TEXT.build_vocab(train, vectors='glove.twitter.27B.200d') #USE \"glove.840B.300d\" or glove.twitter.27B.200d\n",
        "\n",
        "    LABEL.build_vocab(train.subtask_a)\n",
        "\n",
        "    output_dim = len(LABEL.vocab)\n",
        "\n",
        "    #Create iterators\n",
        "    train_iterator, valid_iterator = data.BucketIterator.splits((train, valid),\n",
        "                            batch_sizes=(BATCH_SIZE, len(valid)),  \n",
        "                            sort_key=lambda x: len(x.tweet), device=device)\n",
        "\n",
        "    # For retrieving tweet text later on\n",
        "    train_df = pd.read_csv(train_fp, delimiter=\"\\t\")\n",
        "\n",
        "    #define model\n",
        "    if model_type == \"simple_CNN\":\n",
        "        n_hidden = 64\n",
        "        model = SimpleClassifierGloVe(TEXT.vocab, embedding_dim, window_size, out_channels, \n",
        "                          dropout=True, sentiment = sentiment)\n",
        "    elif model_type == \"Deep_CNN\":\n",
        "        n_hidden = (64, 32, 16, 8, 4)\n",
        "        model = ClassifierGloVeDeep(TEXT.vocab, embedding_dim, window_size, out_channels, \n",
        "             dropout=dropout_rate, n_hidden = n_hidden, sentiment=sentiment)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid NN architecture. n_hidden must be either \\\"funnel\\\" or \\\"diamond\\\".\")\n",
        "\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr = lr, weight_decay = weight_decay)\n",
        "\n",
        "    pos_weight = torch.tensor([2.], device = device) #deals with unbalanced classes\n",
        "\n",
        "    loss_fn = nn.BCEWithLogitsLoss(pos_weight = pos_weight)\n",
        "\n",
        "    #train model\n",
        "    _, v_losses, optim_metric = train_helper('subtask_a', model, optimizer, loss_fn = loss_fn, \n",
        "                                      epochs = 25, train_loader=train_iterator, \n",
        "                                      valid_loader=valid_iterator, ret_optim_metric=True, verbose=False)\n",
        "\n",
        "    max_val = max(optim_metric[4:])\n",
        "\n",
        "    return - max_val #return negative value as this will minimize this\n",
        "\n",
        "hyperparam_tuning()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration No: 1 started. Evaluating function at random point.\n",
            "Iteration No: 1 ended. Evaluation done at random point.\n",
            "Time taken: 55.9868\n",
            "Function value obtained: -0.0000\n",
            "Current minimum: -0.0000\n",
            "Iteration No: 2 started. Evaluating function at random point.\n",
            "Iteration No: 2 ended. Evaluation done at random point.\n",
            "Time taken: 340.7466\n",
            "Function value obtained: -0.0000\n",
            "Current minimum: -0.0000\n",
            "Iteration No: 3 started. Evaluating function at random point.\n",
            "Iteration No: 3 ended. Evaluation done at random point.\n",
            "Time taken: 57.6394\n",
            "Function value obtained: -0.0000\n",
            "Current minimum: -0.0000\n",
            "Iteration No: 4 started. Evaluating function at random point.\n",
            "Iteration No: 4 ended. Evaluation done at random point.\n",
            "Time taken: 342.7397\n",
            "Function value obtained: -0.0000\n",
            "Current minimum: -0.0000\n",
            "Iteration No: 5 started. Evaluating function at random point.\n",
            "Iteration No: 5 ended. Evaluation done at random point.\n",
            "Time taken: 348.3860\n",
            "Function value obtained: -0.7263\n",
            "Current minimum: -0.7263\n",
            "Iteration No: 6 started. Evaluating function at random point.\n",
            "Iteration No: 6 ended. Evaluation done at random point.\n",
            "Time taken: 57.2985\n",
            "Function value obtained: -0.5166\n",
            "Current minimum: -0.7263\n",
            "Iteration No: 7 started. Evaluating function at random point.\n",
            "Iteration No: 7 ended. Evaluation done at random point.\n",
            "Time taken: 343.5670\n",
            "Function value obtained: -0.4161\n",
            "Current minimum: -0.7263\n",
            "Iteration No: 8 started. Evaluating function at random point.\n",
            "Iteration No: 8 ended. Evaluation done at random point.\n",
            "Time taken: 348.3771\n",
            "Function value obtained: -0.0000\n",
            "Current minimum: -0.7263\n",
            "Iteration No: 9 started. Evaluating function at random point.\n",
            "Iteration No: 9 ended. Evaluation done at random point.\n",
            "Time taken: 346.0011\n",
            "Function value obtained: -0.0000\n",
            "Current minimum: -0.7263\n",
            "Iteration No: 10 started. Evaluating function at random point.\n",
            "Iteration No: 10 ended. Evaluation done at random point.\n",
            "Time taken: 56.7545\n",
            "Function value obtained: -0.0000\n",
            "Current minimum: -0.7263\n",
            "Iteration No: 11 started. Evaluating function at random point.\n",
            "Iteration No: 11 ended. Evaluation done at random point.\n",
            "Time taken: 54.3021\n",
            "Function value obtained: -0.0000\n",
            "Current minimum: -0.7263\n",
            "Iteration No: 12 started. Evaluating function at random point.\n",
            "Iteration No: 12 ended. Evaluation done at random point.\n",
            "Time taken: 347.4415\n",
            "Function value obtained: -0.0000\n",
            "Current minimum: -0.7263\n",
            "Iteration No: 13 started. Evaluating function at random point.\n",
            "Iteration No: 13 ended. Evaluation done at random point.\n",
            "Time taken: 56.2298\n",
            "Function value obtained: -0.0000\n",
            "Current minimum: -0.7263\n",
            "Iteration No: 14 started. Evaluating function at random point.\n",
            "Iteration No: 14 ended. Evaluation done at random point.\n",
            "Time taken: 58.3771\n",
            "Function value obtained: -0.0000\n",
            "Current minimum: -0.7263\n",
            "Iteration No: 15 started. Searching for the next optimal point.\n",
            "Iteration No: 15 ended. Search finished for the next optimal point.\n",
            "Time taken: 57.3053\n",
            "Function value obtained: -0.0000\n",
            "Current minimum: -0.7263\n",
            "Iteration No: 16 started. Searching for the next optimal point.\n",
            "Iteration No: 16 ended. Search finished for the next optimal point.\n",
            "Time taken: 351.4165\n",
            "Function value obtained: -0.0000\n",
            "Current minimum: -0.7263\n",
            "Iteration No: 17 started. Searching for the next optimal point.\n",
            "Iteration No: 17 ended. Search finished for the next optimal point.\n",
            "Time taken: 350.9225\n",
            "Function value obtained: -0.0000\n",
            "Current minimum: -0.7263\n",
            "Iteration No: 18 started. Searching for the next optimal point.\n",
            "Iteration No: 18 ended. Search finished for the next optimal point.\n",
            "Time taken: 60.2575\n",
            "Function value obtained: -0.0000\n",
            "Current minimum: -0.7263\n",
            "Iteration No: 19 started. Searching for the next optimal point.\n",
            "Iteration No: 19 ended. Search finished for the next optimal point.\n",
            "Time taken: 349.4024\n",
            "Function value obtained: -0.0000\n",
            "Current minimum: -0.7263\n",
            "Iteration No: 20 started. Searching for the next optimal point.\n",
            "Iteration No: 20 ended. Search finished for the next optimal point.\n",
            "Time taken: 349.7365\n",
            "Function value obtained: -0.7387\n",
            "Current minimum: -0.7387\n",
            "Iteration No: 21 started. Searching for the next optimal point.\n",
            "Iteration No: 21 ended. Search finished for the next optimal point.\n",
            "Time taken: 59.7159\n",
            "Function value obtained: -0.7228\n",
            "Current minimum: -0.7387\n",
            "Iteration No: 22 started. Searching for the next optimal point.\n",
            "Iteration No: 22 ended. Search finished for the next optimal point.\n",
            "Time taken: 349.1690\n",
            "Function value obtained: -0.7161\n",
            "Current minimum: -0.7387\n",
            "Iteration No: 23 started. Searching for the next optimal point.\n",
            "Iteration No: 23 ended. Search finished for the next optimal point.\n",
            "Time taken: 55.6416\n",
            "Function value obtained: -0.1959\n",
            "Current minimum: -0.7387\n",
            "Iteration No: 24 started. Searching for the next optimal point.\n",
            "Iteration No: 24 ended. Search finished for the next optimal point.\n",
            "Time taken: 58.6246\n",
            "Function value obtained: -0.7361\n",
            "Current minimum: -0.7387\n",
            "Iteration No: 25 started. Searching for the next optimal point.\n",
            "Iteration No: 25 ended. Search finished for the next optimal point.\n",
            "Time taken: 344.5643\n",
            "Function value obtained: -0.0000\n",
            "Current minimum: -0.7387\n",
            "Iteration No: 26 started. Searching for the next optimal point.\n",
            "Iteration No: 26 ended. Search finished for the next optimal point.\n",
            "Time taken: 349.3844\n",
            "Function value obtained: -0.7310\n",
            "Current minimum: -0.7387\n",
            "Iteration No: 27 started. Searching for the next optimal point.\n",
            "Iteration No: 27 ended. Search finished for the next optimal point.\n",
            "Time taken: 58.5444\n",
            "Function value obtained: -0.0000\n",
            "Current minimum: -0.7387\n",
            "Iteration No: 28 started. Searching for the next optimal point.\n",
            "Iteration No: 28 ended. Search finished for the next optimal point.\n",
            "Time taken: 57.7177\n",
            "Function value obtained: -0.0000\n",
            "Current minimum: -0.7387\n",
            "Iteration No: 29 started. Searching for the next optimal point.\n",
            "Iteration No: 29 ended. Search finished for the next optimal point.\n",
            "Time taken: 58.3478\n",
            "Function value obtained: -0.7406\n",
            "Current minimum: -0.7406\n",
            "Iteration No: 30 started. Searching for the next optimal point.\n",
            "Iteration No: 30 ended. Search finished for the next optimal point.\n",
            "Time taken: 57.3396\n",
            "Function value obtained: -0.0000\n",
            "Current minimum: -0.7406\n",
            "Iteration No: 31 started. Searching for the next optimal point.\n",
            "Iteration No: 31 ended. Search finished for the next optimal point.\n",
            "Time taken: 58.2099\n",
            "Function value obtained: -0.7388\n",
            "Current minimum: -0.7406\n",
            "Iteration No: 32 started. Searching for the next optimal point.\n",
            "Iteration No: 32 ended. Search finished for the next optimal point.\n",
            "Time taken: 57.0411\n",
            "Function value obtained: -0.7307\n",
            "Current minimum: -0.7406\n",
            "Iteration No: 33 started. Searching for the next optimal point.\n",
            "Iteration No: 33 ended. Search finished for the next optimal point.\n",
            "Time taken: 346.2460\n",
            "Function value obtained: -0.7389\n",
            "Current minimum: -0.7406\n",
            "Iteration No: 34 started. Searching for the next optimal point.\n",
            "Iteration No: 34 ended. Search finished for the next optimal point.\n",
            "Time taken: 57.0400\n",
            "Function value obtained: -0.0000\n",
            "Current minimum: -0.7406\n",
            "Iteration No: 35 started. Searching for the next optimal point.\n",
            "Iteration No: 35 ended. Search finished for the next optimal point.\n",
            "Time taken: 58.9253\n",
            "Function value obtained: -0.0000\n",
            "Current minimum: -0.7406\n",
            "Iteration No: 36 started. Searching for the next optimal point.\n",
            "Iteration No: 36 ended. Search finished for the next optimal point.\n",
            "Time taken: 344.2096\n",
            "Function value obtained: -0.7374\n",
            "Current minimum: -0.7406\n",
            "Iteration No: 37 started. Searching for the next optimal point.\n",
            "Iteration No: 37 ended. Search finished for the next optimal point.\n",
            "Time taken: 58.3881\n",
            "Function value obtained: -0.7383\n",
            "Current minimum: -0.7406\n",
            "Iteration No: 38 started. Searching for the next optimal point.\n",
            "Iteration No: 38 ended. Search finished for the next optimal point.\n",
            "Time taken: 58.5403\n",
            "Function value obtained: -0.7367\n",
            "Current minimum: -0.7406\n",
            "Iteration No: 39 started. Searching for the next optimal point.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "n1TwMNFOKRSm"
      },
      "cell_type": "markdown",
      "source": [
        "## Task A"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WO69uqM3LtBS",
        "outputId": "eadfa90c-f5fd-48c4-9e0e-3fb9fd5c5ff1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "#Use Torchtext to create vocabulary \n",
        "BATCH_SIZE = 128\n",
        "\n",
        "TOKENIZE_PARAMS = {\"lemmatize\": True, \n",
        "                   \"rem_punct\": True, #remove punctuation\n",
        "                   \"rem_stopwords\": True}\n",
        "TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, batch_first = True)\n",
        "LABEL = data.LabelField(sequential=False, use_vocab=True, batch_first = True)\n",
        "ID = data.LabelField(sequential=False, use_vocab=False, batch_first=True)\n",
        "\n",
        "data_fields = [('id', ID), \n",
        "               ('tweet', TEXT),\n",
        "               ('subtask_a',LABEL),\n",
        "               ('subtask_b',LABEL),\n",
        "               ('subtask_c',LABEL)]\n",
        "\n",
        "\n",
        "train = data.TabularDataset(train_fp, format='TSV', fields = \n",
        "                            data_fields, skip_header=True, filter_pred=None)\n",
        "\n",
        "train, valid = train.split(split_ratio=0.8, random_state=random.seed(SEED))\n",
        "\n",
        "print(f'Train size: {len(train)}')\n",
        "print(f'Validation size: {len(valid)}')\n",
        "\n",
        "#Now build vocab (using only the training set)\n",
        "TEXT.build_vocab(train, vectors='glove.twitter.27B.200d') #USE \"glove.840B.300d\" or glove.twitter.27B.200d\n",
        "\n",
        "LABEL.build_vocab(train.subtask_a)\n",
        "\n",
        "output_dim = len(LABEL.vocab)\n",
        "\n",
        "#Create iterators\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid),\n",
        "                        batch_sizes=(BATCH_SIZE, len(valid)),  \n",
        "                        sort_key=lambda x: len(x.tweet), device=device)\n",
        "\n",
        "# For retrieving tweet text later on\n",
        "train_df = pd.read_csv(train_fp, delimiter=\"\\t\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 10592\n",
            "Validation size: 2648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "KkGDZeI-rccB",
        "outputId": "d0af09a2-161f-4c53-f1e6-3efe787dc0aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "print('first tweet', train[0].tweet)\n",
        "print('first label', train[0].subtask_a)\n",
        "print(\"first tweet id:\", train[0].id)\n",
        "# print(TEXT.vocab.stoi) # word to index\n",
        "# print(LABEL.vocab.stoi) # word to index\n"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first tweet ['@user', '@user', 'a', 'read', 'url']\n",
            "first label NOT\n",
            "first tweet id: 29719\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1hrLG6N2Ex17",
        "colab_type": "code",
        "outputId": "f99d48c7-94b1-4eee-f15e-56316f3e0e96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "cell_type": "code",
      "source": [
        "#Simple Conv with Glove\n",
        "embedding_dim = 200\n",
        "window_size = 6\n",
        "lr = 0.0025\n",
        "out_channels = 128\n",
        "dropout = 0.5\n",
        "n_hidden = 64\n",
        "pos_weight = torch.tensor([2.], device = device) #deals with unbalanced classes\n",
        "\n",
        "set_seed()\n",
        "\n",
        "model = SimpleClassifierGloVe(TEXT.vocab, embedding_dim, window_size, out_channels, \n",
        "                              dropout, sentiment = True, n_hidden = n_hidden)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight = pos_weight)\n",
        "\n",
        "\n",
        "t_losses, v_losses = train_helper('subtask_a', model, optimizer, loss_fn = loss_fn, \n",
        "                                  epochs = 25, train_loader=train_iterator, \n",
        "                                  valid_loader=valid_iterator, verbose=False)\n"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Got 1869 / 2648 correct (70.58)\n",
            "Got 1944 / 2648 correct (73.41)\n",
            "Got 2012 / 2648 correct (75.98)\n",
            "Got 1988 / 2648 correct (75.08)\n",
            "Got 1970 / 2648 correct (74.40)\n",
            "Got 1958 / 2648 correct (73.94)\n",
            "Got 1997 / 2648 correct (75.42)\n",
            "Got 1959 / 2648 correct (73.98)\n",
            "Got 1991 / 2648 correct (75.19)\n",
            "Got 1935 / 2648 correct (73.07)\n",
            "Got 1947 / 2648 correct (73.53)\n",
            "Got 1969 / 2648 correct (74.36)\n",
            "Got 1970 / 2648 correct (74.40)\n",
            "Got 1964 / 2648 correct (74.17)\n",
            "Got 1959 / 2648 correct (73.98)\n",
            "Got 1968 / 2648 correct (74.32)\n",
            "Got 1949 / 2648 correct (73.60)\n",
            "Got 1939 / 2648 correct (73.23)\n",
            "Got 1945 / 2648 correct (73.45)\n",
            "Got 1949 / 2648 correct (73.60)\n",
            "Got 1980 / 2648 correct (74.77)\n",
            "Got 1967 / 2648 correct (74.28)\n",
            "Got 1935 / 2648 correct (73.07)\n",
            "Got 1969 / 2648 correct (74.36)\n",
            "Got 1955 / 2648 correct (73.83)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "X9LseL5F9n7P",
        "outputId": "be7cb91b-35c9-41c1-dec7-da0f59835902",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6373
        }
      },
      "cell_type": "code",
      "source": [
        "# Bidirectional GRU with Glove\n",
        "# Beats baseline for minor class. Not as good as convolution\n",
        "\n",
        "embedding_dim = 200\n",
        "window_size = 3\n",
        "lr = 0.00025\n",
        "out_channels = 128\n",
        "dropout = 0\n",
        "pos_weight = torch.tensor([2.], device = device) #deals with unbalanced classes\n",
        "\n",
        "\n",
        "model = BidirectionalGRU(TEXT.vocab, embedding_dim, window_size, out_channels, dropout, n_hidden = 64)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "\n",
        "t_losses, v_losses = train_helper('subtask_a', model, optimizer, loss_fn = loss_fn,\n",
        "                                  epochs = 20, train_loader=train_iterator, \n",
        "                                  valid_loader=valid_iterator, RNN=True)\n"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Iteration 0, loss = 0.9607\n",
            "Iteration 50, loss = 0.8707\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1760 / 2648 correct (66.47)\n",
            "[[1753   20]\n",
            " [ 868    7]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.99      0.80      1773\n",
            "           1       0.26      0.01      0.02       875\n",
            "\n",
            "   micro avg       0.66      0.66      0.66      2648\n",
            "   macro avg       0.46      0.50      0.41      2648\n",
            "weighted avg       0.53      0.66      0.54      2648\n",
            "\n",
            "Kappa: -0.0043\n",
            "\n",
            "Epoch: 1\n",
            "Iteration 0, loss = 0.9503\n",
            "Iteration 50, loss = 0.9152\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1736 / 2648 correct (65.56)\n",
            "[[1690   83]\n",
            " [ 829   46]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.95      0.79      1773\n",
            "           1       0.36      0.05      0.09       875\n",
            "\n",
            "   micro avg       0.66      0.66      0.66      2648\n",
            "   macro avg       0.51      0.50      0.44      2648\n",
            "weighted avg       0.57      0.66      0.56      2648\n",
            "\n",
            "Kappa: 0.0073\n",
            "\n",
            "Epoch: 2\n",
            "Iteration 0, loss = 0.9427\n",
            "Iteration 50, loss = 0.9269\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1758 / 2648 correct (66.39)\n",
            "[[1747   26]\n",
            " [ 864   11]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.99      0.80      1773\n",
            "           1       0.30      0.01      0.02       875\n",
            "\n",
            "   micro avg       0.66      0.66      0.66      2648\n",
            "   macro avg       0.48      0.50      0.41      2648\n",
            "weighted avg       0.55      0.66      0.54      2648\n",
            "\n",
            "Kappa: -0.0028\n",
            "\n",
            "Epoch: 3\n",
            "Iteration 0, loss = 0.9109\n",
            "Iteration 50, loss = 0.8971\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1723 / 2648 correct (65.07)\n",
            "[[1656  117]\n",
            " [ 808   67]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.93      0.78      1773\n",
            "           1       0.36      0.08      0.13       875\n",
            "\n",
            "   micro avg       0.65      0.65      0.65      2648\n",
            "   macro avg       0.52      0.51      0.45      2648\n",
            "weighted avg       0.57      0.65      0.57      2648\n",
            "\n",
            "Kappa: 0.0132\n",
            "\n",
            "Epoch: 4\n",
            "Iteration 0, loss = 0.9096\n",
            "Iteration 50, loss = 0.9543\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1018 / 2648 correct (38.44)\n",
            "[[ 297 1476]\n",
            " [ 154  721]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.17      0.27      1773\n",
            "           1       0.33      0.82      0.47       875\n",
            "\n",
            "   micro avg       0.38      0.38      0.38      2648\n",
            "   macro avg       0.49      0.50      0.37      2648\n",
            "weighted avg       0.55      0.38      0.33      2648\n",
            "\n",
            "Kappa: -0.0061\n",
            "\n",
            "Epoch: 5\n",
            "Iteration 0, loss = 0.9187\n",
            "Iteration 50, loss = 0.9121\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1758 / 2648 correct (66.39)\n",
            "[[1747   26]\n",
            " [ 864   11]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.99      0.80      1773\n",
            "           1       0.30      0.01      0.02       875\n",
            "\n",
            "   micro avg       0.66      0.66      0.66      2648\n",
            "   macro avg       0.48      0.50      0.41      2648\n",
            "weighted avg       0.55      0.66      0.54      2648\n",
            "\n",
            "Kappa: -0.0028\n",
            "\n",
            "Epoch: 6\n",
            "Iteration 0, loss = 0.8941\n",
            "Iteration 50, loss = 0.9546\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1683 / 2648 correct (63.56)\n",
            "[[1590  183]\n",
            " [ 782   93]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.90      0.77      1773\n",
            "           1       0.34      0.11      0.16       875\n",
            "\n",
            "   micro avg       0.64      0.64      0.64      2648\n",
            "   macro avg       0.50      0.50      0.46      2648\n",
            "weighted avg       0.56      0.64      0.57      2648\n",
            "\n",
            "Kappa: 0.0037\n",
            "\n",
            "Epoch: 7\n",
            "Iteration 0, loss = 0.9182\n",
            "Iteration 50, loss = 0.9074\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 923 / 2648 correct (34.86)\n",
            "[[ 100 1673]\n",
            " [  52  823]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.06      0.10      1773\n",
            "           1       0.33      0.94      0.49       875\n",
            "\n",
            "   micro avg       0.35      0.35      0.35      2648\n",
            "   macro avg       0.49      0.50      0.30      2648\n",
            "weighted avg       0.55      0.35      0.23      2648\n",
            "\n",
            "Kappa: -0.0021\n",
            "\n",
            "Epoch: 8\n",
            "Iteration 0, loss = 0.9294\n",
            "Iteration 50, loss = 0.9679\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 884 / 2648 correct (33.38)\n",
            "[[  24 1749]\n",
            " [  15  860]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.01      0.03      1773\n",
            "           1       0.33      0.98      0.49       875\n",
            "\n",
            "   micro avg       0.33      0.33      0.33      2648\n",
            "   macro avg       0.47      0.50      0.26      2648\n",
            "weighted avg       0.52      0.33      0.18      2648\n",
            "\n",
            "Kappa: -0.0024\n",
            "\n",
            "Epoch: 9\n",
            "Iteration 0, loss = 0.9358\n",
            "Iteration 50, loss = 0.8634\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1725 / 2648 correct (65.14)\n",
            "[[1653  120]\n",
            " [ 803   72]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.93      0.78      1773\n",
            "           1       0.38      0.08      0.13       875\n",
            "\n",
            "   micro avg       0.65      0.65      0.65      2648\n",
            "   macro avg       0.52      0.51      0.46      2648\n",
            "weighted avg       0.57      0.65      0.57      2648\n",
            "\n",
            "Kappa: 0.0182\n",
            "\n",
            "Epoch: 10\n",
            "Iteration 0, loss = 0.8702\n",
            "Iteration 50, loss = 0.9036\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1750 / 2648 correct (66.09)\n",
            "[[1731   42]\n",
            " [ 856   19]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.98      0.79      1773\n",
            "           1       0.31      0.02      0.04       875\n",
            "\n",
            "   micro avg       0.66      0.66      0.66      2648\n",
            "   macro avg       0.49      0.50      0.42      2648\n",
            "weighted avg       0.55      0.66      0.55      2648\n",
            "\n",
            "Kappa: -0.0026\n",
            "\n",
            "Epoch: 11\n",
            "Iteration 0, loss = 0.8769\n",
            "Iteration 50, loss = 0.8869\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1736 / 2648 correct (65.56)\n",
            "[[1713   60]\n",
            " [ 852   23]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.97      0.79      1773\n",
            "           1       0.28      0.03      0.05       875\n",
            "\n",
            "   micro avg       0.66      0.66      0.66      2648\n",
            "   macro avg       0.47      0.50      0.42      2648\n",
            "weighted avg       0.54      0.66      0.54      2648\n",
            "\n",
            "Kappa: -0.0098\n",
            "\n",
            "Epoch: 12\n",
            "Iteration 0, loss = 0.8888\n",
            "Iteration 50, loss = 0.9001\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1735 / 2648 correct (65.52)\n",
            "[[1707   66]\n",
            " [ 847   28]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.96      0.79      1773\n",
            "           1       0.30      0.03      0.06       875\n",
            "\n",
            "   micro avg       0.66      0.66      0.66      2648\n",
            "   macro avg       0.48      0.50      0.42      2648\n",
            "weighted avg       0.55      0.66      0.55      2648\n",
            "\n",
            "Kappa: -0.0068\n",
            "\n",
            "Epoch: 13\n",
            "Iteration 0, loss = 0.8627\n",
            "Iteration 50, loss = 0.9128\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 944 / 2648 correct (35.65)\n",
            "[[ 161 1612]\n",
            " [  92  783]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.09      0.16      1773\n",
            "           1       0.33      0.89      0.48       875\n",
            "\n",
            "   micro avg       0.36      0.36      0.36      2648\n",
            "   macro avg       0.48      0.49      0.32      2648\n",
            "weighted avg       0.53      0.36      0.26      2648\n",
            "\n",
            "Kappa: -0.0100\n",
            "\n",
            "Epoch: 14\n",
            "Iteration 0, loss = 0.9018\n",
            "Iteration 50, loss = 0.9782\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 889 / 2648 correct (33.57)\n",
            "[[  30 1743]\n",
            " [  16  859]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.02      0.03      1773\n",
            "           1       0.33      0.98      0.49       875\n",
            "\n",
            "   micro avg       0.34      0.34      0.34      2648\n",
            "   macro avg       0.49      0.50      0.26      2648\n",
            "weighted avg       0.55      0.34      0.19      2648\n",
            "\n",
            "Kappa: -0.0009\n",
            "\n",
            "Epoch: 15\n",
            "Iteration 0, loss = 0.9393\n",
            "Iteration 50, loss = 0.8990\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 909 / 2648 correct (34.33)\n",
            "[[  79 1694]\n",
            " [  45  830]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.04      0.08      1773\n",
            "           1       0.33      0.95      0.49       875\n",
            "\n",
            "   micro avg       0.34      0.34      0.34      2648\n",
            "   macro avg       0.48      0.50      0.29      2648\n",
            "weighted avg       0.54      0.34      0.22      2648\n",
            "\n",
            "Kappa: -0.0047\n",
            "\n",
            "Epoch: 16\n",
            "Iteration 0, loss = 0.9125\n",
            "Iteration 50, loss = 0.9163\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1755 / 2648 correct (66.28)\n",
            "[[1736   37]\n",
            " [ 856   19]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.98      0.80      1773\n",
            "           1       0.34      0.02      0.04       875\n",
            "\n",
            "   micro avg       0.66      0.66      0.66      2648\n",
            "   macro avg       0.50      0.50      0.42      2648\n",
            "weighted avg       0.56      0.66      0.55      2648\n",
            "\n",
            "Kappa: 0.0011\n",
            "\n",
            "Epoch: 17\n",
            "Iteration 0, loss = 0.8853\n",
            "Iteration 50, loss = 0.9132\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 907 / 2648 correct (34.25)\n",
            "[[  68 1705]\n",
            " [  36  839]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.04      0.07      1773\n",
            "           1       0.33      0.96      0.49       875\n",
            "\n",
            "   micro avg       0.34      0.34      0.34      2648\n",
            "   macro avg       0.49      0.50      0.28      2648\n",
            "weighted avg       0.55      0.34      0.21      2648\n",
            "\n",
            "Kappa: -0.0019\n",
            "\n",
            "Epoch: 18\n",
            "Iteration 0, loss = 0.9382\n",
            "Iteration 50, loss = 0.9267\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1750 / 2648 correct (66.09)\n",
            "[[1728   45]\n",
            " [ 853   22]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.97      0.79      1773\n",
            "           1       0.33      0.03      0.05       875\n",
            "\n",
            "   micro avg       0.66      0.66      0.66      2648\n",
            "   macro avg       0.50      0.50      0.42      2648\n",
            "weighted avg       0.56      0.66      0.55      2648\n",
            "\n",
            "Kappa: -0.0003\n",
            "\n",
            "Epoch: 19\n",
            "Iteration 0, loss = 0.9021\n",
            "Iteration 50, loss = 0.9569\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1717 / 2648 correct (64.84)\n",
            "[[1677   96]\n",
            " [ 835   40]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.95      0.78      1773\n",
            "           1       0.29      0.05      0.08       875\n",
            "\n",
            "   micro avg       0.65      0.65      0.65      2648\n",
            "   macro avg       0.48      0.50      0.43      2648\n",
            "weighted avg       0.54      0.65      0.55      2648\n",
            "\n",
            "Kappa: -0.0107\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "a4wCjiR5O5Qf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6426
        },
        "outputId": "e2327c21-fedd-4f0a-e06f-0c7a9f606e76"
      },
      "cell_type": "code",
      "source": [
        "#Conv with Glove Deep\n",
        "embedding_dim = 200\n",
        "window_size = 3\n",
        "lr = 0.00025\n",
        "out_channels = 128\n",
        "dropout = 0.5\n",
        "pos_weight = torch.tensor([0.6], device = device) #deals with unbalanced classes\n",
        "n_hidden = (64, 32, 16, 8, 4)\n",
        "\n",
        "set_seed()\n",
        "\n",
        "model = ClassifierGloVeDeep(TEXT.vocab, embedding_dim, window_size, out_channels, dropout, n_hidden = n_hidden)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight = pos_weight)\n",
        "\n",
        "\n",
        "t_losses, v_losses = train_helper('subtask_a', model, optimizer, loss_fn = loss_fn, epochs = 20, train_loader=train_iterator, valid_loader=valid_iterator)\n"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Iteration 0, loss = 0.5159\n",
            "Iteration 50, loss = 0.5324\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1773 / 2648 correct (66.96)\n",
            "[[1773    0]\n",
            " [ 875    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      1773\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.33      0.50      0.40      2648\n",
            "weighted avg       0.45      0.67      0.54      2648\n",
            "\n",
            "Kappa: 0.0000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Iteration 0, loss = 0.5328\n",
            "Iteration 50, loss = 0.5546\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1773 / 2648 correct (66.96)\n",
            "[[1773    0]\n",
            " [ 875    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      1773\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.33      0.50      0.40      2648\n",
            "weighted avg       0.45      0.67      0.54      2648\n",
            "\n",
            "Kappa: 0.0000\n",
            "\n",
            "Epoch: 2\n",
            "Iteration 0, loss = 0.5396\n",
            "Iteration 50, loss = 0.5154\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1773 / 2648 correct (66.96)\n",
            "[[1773    0]\n",
            " [ 875    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      1773\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.33      0.50      0.40      2648\n",
            "weighted avg       0.45      0.67      0.54      2648\n",
            "\n",
            "Kappa: 0.0000\n",
            "\n",
            "Epoch: 3\n",
            "Iteration 0, loss = 0.4959\n",
            "Iteration 50, loss = 0.5049\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1773 / 2648 correct (66.96)\n",
            "[[1773    0]\n",
            " [ 875    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      1773\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.33      0.50      0.40      2648\n",
            "weighted avg       0.45      0.67      0.54      2648\n",
            "\n",
            "Kappa: 0.0000\n",
            "\n",
            "Epoch: 4\n",
            "Iteration 0, loss = 0.4987\n",
            "Iteration 50, loss = 0.5407\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1773 / 2648 correct (66.96)\n",
            "[[1773    0]\n",
            " [ 875    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      1773\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.33      0.50      0.40      2648\n",
            "weighted avg       0.45      0.67      0.54      2648\n",
            "\n",
            "Kappa: 0.0000\n",
            "\n",
            "Epoch: 5\n",
            "Iteration 0, loss = 0.5149\n",
            "Iteration 50, loss = 0.5320\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1773 / 2648 correct (66.96)\n",
            "[[1773    0]\n",
            " [ 875    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      1773\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.33      0.50      0.40      2648\n",
            "weighted avg       0.45      0.67      0.54      2648\n",
            "\n",
            "Kappa: 0.0000\n",
            "\n",
            "Epoch: 6\n",
            "Iteration 0, loss = 0.4989\n",
            "Iteration 50, loss = 0.5355\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1773 / 2648 correct (66.96)\n",
            "[[1773    0]\n",
            " [ 875    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      1773\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.33      0.50      0.40      2648\n",
            "weighted avg       0.45      0.67      0.54      2648\n",
            "\n",
            "Kappa: 0.0000\n",
            "\n",
            "Epoch: 7\n",
            "Iteration 0, loss = 0.5198\n",
            "Iteration 50, loss = 0.4980\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1773 / 2648 correct (66.96)\n",
            "[[1773    0]\n",
            " [ 875    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      1773\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.33      0.50      0.40      2648\n",
            "weighted avg       0.45      0.67      0.54      2648\n",
            "\n",
            "Kappa: 0.0000\n",
            "\n",
            "Epoch: 8\n",
            "Iteration 0, loss = 0.5213\n",
            "Iteration 50, loss = 0.4691\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1773 / 2648 correct (66.96)\n",
            "[[1773    0]\n",
            " [ 875    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      1773\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.33      0.50      0.40      2648\n",
            "weighted avg       0.45      0.67      0.54      2648\n",
            "\n",
            "Kappa: 0.0000\n",
            "\n",
            "Epoch: 9\n",
            "Iteration 0, loss = 0.4713\n",
            "Iteration 50, loss = 0.5003\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1773 / 2648 correct (66.96)\n",
            "[[1773    0]\n",
            " [ 875    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      1773\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.33      0.50      0.40      2648\n",
            "weighted avg       0.45      0.67      0.54      2648\n",
            "\n",
            "Kappa: 0.0000\n",
            "\n",
            "Epoch: 10\n",
            "Iteration 0, loss = 0.4987\n",
            "Iteration 50, loss = 0.4703\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1773 / 2648 correct (66.96)\n",
            "[[1773    0]\n",
            " [ 875    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      1773\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.33      0.50      0.40      2648\n",
            "weighted avg       0.45      0.67      0.54      2648\n",
            "\n",
            "Kappa: 0.0000\n",
            "\n",
            "Epoch: 11\n",
            "Iteration 0, loss = 0.4744\n",
            "Iteration 50, loss = 0.4847\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1773 / 2648 correct (66.96)\n",
            "[[1773    0]\n",
            " [ 875    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      1773\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.33      0.50      0.40      2648\n",
            "weighted avg       0.45      0.67      0.54      2648\n",
            "\n",
            "Kappa: 0.0000\n",
            "\n",
            "Epoch: 12\n",
            "Iteration 0, loss = 0.4851\n",
            "Iteration 50, loss = 0.4892\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1773 / 2648 correct (66.96)\n",
            "[[1773    0]\n",
            " [ 875    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      1773\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.33      0.50      0.40      2648\n",
            "weighted avg       0.45      0.67      0.54      2648\n",
            "\n",
            "Kappa: 0.0000\n",
            "\n",
            "Epoch: 13\n",
            "Iteration 0, loss = 0.5234\n",
            "Iteration 50, loss = 0.4699\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1773 / 2648 correct (66.96)\n",
            "[[1773    0]\n",
            " [ 875    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      1773\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.33      0.50      0.40      2648\n",
            "weighted avg       0.45      0.67      0.54      2648\n",
            "\n",
            "Kappa: 0.0000\n",
            "\n",
            "Epoch: 14\n",
            "Iteration 0, loss = 0.4724\n",
            "Iteration 50, loss = 0.5022\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1773 / 2648 correct (66.96)\n",
            "[[1773    0]\n",
            " [ 875    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      1773\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.33      0.50      0.40      2648\n",
            "weighted avg       0.45      0.67      0.54      2648\n",
            "\n",
            "Kappa: 0.0000\n",
            "\n",
            "Epoch: 15\n",
            "Iteration 0, loss = 0.5349\n",
            "Iteration 50, loss = 0.4830\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1773 / 2648 correct (66.96)\n",
            "[[1773    0]\n",
            " [ 875    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      1773\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.33      0.50      0.40      2648\n",
            "weighted avg       0.45      0.67      0.54      2648\n",
            "\n",
            "Kappa: 0.0000\n",
            "\n",
            "Epoch: 16\n",
            "Iteration 0, loss = 0.5176\n",
            "Iteration 50, loss = 0.4938\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1773 / 2648 correct (66.96)\n",
            "[[1773    0]\n",
            " [ 875    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      1773\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.33      0.50      0.40      2648\n",
            "weighted avg       0.45      0.67      0.54      2648\n",
            "\n",
            "Kappa: 0.0000\n",
            "\n",
            "Epoch: 17\n",
            "Iteration 0, loss = 0.4272\n",
            "Iteration 50, loss = 0.4976\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1773 / 2648 correct (66.96)\n",
            "[[1773    0]\n",
            " [ 875    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      1773\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.33      0.50      0.40      2648\n",
            "weighted avg       0.45      0.67      0.54      2648\n",
            "\n",
            "Kappa: 0.0000\n",
            "\n",
            "Epoch: 18\n",
            "Iteration 0, loss = 0.4844\n",
            "Iteration 50, loss = 0.4596\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1773 / 2648 correct (66.96)\n",
            "[[1773    0]\n",
            " [ 875    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      1773\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.33      0.50      0.40      2648\n",
            "weighted avg       0.45      0.67      0.54      2648\n",
            "\n",
            "Kappa: 0.0000\n",
            "\n",
            "Epoch: 19\n",
            "Iteration 0, loss = 0.4637\n",
            "Iteration 50, loss = 0.4697\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1773 / 2648 correct (66.96)\n",
            "[[1773    0]\n",
            " [ 875    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      1773\n",
            "           1       0.00      0.00      0.00       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.33      0.50      0.40      2648\n",
            "weighted avg       0.45      0.67      0.54      2648\n",
            "\n",
            "Kappa: 0.0000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S_q8EEjwwLf3",
        "colab_type": "code",
        "outputId": "178ce990-c810-4f8c-e495-8ceb00f2c194",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "fig1, ax1 = plt.subplots()\n",
        "\n",
        "ax1.plot(t_losses, label='Training')\n",
        "ax1.plot(v_losses, label='Validation')\n",
        "\n",
        "ax1.set_title('Losses')\n",
        "ax1.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAFZCAYAAABJ+lxSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "VFiG4aRQPpN5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task B"
      ]
    },
    {
      "metadata": {
        "id": "2YbsX0DyP1B0",
        "colab_type": "code",
        "outputId": "33054b1d-3c9b-4cc3-df83-ee4a9f0821bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "#Select data that does not have subtask_a == \"OFF\":\n",
        "train = data.TabularDataset(train_fp, format='TSV', fields = \n",
        "                            data_fields, skip_header=True, filter_pred=lambda d: d.subtask_a == 'OFF')\n",
        "\n",
        "train, valid = train.split(split_ratio=0.8, random_state=random.seed(SEED))\n",
        "\n",
        "print(f'Train size: {len(train)}')\n",
        "print(f'Validation size: {len(valid)}')\n",
        "\n",
        "#Now build vocab (using only the training set)\n",
        "TEXT.build_vocab(train, vectors='glove.twitter.27B.200d') #USE \"glove.840B.300d\" or glove.twitter.27B.200d\n",
        "\n",
        "LABEL.build_vocab(train.subtask_b)\n",
        "\n",
        "output_dim = len(LABEL.vocab)\n",
        "\n",
        "print(LABEL.vocab.stoi)\n",
        "\n",
        "#Create iterators\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid),\n",
        "                        batch_sizes=(BATCH_SIZE, len(valid)),  \n",
        "                        sort_key=lambda x: len(x.tweet), device=device)"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 3520\n",
            "Validation size: 880\n",
            "defaultdict(<function _default_unk_index at 0x7fa37d683840>, {'TIN': 0, 'UNT': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uww_bdubSnf3",
        "colab_type": "code",
        "outputId": "a0c3a488-6a63-478a-d06a-0933df2a2e4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9103
        }
      },
      "cell_type": "code",
      "source": [
        "#CONV with Glove\n",
        "embedding_dim = 200\n",
        "window_size = 3\n",
        "lr = 0.00025\n",
        "out_channels = 512\n",
        "dropout = 0.5\n",
        "n_hidden = (64, 32, 16, 8, 4)\n",
        "\n",
        "pos_weight = torch.tensor([6.8], device = device) #deals with unbalanced classes\n",
        "\n",
        "model = ClassifierGloVe(TEXT.vocab, embedding_dim, window_size, out_channels, dropout, n_hidden = n_hidden)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "t_losses, v_losses = train_helper('subtask_b', model, optimizer, loss_fn = loss_fn, epochs = 30, \n",
        "                                  train_loader=train_iterator, valid_loader=valid_iterator)"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Iteration 0, loss = 1.3340\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 1\n",
            "Iteration 0, loss = 1.3140\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 2\n",
            "Iteration 0, loss = 1.2032\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 3\n",
            "Iteration 0, loss = 0.9265\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 4\n",
            "Iteration 0, loss = 1.1293\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 5\n",
            "Iteration 0, loss = 1.0648\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 6\n",
            "Iteration 0, loss = 1.2478\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 7\n",
            "Iteration 0, loss = 1.0509\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 8\n",
            "Iteration 0, loss = 1.1639\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 9\n",
            "Iteration 0, loss = 1.1975\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 10\n",
            "Iteration 0, loss = 1.1779\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 11\n",
            "Iteration 0, loss = 1.1894\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 12\n",
            "Iteration 0, loss = 1.1272\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 13\n",
            "Iteration 0, loss = 0.9544\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 14\n",
            "Iteration 0, loss = 1.0931\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 15\n",
            "Iteration 0, loss = 1.2710\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 16\n",
            "Iteration 0, loss = 1.0482\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 17\n",
            "Iteration 0, loss = 1.1068\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 112 / 880 correct (12.73)\n",
            "[[  0 768]\n",
            " [  0 112]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       768\n",
            "           1       0.13      1.00      0.23       112\n",
            "\n",
            "   micro avg       0.13      0.13      0.13       880\n",
            "   macro avg       0.06      0.50      0.11       880\n",
            "weighted avg       0.02      0.13      0.03       880\n",
            "\n",
            "Kappa 0.0000\n",
            "\n",
            "Epoch: 18\n",
            "Iteration 0, loss = 1.1468\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 152 / 880 correct (17.27)\n",
            "[[ 43 725]\n",
            " [  3 109]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.06      0.11       768\n",
            "           1       0.13      0.97      0.23       112\n",
            "\n",
            "   micro avg       0.17      0.17      0.17       880\n",
            "   macro avg       0.53      0.51      0.17       880\n",
            "weighted avg       0.83      0.17      0.12       880\n",
            "\n",
            "Kappa 0.0078\n",
            "\n",
            "Epoch: 19\n",
            "Iteration 0, loss = 1.0760\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 122 / 880 correct (13.86)\n",
            "[[ 12 756]\n",
            " [  2 110]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.02      0.03       768\n",
            "           1       0.13      0.98      0.22       112\n",
            "\n",
            "   micro avg       0.14      0.14      0.14       880\n",
            "   macro avg       0.49      0.50      0.13       880\n",
            "weighted avg       0.76      0.14      0.06       880\n",
            "\n",
            "Kappa -0.0006\n",
            "\n",
            "Epoch: 20\n",
            "Iteration 0, loss = 0.9968\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 310 / 880 correct (35.23)\n",
            "[[215 553]\n",
            " [ 17  95]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.28      0.43       768\n",
            "           1       0.15      0.85      0.25       112\n",
            "\n",
            "   micro avg       0.35      0.35      0.35       880\n",
            "   macro avg       0.54      0.56      0.34       880\n",
            "weighted avg       0.83      0.35      0.41       880\n",
            "\n",
            "Kappa 0.0421\n",
            "\n",
            "Epoch: 21\n",
            "Iteration 0, loss = 0.9404\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 651 / 880 correct (73.98)\n",
            "[[601 167]\n",
            " [ 62  50]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.78      0.84       768\n",
            "           1       0.23      0.45      0.30       112\n",
            "\n",
            "   micro avg       0.74      0.74      0.74       880\n",
            "   macro avg       0.57      0.61      0.57       880\n",
            "weighted avg       0.82      0.74      0.77       880\n",
            "\n",
            "Kappa 0.1635\n",
            "\n",
            "Epoch: 22\n",
            "Iteration 0, loss = 1.0275\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 649 / 880 correct (73.75)\n",
            "[[598 170]\n",
            " [ 61  51]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.78      0.84       768\n",
            "           1       0.23      0.46      0.31       112\n",
            "\n",
            "   micro avg       0.74      0.74      0.74       880\n",
            "   macro avg       0.57      0.62      0.57       880\n",
            "weighted avg       0.82      0.74      0.77       880\n",
            "\n",
            "Kappa 0.1653\n",
            "\n",
            "Epoch: 23\n",
            "Iteration 0, loss = 1.2659\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 683 / 880 correct (77.61)\n",
            "[[637 131]\n",
            " [ 66  46]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.83      0.87       768\n",
            "           1       0.26      0.41      0.32       112\n",
            "\n",
            "   micro avg       0.78      0.78      0.78       880\n",
            "   macro avg       0.58      0.62      0.59       880\n",
            "weighted avg       0.82      0.78      0.80       880\n",
            "\n",
            "Kappa 0.1924\n",
            "\n",
            "Epoch: 24\n",
            "Iteration 0, loss = 1.2817\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 762 / 880 correct (86.59)\n",
            "[[732  36]\n",
            " [ 82  30]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.95      0.93       768\n",
            "           1       0.45      0.27      0.34       112\n",
            "\n",
            "   micro avg       0.87      0.87      0.87       880\n",
            "   macro avg       0.68      0.61      0.63       880\n",
            "weighted avg       0.84      0.87      0.85       880\n",
            "\n",
            "Kappa 0.2680\n",
            "\n",
            "Epoch: 25\n",
            "Iteration 0, loss = 1.0795\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 769 / 880 correct (87.39)\n",
            "[[757  11]\n",
            " [100  12]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.99      0.93       768\n",
            "           1       0.52      0.11      0.18       112\n",
            "\n",
            "   micro avg       0.87      0.87      0.87       880\n",
            "   macro avg       0.70      0.55      0.55       880\n",
            "weighted avg       0.84      0.87      0.84       880\n",
            "\n",
            "Kappa 0.1405\n",
            "\n",
            "Epoch: 26\n",
            "Iteration 0, loss = 1.1683\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 769 / 880 correct (87.39)\n",
            "[[759   9]\n",
            " [102  10]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.99      0.93       768\n",
            "           1       0.53      0.09      0.15       112\n",
            "\n",
            "   micro avg       0.87      0.87      0.87       880\n",
            "   macro avg       0.70      0.54      0.54       880\n",
            "weighted avg       0.84      0.87      0.83       880\n",
            "\n",
            "Kappa 0.1202\n",
            "\n",
            "Epoch: 27\n",
            "Iteration 0, loss = 1.1371\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 767 / 880 correct (87.16)\n",
            "[[751  17]\n",
            " [ 96  16]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.98      0.93       768\n",
            "           1       0.48      0.14      0.22       112\n",
            "\n",
            "   micro avg       0.87      0.87      0.87       880\n",
            "   macro avg       0.69      0.56      0.58       880\n",
            "weighted avg       0.84      0.87      0.84       880\n",
            "\n",
            "Kappa 0.1728\n",
            "\n",
            "Epoch: 28\n",
            "Iteration 0, loss = 1.1649\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 769 / 880 correct (87.39)\n",
            "[[765   3]\n",
            " [108   4]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      1.00      0.93       768\n",
            "           1       0.57      0.04      0.07       112\n",
            "\n",
            "   micro avg       0.87      0.87      0.87       880\n",
            "   macro avg       0.72      0.52      0.50       880\n",
            "weighted avg       0.84      0.87      0.82       880\n",
            "\n",
            "Kappa 0.0530\n",
            "\n",
            "Epoch: 29\n",
            "Iteration 0, loss = 1.1245\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 767 / 880 correct (87.16)\n",
            "[[767   1]\n",
            " [112   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      1.00      0.93       768\n",
            "           1       0.00      0.00      0.00       112\n",
            "\n",
            "   micro avg       0.87      0.87      0.87       880\n",
            "   macro avg       0.44      0.50      0.47       880\n",
            "weighted avg       0.76      0.87      0.81       880\n",
            "\n",
            "Kappa -0.0023\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AiLub0TBVF_i",
        "colab_type": "code",
        "outputId": "24b14697-f0d1-4380-8f7f-fe716490e63f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "fig1, ax1 = plt.subplots()\n",
        "\n",
        "ax1.plot(t_losses, label='Training')\n",
        "ax1.plot(v_losses, label='Validation')\n",
        "\n",
        "ax1.set_title('Losses')\n",
        "ax1.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFZCAYAAACizedRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xd8W9XdP/DP1V62LNvyHnGWR/Yg\nIWUkhQQChDIKIYzQQiltKfArXYynEGjKbMtTaHk6WIWElpEGaFhhJYxAErKn7dhxPOMtWbJl7fv7\nQ7LjJPKWrGvr8+4rL9tX6+ji+qNz7jnfI4iiKIKIiIgkQxbtBhAREdHJGM5EREQSw3AmIiKSGIYz\nERGRxDCciYiIJIbhTEREJDEMZ6JRKj8/H/X19dFuBhFFAMOZiIhIYhTRbgARhZfL5cLDDz+Mbdu2\nQSaTYeHChfjVr34FuVyOtWvX4pVXXoEoijAYDHj00UcxadKkXo+XlZXhwQcfRFNTE1QqFR555BFM\nmzYNHR0d+PWvf42jR4/C7XZjwYIFWLVqFZRKZbTfPtGYwHAmGmNeeukl1NfX491334XX68UNN9yA\nd955B+effz6eeuopbNq0CQaDAe+//z42b96M9PT0kMcnTJiAn/70p7jllltw9dVXY+fOnbjtttuw\nadMmvPXWW4iPj8f7778Pr9eL1atXo6ysDIWFhdF++0RjAsOZaIzZvHkzbr75ZigUCigUClx66aXY\nsmULLr74YgiCgHXr1mHZsmW46KKLAAAejyfk8bKyMrS0tOCqq64CAMyZMweJiYnYvXt399cvv/wS\n8+bNw0MPPRS190s0FvGaM9EY09raCqPR2P2z0WhES0sLlEol/vnPf2LXrl248MILcd1116GkpKTX\n4zabDU6nExdddBGWLl2KpUuXoqWlBVarFRdddBG+//3v46mnnsKCBQvw0EMPwe12R/FdE40t7DkT\njTHJycmwWq3dP1utViQnJwMAioqK8PTTT8PtduO5557DqlWr8Oqrr4Y8/oc//AF6vR4ffPBByNdZ\nsWIFVqxYgYaGBtxxxx146623sHz58hF5j0RjHXvORGPMokWLsG7dOvh8PjgcDrz99ttYuHAhSkpK\ncOedd8LtdkOlUmHq1KkQBKHX45mZmUhLS+sO59bWVvz85z+Hw+HAM888g3Xr1gEAUlNTkZWVBUEQ\novm2icYU9pyJRrGVK1dCLpd3//y73/0OK1euRHV1NS655BIIgoClS5d2X0fOysrCsmXLoFQqodfr\n8cADD2Dy5MkhjwuCgCeffBIPPvgg/vSnP0Emk+Gmm26CTqfDZZddhnvvvRfPPvssBEHAjBkzcNll\nl0XrNBCNOQL3cyYiIpIWDmsTERFJDMOZiIhIYhjOREREEsNwJiIikhiGMxERkcRIZilVU5M9rM9n\nMulgsTjC+pxjAc9LaDwvofG8hMbzEhrPS2i9nRezOa7Xx4zZnrNCIe//TjGI5yU0npfQeF5C43kJ\njecltKGclzEbzkRERKMVw5mIiEhiGM5EREQSw3AmIiKSGIYzERGRxDCciYiIJIbhTEREJDGSKUJC\nRETUlz//+X9RUnIYra0tcDqdyMjIRHy8EY888vs+H/feexug1xuwcOG3Q97+1FN/xNVXr0BGRmYk\nmj0kktnPOdwVwszmuLA/51jA8xIaz0toPC+h8byENlLn5b33NuDo0XLcfvvPIv5a4dDbeemrQhh7\nzkRENGrt2rUDr766Fg6HA7fffhd2796JzZs/gd/vx4IFZ+Hmm2/F88//HQkJCcjLm4D161+HIMhQ\nWVmBRYvOx80334rbb78VP//5r7Fp0yfo6GhHVVUlamtrcOedv8CCBWdh7dp/4uOPP0RGRia8Xi9W\nrLges2fPjej7YjiH4Bf98Pi98Pg8cPvdwa9eePxuuH0eePweuH0euP0eeII/65U6TE0uhEGpj3bz\niYgi6vVPy/BNceNpx+VyAT7f0AZjzyhIwfLzJg7pseXlZfj3v9dDpVJh9+6d+L//ew4ymQzLl1+G\na6657qT7Hjp0EP/613/g9/tx9dWX4uabbz3p9sbGBvzhD09j69av8Pbb/8GUKVOxfv0b+Pe//4OO\njg6sWHElVqy4fkjtHIyYDmeP34va9jocs1WjylaDY7ZqtDhb4fV7h/R8MkGGiQnjMcs8FdPNU5Cg\nNoa5xUREdKqJEydBpVIBADQaDW6//VbI5XJYrVbYbLaT7pufXwCNRtPrc02fPhMAkJKSgvb2dtTU\nVGP8+AlQqzVQqzUoLJwSuTfSQ8yEs1/0o9HRjEpbNY7ZqlFpq0ZNex18oq/7Phq5Bhn6NKjlKijl\nSqhkSihlKqjkCqhkgWNKmQIquQpKWfB2uRJKmRKNjibsbTqAUksZSi1leL30beQZczDDPBUzzdOQ\nrE2M4rsnIgqf5edNDNnLjda1eKVSCQCorz+O1157BS+88Ap0Oh1Wrlx+2n3l8r43oeh5uyiKEEVA\nJjuxsEkQwtTofozZcG7ttGJv0+HuIK601cDpc3bfLhfkyDJkIDc+G+Pis5Ebn40UXTJkwtBXly3J\nXQSL04q9TQexp2k/yqwVONpWiTfL3kW2IQMzU6Zhpnka0vQp4XiLRETUg9Vqhclkgk6nQ0lJMerr\n6+HxeIb1nOnp6Th6tBxerxd2ux3FxYfD1Nq+jclwfrPsXXxc9dlJx1J1ZkyPL+oO40xDBpSy8L99\nkyYBi7LPwqLss2B3t2Nf80HsaTyAEksZqo/WYcPRjUjTpQSDeiqyDBkQRuqjGBHRGDZp0mRotTr8\n5Cc3Y9q0mbjssivxxz8+junTZwz5ORMTk7BkyVL88Ic3Ijc3D0VFU/rtfYfDmFxKtb1+F4ptJUhR\npWJcfDZy4rKgU2rD9vxD4fB04kDLYexp3I9DrSXwBK9rm7VJmJ82F/PTZyNRY4p4O7gEJDSel9B4\nXkLjeQltrJ6X997bgCVLlkIul+PGG1fgySf/jJSU1AE/nkupgualzcYl0xZK6pdEp9RiXtpszEub\nDZfPjYMtxdjTuB/7mg/hnYqNeLfiQ+SbJmJ++hzMNE+FSq6KdpOJiAhAS0sLbr31e1AqVbjggqWD\nCuahGpM9Z2D0fILr9Dqxu3Efth7fgfK2YwACE9PmpE7HmelzkRefG9Zh79FyXkYaz0toPC+h8byE\nxvMSGnvOo5BWocG3MubhWxnz0OhowrbjO7G1fie21G3HlrrtSNEl48y0uZifPodLs4iIYgTDWUJS\ndGZcOmEpLhl/AUpay7C1fgf2Nh3Af49+gA1HN6IgcRIWpM/F9OQpUMqV0W4uERFFCMNZgmSCDIVJ\nk1GYNBkOTyd2Nu7FtuM7cLi1FIdbS6FVaFFgmog4lQE6pQ56hTbwNfhPp+j6qoVcFvlZhUREFF4M\nZ4nTKbU4J/NMnJN5Juo7GrD1+E5sr9+J3U37B/R4jVwDvTIY3godTIZ4yHwK6BRa6BRaaJWawNfg\nP50yeFyhgSICS82IiKh//Os7iqTpU3H5xIvxnQlLYXPb4fB0osPTgQ5vJxweBzqC/xxeBzo8wWPe\nwLGGjka4/R7AMvDXU8qUwQDXIl4Vh1SdGak6M9J0KUjRmWHSGIdVtIWIaDB+9KObcNddv0ZBQWH3\nsb/97S8wGhNw7bU3nHTfXbt2YP361/G73z2Be+75OR577MmTbv/Pf16D1WrFD37wo5CvVVZ2BCqV\nCjk5uVi16l7cd98qqNW9l/0MN4bzKCQTZEhQGwc9Qczj80BrlKOmoQkOrxOd3k50ejrh8Hae+Nnb\nCYenE51eJxzBn+1uO+o7GlBqKTvp+ZQyJVJ0yUjTpXQHd6o+ENxqLgUjojBbsuRCfPrpRyeF8+bN\nn+LPf/5bn487NZgH4rPPPkVBQRFycnLx0EOPDvrxw8VwjiFKuRImbRy8+sH3dl0+NxodzWh0NKLe\n0YSGjkY0OprQ4GhCbfvx0+5vUicgVWdGuiEVOXFZyI3LgnmY5VGJKLadf/4F+MlPfoDbbrsTAFBc\nfBhmsxnHjlXgN7+5G0qlEnFxcfjtbx876XGXXHI+3n33E+zYsR1PP/1HJCYmISkpuXsLyIcffhBN\nTY3o7OzEzTffirS0dLz99np89tmnMJlMeOCBe/Hyy6+hvd2ORx/9LTweD2QyGe65534IgoCHH34Q\nGRmZKCs7gsmT83HPPfcP+70ynGlA1HIVsuMykB2XcdJxv+iH1dWGBkcTGjoCYd3gaESDownFliMo\nthzpvq9GrkZ2XCZy4rKQE5+FnLgsmLVJLF9KNMqsL3sHuxtPn/cilwnw+YdWOmNWyjRcOXFZn/cx\nmRKRkZGJQ4cOoKhoKj799CMsWbIUdrsdq1b9DhkZmVi9+gFs2/Y1dDrdaY//+9//gvvvX41Jkybj\nl7+8ExkZmbDbbZg370xcdNEy1NbW4P7778ELL6zF/PkLsGjR+Sgqmtr9+Oee+xuWLbsM559/ATZt\n+hgvvPAP/OAHP0JJyWE89NAjMJkSccUVF8NutyMurvc1zAPBcKZhkQkyJGpMSNSYUJg4+aTbnF4n\n6jrqUWmrQZW9BlW2GpRZK3DEerT7PlqFFjk9Ajs3LguJGhMDm4hCWrJkKT755CMUFU3Fli2f469/\nfQFlZaV4/PHfwefzoa6uFnPmnBEynI8fP45JkwJ/p2bOnA2Xy4W4uHgcPnwQ//3vegiCDDZbW6+v\nXVJyGD/+8e0AgNmz5+Kf/3wOAJCZmY2kpGQAQHKyGR0d7Qxnki6NQoPxxnEYbxzXfczpdaKm/Tiq\nbNWotAdCu8RShpIe17P1Sh0y9ekw65KQrE2CWZsc/JoIjWLkJmQQUWhXTlwWspc7EhXCFi78Nl5+\n+QUsWXIhsrNzEB8fj0cfXY3f//5PGDcuD08++Xivj+259WNXccyPPvoANpsNzzzzHGw2G265ZWUf\nry50P87j8UIIXqY7dSOMcBTeZDjTiNIoNJiYkIeJCXndxzq9nai213b3sCttNSi1lqPUWn7a4+OU\nhh6hfSK8zdok6JW6fnvcoijCJ/rgE/3wiz74/H74RD9kDi9aOm3wiyL88EMU/YHvRX/wZzH4mB63\nwQ8gsP2oXJBDLpNBJshO/CzIAz/LehyTBY4pBDlHB4iGQKfTY8KESXj55RexZMlSAEBHRztSU9Ng\nt9uxa9dOTJgwKeRjk5PNqKo6huzsXOzevRNTpkyD1WpFenoGZDIZPvvs0+4tJgVBgM/nO+nxhYVF\n2LVrB5YsWYo9e3aeNDEt3BjOFHVahRaTTRMx2XRi83a3z4PmzhY0d7agqcfXps4WHLNV42hb5WnP\no5FrYFDqTgSv6A/+88Ev+uHz+yBCEqXkoRDkMKrjYVQbYVTHI0EdD6MqHgk9f1YbOeudKIQlS5bi\nd79bhVWrVgMArrzyavzkJz9AdnYOrr/+Rrzwwj9w6623nfa4W2+9Db/5zd1IS0vv3rxi0aLzcM89\nP8ehQwdwySXfQUpKCl588VnMmDELf/rT708aHr/llh/j0UdXY8OGt6BQKHHvvffD6/VG5D1y44sY\nMxbOi8/vQ6vTGgzs5u7Qbu5sQafXCblwogcb6Ln26MV2HZed+F4uyKHVqOBx+yEIAmSQQSYIkAky\nCMKJ7wPHAz8LwZ8BBHvivh4fAvwnjvl7flAI3sfvR6fXCaurDTa3vc8PDFqF5qTQTtSYgkvWzEjV\npUQ8vMfC70sk8LyExvMSGje+oJggl8lh1iXBrEtCISb3/4ABiNYfFb/oh81tR5vLhjaXDVaXDW1u\nG6yutu5jbS4b6h2NIR/ftWQtVZ+CNF0gsFP1ZhhV8YMeNvf5fcE174G17g5vJxK8WngdAvRKPQxK\nHdRy9bCH4z0+D9rc9h7vsQ1Wd+B9unxuCBAgCAIEAAIEQBAgQ+A1A8eFE1+D3/u7Pvz4ffB2fzDy\nwev39fig5INX9HZ/ePL6vVDKlIhTGbr/xaviEKfUI04Vd9IxvVLHZYA0ohjORFE00IIygUCzobmz\nFfWORjR0NKHR0YR6R+NpS9aAwLK1rqBO1aVAq9AEA9cRKC7TXXyms/u4y+fut70KQR6s4a6HXqmD\n4ZSvXcdFiIEPGsEAtrqCHzjcNnR4HMM6Z4OlCF7rVwgKyGQyKAQFFDIF1HI1PH4PjnfUo8re99Ck\nAAEGlR5xykBYpxgToYMBiZoEJKpNSNQkwKRJ4D7sFDYMZ6JRQClXIjk4Aa4g8eTJLk6vE42O5kBo\nBwvEBIrD1KHSXt3n82rkGuiUWpi1yYF661211ZXawAYqehUarRZ0eBxo93R0f7W42lDXUT+o96CR\na5CgjkeWISN4Xd0YHLKP7/5ZLVcDwYF+URTR/T8REIMT8LqPiyfuJwiAQqbovkyh6HEpYyCTBF0+\nF2zudtjd7bB72mF322Fzt6Pd3R48bofd0w6Ly4q6jnoU91IG16DUB4PahER1wonvNQlI1JigU2gH\n1CYihjPRKKdRaAJFXeKzTjruF/1o6bSgwdEIl88FnUIHnfLEBidauabfXcv6Gu7vGgbvGdodng50\nuB0QBOFEAAcnu2kU6rC953ASBAEahQYahQYpuuR+7+/xeSA3+FBWV4NWpxWtTgssTmvge5cFxzsa\nUGWv7fM5Tp7VLwvOgZCfmBMRnCvRdT9lsKevUagDX+VqqBVqqOUqaHr8fNJXuRoGpZ47041SDGei\nMUomyLqvzUeCXCbvvi4bS5RyJcyGRMhModfci6KIdk8HWp0WtDqtsAS/tjot6PS54PP7TlwjF098\n7w8u6/P6XT0mEPrhD15HH1JbZQpkGTKRG5+F3PhsltEdRcZkOLe0OdHm8sGo5idGIhpZgiB0f2jJ\njc8Oy3P6RT9cPhecXlfgq88Fl9cd+Hra8cD3nT4XGh1NqLRXo8J2YumhVqEJ1LsPhnVufDYS1EYO\ntUvMmAznVz4qRUm1BU//v3Mgl/ETIhGNbjJB1r3n+mC5fW7UtNeh0laDSls1Ku3Vp1Xli1MZMC4+\nG7lx2ciJz0Z2XAbilAYGdhSNyXDWqOTodPlgsbmQnDD4X2YiorFCJVedVkbX4ekMVuMLlNGttFVj\nf/Nh7G8+3H0fvVKHdH0q0vVpwa+Bf7F2GaOLX/SP6OWAMRnOyQmBa0HNbU6GMxHRKXRKLQoSJ500\n87/NZUeVvRqVtmrUttfjeEc9yq3HUGatOOmxBqX+9NA2pMKg1I/02xgQv+iHw9OJdk872j0OtLvb\n0el1Bi8JuOHq+hq8HNB17KTbvS64/R6clTEf1xV8d0TaPTbD2RgI5Ka2ThTAFOXWEBFJn1Edh2nq\nIkxLLuo+5vZ50OBoxPGOhuC/ehxvbzhtdzkgMDSeYkiCXFRC22PWuEahOWmG+am3aeRqyAQ5xGAN\ne78oQkSgfr0o+iGi65h4yjE/nD4X2t0daPcE/7nbe3zf0b2SYLBlewUIUMsDs+G1Ck1wmZ8Kecbc\nsJzrgRij4RzoObe0OaPcEiKi0UslVyI7LhPZcZknHXf53GjoOBHadR31gSVk1lp4/JGpNT0YAgTo\nlFoYlHqk6swwqAwwKHUwKA0wqPTQKbTd4dv1tWuZmlquglKmjPr19jEdzk1WhjMRUbip5aqQa+vN\n5jjUN1jh9Lng7B46dqHTe2JWudPnhMvrOuk+ftEPAUJ3gRYZgl+DZVpPPd71vVqugkGlh16pR5xS\nHwzhQPiO9vXdYzKcE+M1kAlAS1tntJtCRBRT5DI59DId9Epd/3emXo3JdUYKuQyJRi2aOKxNRESj\n0JgMZwBITdTBanfB6/NHuylERESDMqbDWQTQYmPvmYiIRpcxG84ppsD1jmYObRMR0SgzoHAuLS3F\n4sWLsXbt2tNu27p1K5YvX44VK1bg3nvvhd8fGEZ+5JFHcM0112DFihXYt29feFs9AKmJwXC2clIY\nERGNLv3O1nY4HFi9ejUWLFgQ8vYHHngAL7/8MtLS0nDnnXfiiy++gFarRWVlJV577TWUl5fjvvvu\nw2uvvRb2xvelO5zZcyYiolGm356zSqXCs88+i5SUlJC3r1+/HmlpaQCAxMREWCwWfP3111i8eDEA\nYMKECWhra0N7e3sYm90/hjMREY1W/YazQqGARhN631IAMBgCRdAbGxuxZcsWLFy4EM3NzTCZTpTN\nTExMRFNTUxiaO3BJRg1kgoBmrnUmIqJRJixFSFpaWvDjH/8Yq1atOimUu4hi/3VNTSYdFIrwVnQx\nm7RotblgNseF9XlHO56P0HheQuN5CY3nJTSel9AGe16GHc7t7e344Q9/iJ/97Gc4++yzAQApKSlo\nbm7uvk9jYyPMZnOfz2OxOIbblJOYzXEwGVQorrKits4KlXJ0l3ILF7M5Dk1N9mg3Q3J4XkLjeQmN\n5yU0npfQejsvfQX2sJdSPfbYY/je976Hc889t/vYWWedhY0bNwIADh48iJSUlO7h75HUtV0k1zoT\nEdFo0m/P+cCBA3j88cdRW1sLhUKBjRs34rzzzkNWVhbOPvtsvPXWW6isrMS6desAAMuWLcM111yD\nKVOmYMWKFRAEAatWrYr4GwmlawOM5jYn0pOkudcoERHRqfoN56lTp2LNmjW93n7gwIGQx3/5y18O\nvVVhYg7u68wZ20RENJqM2QphQGDGNsBCJERENLqM6XA2J7DnTEREo8+YDmejQQWFnGudiYhodBnT\n4SwTBCTFa9hzJiKiUWVMhzMQmLFtd3jgdHuj3RQiIqIBGfvh3LXWmb1nIiIaJcZ+OAdnbDcxnImI\naJSIgXBmz5mIiEaXGAjnYM+Za52JiGiUGPvhzGvOREQ0yoz5cI7XKaFSyNDEtc5ERDRKjPlwFgQB\nSUYNe85ERDRqjPlwBgKTwjqcXjicXOtMRETSFxvhnNC1dSSHtomISPpiI5x77OtMREQkdTERztzX\nmYiIRpOYCGfu60xERKNJTIQz93UmIqLRJCbCWa9RQK2Sc0IYERGNCjERzoIgwGwM7OssimK0m0NE\nRNSnmAhnILDW2en2oYNrnYmISOJiKJy51pmIiEaH2AtnKyeFERGRtMVOOHPGNhERjRKxE85d+zpz\nWJuIiCQu5sKZu1MREZHUxUw46zRK6NQKNLFKGBERSVzMhDMQ2J2qhWudiYhI4mIrnI1auL1+2Bye\naDeFiIioVzEWzlzrTERE0heT4cxJYUREJGWxFc7Btc6cFEZERFIWW+HMnjMREY0CMRnOTQxnIiKS\nsJgKZ41KAYNWyRKeREQkaTEVzgBgTtCgpa0Tfq51JiIiiYq5cE4yauH1iWhrd0e7KURERCHFXDib\nudaZiIgkLubC+UQhEl53JiIiaYq9cO7a15lrnYmISKJiL5zZcyYiIomLuXBOimc4ExGRtMVcOKuU\nchj1Kk4IIyIiyYq5cAYC+zq32lzw+7nWmYiIpCc2w9mohc8vwmJ3RbspREREp4nRcOZaZyIikq4Y\nD2dOCiMiIumJzXDmvs5ERCRhAwrn0tJSLF68GGvXrj3tNpfLhbvvvhtXXnll9zG/34/7778fK1as\nwMqVK1FeXh6+FocB93UmIiIp6zecHQ4HVq9ejQULFoS8/YknnkBhYeFJxz755BPY7Xa8+uqrePjh\nh/HEE0+Ep7VhkhSvgQDu60xERNLUbzirVCo8++yzSElJCXn7XXfdhcWLF5907NixY5g+fToAICcn\nB3V1dfD5fGFobngo5DIkxKnRwglhREQkQf2Gs0KhgEaj6fV2g8Fw2rHJkyfjyy+/hM/nw9GjR1Fd\nXQ2LxTK8loaZ2ahBq90Fr88f7aYQERGdRBGJJ124cCF27dqF66+/Hvn5+Rg/fjxEse+CHyaTDgqF\nPKztMJvjer0tMzUOpTVtEJQKmJP0YX1dqevrvMQynpfQeF5C43kJjecltMGel4iEMxAY7u6yePFi\nJCUl9Xl/i8UR1tc3m+PQ1GTv9XaDOvDWS442Q+6Pnd5zf+clVvG8hMbzEhrPS2g8L6H1dl76CuyI\nLKUqLi7GvffeCwD4/PPPUVRUBJlMWqu2khO41pmIiKSp357zgQMH8Pjjj6O2thYKhQIbN27Eeeed\nh6ysLCxZsgR33nkn6uvrUVFRgZUrV2L58uW45JJLIIoirrrqKqjVavzhD38YifcyKMnG4L7OnBRG\nREQS0284T506FWvWrOn19qeffjrk8ccee2zorRoBZlYJIyIiiZLWWPMIMsWrIRMENFsZzkREJC0x\nG85ymQyJ8WoOaxMRkeTEbDgDgTKe1nY3PN7Yma1NRETSF+PhHJgU1mLj0DYREUlHjIcz93UmIiLp\nie1w7lrrzElhREQkIbEdzt1rnRnOREQkHTEezhzWJiIi6YnpcE4wqCGXCew5ExGRpMR0OMtkApKM\nGjRb2XMmIiLpiOlwBgJD2zaHBy6PL9pNISIiAsBw5qQwIiKSHIZzcFJYCyeFERGRRDCcg2udm7jW\nmYiIJILh3FXCk8PaREQkETEfzl37OjdxWJuIiCQi5sM5Xq+CUiHjhDAiIpKMmA9nQRCQzLXOREQk\nITEfzgCQZNSgw+lFp8sb7aYQERExnAHAzLXOREQkIQxncAMMIiKSFoYzgOSEYM+Za52JiEgCGM7o\n2XNmOBMRUfQxnMFhbSIikhaGMwCDVgm1Us6eMxERSQLDGcG1zgkaNLd1QhTFaDeHiIhiHMM5KDle\ng06XDw6udSYioihjOAdxxjYREUkFwzmIk8KIiEgqGM5BXVtHcl9nIiKKNoZzUFfPmfs6ExFRtDGc\ng8wJ3NeZiIikgeEcpNMooVUr2HMmIqKoYzj3YDZq0GTthNPN5VRERBQ9DOceZk02w+314+MdNdFu\nChERxTCGcw8XnJENg1aJ97dVocPpiXZziIgoRjGce9CqFbj4zFx0urx4f2tVtJtDREQxiuF8ivNm\nZ8IUp8bHO6rR1u6KdnOIiCgGMZxPoVLKcelZ4+D2+vHOV5XRbk5YbfjqGP7337ui3QwiIuoHwzmE\ns6elIyVBi817atFkHTvrnrfsP45Pd1SjeQy9JyKisYjhHIJCLsPl5+TB5xfx3y8rot2csBBFEVZ7\nYJj+cJUlyq0hIqK+MJx7Ma8oFVlmPb46WI/a5o5oN2fYOpxeuL1+AEBxJcOZiEjKGM69kAkCrjx3\nAkQReOvzo9FuzrB19ZoBoLhFPfPwAAAgAElEQVTKClEUo9gaIiLqC8O5DzMmJmFCZjx2ljah4rgt\n2s0ZltYe4Wyxu9Bg4XVnIiKpYjj3QRAEfPfcCQCA9Z+VR7k1w2MNLgvLzzUB4NA2EZGUMZz7UZBr\nwpRxJhw8ZsHhURxorbbAhh7nzc0GgFH9XoiIxjqG8wBcuTDYe/68fNReq+3qOU+bkIwEgwrFVZZR\n+16IiMY6hvMA5KXHY85kM8prbdhb1hLt5gxJ1zXnJKMGhbkm2B2eMTELnYhoLBpQOJeWlmLx4sVY\nu3btabe5XC7cfffduPLKK7uPdXR04Pbbb8fKlSuxYsUKfPHFF+FrcZRcfu54CAj0nv2jsMdptbug\nVcuh0yhRkBO47syhbSIiaeo3nB0OB1avXo0FCxaEvP2JJ55AYWHhScfefPNN5OXlYc2aNXjqqafw\n8MMPh6e1UZSZrMeCqWmoaerA9kMN0W7OoFnsLpjiNACAQk4KIyKStH7DWaVS4dlnn0VKSkrI2++6\n6y4sXrz4pGMmkwlWqxUAYLPZYDKZwtDU6Lvs7DzIZQLe+qICXp8/2s0ZMJfHhw6nFyaDCgCQnKBF\nslGDkior/P7RNwpARDTWKfq9g0IBhaL3uxkMhu4g7nLJJZdg/fr1WLJkCWw2G/7+97/32xCTSQeF\nQj6AJg+c2RwX9udbumAc3t1Sgb0VFixdMC6szx8pdU3tAID04Pkwm+MwKz8FH22vgt3jx8SshGg2\nTzLC/fsyVvC8hMbzEhrPS2iDPS/9hvNQvP3228jIyMDzzz+P4uJi3HfffVi/fn2fj7FYHGFtg9kc\nh6Yme1ifEwDOn5WBj7ZV4pUPDmNabgJUyvB+oIiE8uDwtUYZGChparJjXKoBAPD1nloY1dJ/D5EW\nqd+X0Y7nJTSel9B4XkLr7bz0FdgRma29a9cunH322QCAgoICNDY2wufzReKlRlyCQY3z52bB2u7G\np7tqo92cAbEEZ2onxqm7j3FSGBGRdEUknHNzc7F3714AQG1tLfR6PeTysdM7u2h+LrRqBd7bWolO\nlzfazemXJbjGOaFHOJvi1EhL1KG0xjqqrp8TEcWCfoe1Dxw4gMcffxy1tbVQKBTYuHEjzjvvPGRl\nZWHJkiW48847UV9fj4qKCqxcuRLLly/HNddcg/vuuw833HADvF4vHnzwwRF4KyPHoFVi6fwcvPn5\nUWzcXoXLzxkf7Sb1yWI7vecMBGZtb9pdi2P1dkzMNEajaUREFEK/4Tx16lSsWbOm19uffvrpkMef\neuqpobdqFFgyNwuf7KjGxm+qcf6cLMTpVNFuUq9C9ZyBQGnSTbtrUVxpYTgTEUkIK4QNkUalwCXf\nGgeX24d3v66MdnP6ZLE7oZALiNMqTzqenxOYpc3rzkRE0sJwHoZFMzORGK/Gp7tquzeWkCKL3YUE\ngxqCIJx0PF6nQpZZj7LaNni8vO5MRCQVDOdhUCpkuOysPHh9fmz46li0mxOSz+9HW4f7tOvNXQpy\nTfB4/Tha1zbCLSMiot4wnIfpW9PSkGrS4st9x+F0S2/mdlu7G6J4+vXmLl2lPDm0TUQkHQznYZLL\nZJiTnwKfX8SRGun1Pk+scdaEvD0/OwGCwDrbRERSwnAOAyn3PrvCubees06jRG5qHMrrbHB5xkah\nGCKi0Y7hHAYTs4xQyAVJh3Nv15yBwHVnn19EmQR7/kREsYjhHAZqpRwTMoyoqrejvdMT7eacpLc1\nzj1JuedPRBSLGM5hUphrggigpMra731H0kB6zpOyjJDLpNnzJyKKRQznMCkI9j6lNrHKYndBABCv\n772CmUalQF56PI7V2+BwSm/GORFRrGE4h8n4jHiolXIcqmyNdlNOYrE7EW9QQSHv+z91Qa4JogiU\n1kir509EFIsYzmGikMswKduI4y0OWIPXeaNNFEVY7G6YDL0PaXcpDJbylFrPn4goFjGcw0hqE6va\nOz3w+vww9XG9ucuETCMUchnDmYhIAhjOYVSUmwhAOuHcXwGSnlRKOSZmxqO6sV1yM86JiGINwzmM\nslMM0GsUkul9nihAMrDtLAu6Z5xLo/1ERLGK4RxGMpmA/BwTmtucaLR2Rrs5g+o5A9IbliciilUM\n5zArlNCSqv5Kd54qLz0eKqUMxRJbq01EFGsYzmEmpd7nQAqQ9KSQyzA5KwF1zR1ok8iM87HocKVF\nEr8fRCRdDOcwS0/SwWhQ4XClBaIoRrUtAyndearuYirsPYedKIp4+8sK/P7fu/HUur3weP3RbhIR\nSRTDOcwEQUBhrgm2Djfqmjui2haL3QW9RgG1Uj7gx0ip5z+WeLw+/GPDIbz9ZQUAwO3xo6yWG40Q\nUWgM5wgozJFGwFnsrkH1mgEgJ9UArVo6M87HgrYON574125sO9SAiZlG3HxxIQDg0DFpVZMjIulg\nOEdA4bjoh7PT7UWnyzugAiQ9yWUy5GcnoNHaiZY2Z4RaFztqmtrxu5d2oLzOhgVTUvGra2diTr4Z\ncpmAQ8f4AYiIQmM4R0CyUQtzggbFVVb4/dG57tw1GWwgpTtPdeK6M8NjOPaVN+ORNTvRYnPiinPy\ncMuyIigVcmjVCozPCGw00uFkwRciOh3DOUIKc03odHlR2WCPyut3h/Mge84AUMA628MiiiI+2lGN\np9btg88v4seXTcGlZ+VBEITu+0wZlwhR5DkmotAYzhFSGOVSnsMJ56wUAwxaJQ5XRX/G+Wjj9fmx\n9sNS/PvjI4jTqXD3dbMxrzD1tPsVjQv8fhzk0DYRhcBwjpCCKM96PhHOA6sO1pNMEFCQk4BWmwtN\nEqh0Nlo4nB489cZebNpdiyyzAfffOBfjM+JD3jcvIw4alZyTwogoJIZzhBj1KmQm63Gk2hqV9axd\na5yH0nMGov/hYrRptHbi4TU7cfCYBTMmJOHeG2Yjydj7ByO5TIaCHBMaLZ1o5gcgIjoFwzmCCnNN\ncHv9OFo38utZLbbhhTPXOw9cabUVv3tpB463OHDBGdm447vToVUr+n1cUXBW/yGeYyI6BcM5gqIZ\ncJZ2F5QKGfSa/kMilLREHYx6FYqrrLzu3Ict+4/j9//eDYfTixsvzMeK8ydBJhP6fyCAKXmB684c\n2iaiUzGcIyg/JwGCEKVwtrtgilOfNEN4ME6qdNbiCHPrxob3t1bi+XcPQ62U465rZmDRrMxBPT4t\nUQdTnBqHjlng5wcgIuqB4RxBOo0S49LicLTOBpfbN2Kv6/X5Ye9wD2mNc08FEtphS2raOz1484sK\nJBhU+J8b52BKcPb1YAiCgKJcE9o7PahuaI9AK4lotGI4R1hBrgk+v4gjNSO3kYS13QURgCme4Rwp\nXx+oh9fnx5IzspGepB/y8xR1DW1XcmibiE5gOEdYUW7XH9+RCzir3Q1gaNXBejIbNUiK16C4isOu\nPYmiiM17aqGQCzhrWvqwnqso+AGIpTyJqCeGc4RNzDJCLhNG9Lpzqz1QE3uoM7W7CIKAgtwEdDi9\nqGnksGuXIzVtON7iwOzJZsTrVMN6LqNBjUyzHqXVVni8I3fpg4ikjeEcYWqlHBMyjaiqt49YHWXr\nMKqDnSoaM85bbU40tEp3EtrmPbUAgEUzBzcBrDdTxiXC4/WjrIZbSBJRAMN5BBTlmiACKK4cmevO\nrcOoDnaqgpyRve58pMaK3zy3Datf2jGik+gGqr3Tgx3FTUhL1CE/WIN8uLrWO7OUJxF1YTiPgJGe\nWGUdZnWwnhLjNUhN1OFARSt2ljQN+/n6UlJlwZOv7YXT7YPD5cXussi+3lB8tf84vD4/Fs7MGPIy\ntVNNzk4IbiHJSWFEFMBwHgHjM+KhUspweIS2YGy1uyATBBj1w7se2uXGCyZDIZfh/97aj827a8Py\nnKc6fKwV//v6Xnh9fly1aAIAYOvBhoi81lAFJoLVhWUiWE8alQITMo2orLejvZNbSBIRw3lEKOQy\nTM5OQF1zR3evNpKsdheMBtWAK1X1p3BcIn593SwYtEq8vLEEb39ZEdaqYQcqWvCndfvgF0X89Mpp\nuPjMXOSkGnCwohV2hztsrzNcpdVW1Lc6MDc/BQatMqzPXTSu69IHh7aJiOE8YgpHaGjbL4rd1cHC\nKS89HvfdMAfJRg3e/rICazaWwO8ffkDvK2/B0+v2QxSBO747HTMnJgMAzixKg88v4pvixmG/Rrhs\n3lMHAFg4MyPsz91VxIRD20QEMJxHTFc4R3q9c7vDA59fHPYa51BSE3X4n5VzkJ1iwOY9dfjrWweG\ntfxnz5Fm/GX9PggC8P+umo5p45O6b5tflAoBwNcH68PQ8uGzO9zYWdKI9CQdJmeHZyJYT+PS46BV\ny7nemYgAMJxHTE5KHPQaRcR7zpYwLqMKxWhQ4+7rZqMgJwE7S5vwx9f2wjGEJWI7S5rwzJv7IZMJ\n+NlV07s3gehiilOjINeE8lobGiWwpeKW/fXw+kQsnJkZtolgPXVvIWnt5B7aRMRwHikymYD8HBOa\n25wR/eMb6XAGAJ1GgbuWz8TcfDNKq6147JVd3a87EDuKG/G3tw9AIZfhrqtnoLCXutRnTkkFAGyL\ncu9ZFEV8trcOCrkM35qaFrHXKeLQNhEFMZxH0EgU9LCEqTpYf5QKGX582VScNzsTNU0deGTNThxv\n6ej3cdsONeBvbx+EUiHDz6+ZgfzgOupQ5kxOgUIuw9ZDDVHdtrK4yoqGVgfOKAj/RLCeuvd35tA2\nUcxjOI+gEQnnMK5x7o9MJuD6JZNxxTl5aLE58ejaXThaZ+v1/l8fqMc/NhyEWiXDL66ZiUlZfV+7\n1WkUmDkxCcdbHKiK4q5NnwUrgkViIlhPaYk6JMarcbhy9NYyF0URT72xF398bQ/3AScaBobzCEpP\n0sFoUOFwpSVif7gstpELZyBQf/vSs/Lw/YsK0OH04Il/78L+oy2n3e+LfXV47p1D0KoU+OWKWZiQ\naRzQ8y+YEhhGjtbEMFuHGztLmpCRrMekrIG1eagCW0gmjuotJEuqrNhb3oKDFa1R2cecaKxgOI8g\nQRBQmGuCrcONuub+h4CHoqvnnBCB2dp9OXdGBm6/YhpEEXh63T58deB4922f7anFi+8VQ6dR4FfX\nzkJeevyAn3fahCToNQpsO9wQlqVbg7Vl/3H4/GJYK4L15UQpz9F53fndrZXd33+wvSqKLSEa3QYU\nzqWlpVi8eDHWrl172m0ulwt33303rrzyyu5jb7zxBlauXNn9b9asWeFr8ShXmBPZoW2L3QWDVgmV\nUh6R5+/LrMlm/OKamVAr5XjuncP4YFsVNu2qwUsflMCgVeJX185CblrcoJ5TIZdhbkEK2trdI1Zh\nrYs/OBFMqYjsRLCeCkfxpLDKejsOVrSiICcBk7OMOHC0FTVNo3MEgCja+g1nh8OB1atXY8GCBSFv\nf+KJJ1BYWHjSsauvvhpr1qzBmjVrcMcdd+Dyyy8PT2vHgEhfd7bYXSPea+5pcnYC7rlhNkxxary+\nqQxrPixFvE6JX183CzmpgwvmLmcWBWZtbx3hoe3iSgsaLZ2YV5ACvSZyE8F6MupVyDIbUFrdBrdH\neht/9OW9YK/54gW5uHB+DgDgw+3V0WwS0ajVbzirVCo8++yzSElJCXn7XXfdhcWLF/f6+GeeeQa3\n3Xbb0Fs4xiQnaGFO0KCkyhr2YdpOlxdOtw+J8dELZwDIMhtw3w1zkJmshylOjV9fNxtZZsOQn29S\ndgIS49XYWdI0ooHVXRFsVni2hhyoonEmeH1+HKkdPVtINrQ6sKOkETmpBkwZl4gZE5ORmqjD1kP1\nI1Kylmis6TecFQoFNJretx40GHr/o7tv3z6kp6fDbDYPrXVjVGGuCQ6XF5UN9rA+b9da42j2nLsk\nGTV46OZ5eOxHC5CRrB/Wc8kEAWcWpcHp9mFPWXOYWti3tg43dpc2Icusx4SMgV8jD4eugiyjaWj7\ng+1VEEXg4jNzIQgCZIKAC8/Ihtcn4pOdNdFuHtGoo4jkk69btw5XXHHFgO5rMumgUIT3OqnZPLRh\n1EibNzUDn+89jupmB+ZND1+vrKY1UNwkKy2+z/cu1fPSl4vPHo/3tlZid1kLLjl3YkReo+d52byv\nFD6/iEvOHo+UlJEN52/Fa/Hn/+zHkZo2Sfy36q8NrTYntuyvR3qSHkvPngB5cMOV73x7Et76sgKf\n7anD9y+dCo06on9uBsXvF/HZ7hpo1QqcOXVoO4xJ4b+NFPG8hDbY8xLR/7ds27YNv/nNbwZ0X4vF\nEdbXNpvj0NQU3p5puGQmagEA3xyqx7nTwjfR6FiNFQCgkqHX9y7l89IXnUJAltmAHYcbUFHVGvZi\nID3Pi18U8f5XFVApZJiWmxCV8zUxMx4lVdaIvNfBGMjvyxubyuD1+bHkjCy0tpw8AWzRzAz8d8sx\nvLXpCM6fkxXJpg6Yxe7CC+8dxsGKVshlAh74/hnIThncZZfR+v+jSON5Ca2389JXYEdsKVVDQwP0\nej1UqvDsKTyWGPUqZCbrcaTaCq/PH7bn7aoOljhCa5xH2oIpqfD5ReyI8E5Vh49Z0GR1Yl5hKnQj\nNBHsVIXjEiEisgVrwsHh9GDT7loY9SqcFWJG+3mzs6BUyLBxe1VUlsKdavvhBjzw/DYcrGhFXno8\nfH4RL31QLIm2EfXUbzgfOHAAK1euxJtvvomXX34ZK1euxIsvvoiPPvoIAHDnnXfi5z//OSoqKrBy\n5Ups2LABANDU1ITExNA1kwkoyDXB7fWjPIyTfiztgb2PE8ZoOHftVBXpWdubuyqCzYpsRbC+dK93\nrpD2dedNu2vhdPtwwRnZUIa4LBUfDO3mNid2lTZFoYUBDqcH/9hwEH97+yA8Pj9WXpiP39w4B/MK\nU3C0zoZNu2uj1jaiUPod1p46dSrWrFnT6+1PP/10r4977rnnht6yMa4o14RPdtbgcKWlz/rSg2Gx\nje2ec2K8Bvk5CSiusqK5rRPJRm3YX6Ot3YU9R5qRnWLA+EEUSwm3vLR4aNUKSU8Kc3t8+OibamjV\nCizqY0b7kjOysXlPHT7YXoU5+eYRKebS0+FjrXju3cOw2F3IS4/HDy8tQlqiDgBw7eLJOHC0Ff/5\nrByzJiUjMb73ya9EI4kVwqIkPycBgoCwbiFpaXdBpZRBK6GJN+F2ZrCc57ZDDRF5/i/2BSqCLRqh\nimC9kckC1eSa25yS2DIzlC37j8Pm8OC82Zl9/s6lJ+kxc2IyjtbZUDaCy8M8Xh9e/eQIfv/qHrS1\nu3H52Xm4b+Xs7mAGApeYlp83EU63D698VDpibSPqD8M5SnQaJXJT41BeZ4PLHZ61uxa7C6Y4TVRD\nJdLm5puhkAvYejD8O1X5RRGf762DSinr/hAQTd27VElwaNvn9+P9bVVQyGVYPDe73/svDRYl+WDb\nyJT0rGqw47f/3IEPv6lGaqIO/3PjHHzn7DzIZaf/yTtnejrysxOw+0gzdpZEb+idqCeGcxQV5prg\n84s4Umsd9nN5vH7YHR6YDGN7Ap5Oo8T0Ccmobe5AdWN4S0MeqmhFc5sT8wtTJTH6MEXCpTy/KW5E\nc5sT50xPh1Hf/+/cpCwj8tLjsOdIMxpaw7syoye/X8R7Wyux+qUdqG3uwHmzM/HgTWf0Wc9dEATc\nuDQfCrmAVz4qgcPpjVj7iAaK4RxFBcFSnsWVww9na/dWkWP/mll3Oc8wD213VQTr6/rpSEoxaZHU\ntYWkhGYTi6KI976ugiCgu0xnfwRBwIXzciAC+PCbyJT0bLJ24vF/7cK6zeUw6JS4a/kM3HBBPtQD\nqDOfnqTHsm+Ng7Xdjf98Xh6R9hENBsM5iiZlGSGXCWFZLtNVHWyktoqMphkTk6BVK7DtUPh2qmpp\n68SeI83ISTVg3CA354gUQRBQNC4RHc7wV5Mbjv3BDS3mFaYiJWHgk/Lm5JuRFK/Bl/uPw+5wh609\noijii311eOCF7ThS04a5+Was/sF8TBufNKjnufjMXGQk67F5Vy3KakZP6VQamxjOUaRRKZCXHo/K\nejs6XcMbSoulcFYq5Jibb4bF7kJJ9fBHHQDg4+1V8IsiFs3MlNQ1+yIJDm13bXBx0QB7zV3kMhku\nOCMbHq8/bEuXvD4//v7fg3jxvWLIBOCHy4rwk8unDqlwi0Iuw/eW5kME8NIHxWGtQUA0WAznKCvI\nTYBfFFE6zJCJpXAGTszaDseaZ79fxMZtlVCr5JgfHDKXiq5dzA4dk0YxkrKaNpRWWzFtfNKQdhk7\ne3o6dGoFPtlZA493eBMhPV4/nlm/H9sPN2JilhG/vXk+FkxNG9aHq0lZCVg0KxO1zR14f4QmrxGF\nwnCOsnDt7xxr4ZyfkwBTnBo7SpqG/Uf+QEULmiydOLNIGhPBeorXq5CTYsCRGmlsIdm9LeSZg+s1\nd9GqFVg4KwN2hwdfHRj6ByuP14dn3tyPveUtmDLOhF9cMxNJxvDMt7hq4XgYDSps2HIM9WGavCaK\nIr4pboxqIRYaXRjOUTYh0wiFXEBx1TDDuT22wlkmCJhflIpOlxd7y1qG/Dx2hxsbthwDACycGb2K\nYH0pGpcY2EIyytdBa5vasaesGRMy4zE5O2HIz7N4TjbkMgEfflMN/xCWw7k9Pvx5/X7sK2/B1LxE\n3PHd6QOa9DVQOo0S1y+eDK/Pj5c/KB72kr0Opwd/fesA/vrWAfxl/X48/+6hsC2fpLGL4RxlKqUc\nEzONqG5oR3unZ8jPY7E7IZcJiNeN7aVUPQ131va+8hY88Px2lNfZMH9KGsalRa8iWF+6S3lG+bpz\n1zBv17aQQ2WKU+PMolQcb3FgX/ngPli5PD78+T/7cOBoK6ZPSMId350GVRiDucucfDNmTkxGcZUV\nX+4/PuTnKa22YtUL27GjpAmTsozITYvDlv31+O1L36AmzEsBaWxhOEtAQY4JIoCSqqFfd7baXTAa\nVJDJpDOZKdKyUwzITNZjX3kzOpwD/2Dj8viw5sMS/OmNvWjv9ODqb0/Avd+fF8GWDs+k7AQo5EJU\nJ4U1t3Vi26EGZCTrMWNi8rCf74J5gWHxD7cP/Lquy+PD0+v24eAxC2ZOTMZPr5gWsp53OAiCgBsu\nmAy1So7XPy2DrWNws8t9fj/e/PwoHv/XLljsLlx+dh5+fd0s3HfDHCyZm43jLQ6sfnkHPttTG/Zi\nOjQ2MJwloHu98xCHtv2iCGu7O2aGtLsIgoAzp6TC6xMHXNnpWL0ND734DTbtqkVmsh73f28uLpqf\n270HsRSpg6MrVQ3tYV2CNBgfbq+Gzy/iovk5kIVhNnt2igFT8hJRXGVFxXFbv/d3uX146o29OFxp\nwaxJybjtiqlQKiL75ysxXoMrzx2PDqcXr35yZMCPa7Z24rFXdmHDV8eQGKfBPdfP7q5OplTIcO3i\nSbjjymlQKWR46YMS/P2/B4e9WoPGHoazBIzPiIdKIRtynW17hxs+vwiTIbbCGUD37Or+Zm37/H5s\n+OoYHn55J+pbHVgyNxsPfH/ukGYcR8OUvMCSqmhsIWl3uPH53jokxqvDOpt9abD3vLGf3rPT7cX/\nvr4HxVVWzJlsxk8unwqFfGT+dJ0/Owt56XHYeqgBB472PwS/7VADVr24HeW1NswrTMFDN5+BSVmn\nX5+fNdmMB2+ahwmZ8dh+uBEPvfgNKuuls5adoo/hLAEKuQyTsoyobe5A2yCHzwCg1R471cFOlWzU\nYnKWEcVVVrQGd+U6VaO1E4+/shtvfn4U8XoVfrFiJq5dPCliQ6KREM31zp/srIHb68eF83LCGopF\n40zIMhuwo7gJzW2hN/fodHnx5Ot7UVrThrkFKfjRZVNGLJiBwAYk31taAJkg4OWNJb1O5Op0efH8\nO4fw9/8ehN8P3HxxIX70nSl97geeZNTg7utm4+Izc9Fo7cTDa3bg4x3VHOYmAAxnyega2i4ZwtB2\nrC2jOlVvO1WJoogv9tZh1QvbUVbbFuzJzOuuWT2a5KbGQa9RYG95y4guqep0efHJzhoYtEqcOz28\ns9kDJT2z4RdFfLyj5rTbHU4vnnx9D8pqAv/tfvSdohEN5i45qXG4cH42mtuceHtLxWm3Vxy34aF/\nfoMtB+oxLi0OD950Bs6enj6gSXMKuQxXLZqAu5bPgFatwL8+PoK/rN8/qDkUNDYxnCXiRJ1thvNg\nzS1IgVwm4OuDJ8LZ5nDjL+v348X3g5WjLi3Cj74zZUiVo6RAJhNw7owMtLW7R7Q4xsatx9Dh9GLx\n3CyoVeEfaZhflIoEgwqf7a2Do0cgOZwePPn6HpTX2nBmUSp+eGlRyB2lRsp3zsqDOUGDD7dXdw8/\n+0UR72+txCNrdqLR0omL5ufgvpVzkNpjS8qBmjY+CQ/eNA8FOYHdsR584RuUj+D2miQ9DGeJGJcW\nB41KjsNDmLEd6+Fs0CoxbXwSapraUdPUjn3lzXjg+e3YfaQZ+dkJeOjmeVgwZXiVo6Rg2bfGwWhQ\n4b2tlWgagT2ePV4/3txcDrVSjvNmZ0XkNbq2nHS5ffhsb2DjkQ6nB394dQ+O1tmwYEoablkW3WAG\nApPybrywAH5RxD8/KEaTpRN/fHUP3ghusvGLFTNx9bcnDqtnb4pT45crZuGys/PQanPisVd24f1t\nlUNaC06jH8NZIuQyGSZnJ6Ch1dEdtgMV6+EMAGdOCUxUemb9fvzpjX3oCC6R+tW1s5BsHPjmDFKm\nVStwzbcnwuP1D2r28FBtPViPVpsTC2dmRHTEYdHMDKhVcny8owZtHW784d97cKzejrOmpeEHlxRK\nZnnglLxELJiShsp6O2599GMcrgws6QrnpRKZTMBlZ+fhl9fOgkGnxBubyvHUG/tgi9IsfYoehrOE\nFOQMbWjbYg9MhEqIwdnaXWZOTIZGJUeDpfOkJVJS+cMeLvOLUjE5y4jdR5qxfwCzh4eq0+XFhq+O\nQSEXcMEZ2RF7HSBQkeuc6emw2F24/7ltqGyw45zp6bjpYukEc5drzp8Ig1YJQQCuXzIZd3x3WkQK\n/xTmmvDQTfMwNS8R+6f4waoAABMzSURBVI+24PFXdsElgfKtNHIYzhLStcnBYJfLWNrdiNMpI77u\nU8pUSjluvrgQ3104flQtkRosQRBw/QX5EATgXx+VwuONzM5J//q4FM1tTlx27gQkxkd+FcAFc7Mh\nCEB7pwcLZ2bgexcVhGU9dbjF61R48KYz8I97F+P8OVkRvVQSr1fhZ8tnYNGsTBxvceCNTWURey3q\nmyiK2FHcOKJbicbuX3MJyk41QK9RDKoYiSiKsNidMbnG+VRzC1JwyYJxo2qJ1FBkpxhw3uwsNFg6\n8dGO6rA///bDDdiyvx65aXG4fmlh2J8/lOQELVZekI+rvz0BKy/Ml2Qwd0mM1yB5EPtYD4dMEHDt\n+RORkazHp7tqIzpaQqG1d3rwl/X78X9vHeje+GUkMJwlRCYImJydgOY254An/HS6vHB7/DF9vTkW\nXXFOHuJ0SmzYcqzX9d1D0dLmxEsflECllOHWS4tGdDRm0azMwKUICQdzNCgVcvxwWRHkMgEvvHd4\nWDX4aXAOH2vFA89vw+4jzSjIScDKC/NH7LUZzhJTOMglVa2cDBaTdBolrlo4AS6PD6+HabjT7xfx\n7DuH0Ony4trzJyE9SR+W56Xhy02Lw+Xn5KGt3R2WnbKob16fH+s2l+MPr+6B3eHBdxeOxy9XzBrR\nv7MMZ4kZbJ1tK8M5Zp01PR156YHyj0Mt/drTe1srUVptxezJZpw7Q5rbZ8ayi+bnYlKWETtKmvB1\nP+VqaegaLA48unYn3ttaieQEDe69YQ4uWTBuxCcnMpwlJjNZjzidEsVV1gF9Ou7qOScwnGOOLLhz\nkgDglY9L4fUNfXLY0Tob3v6yAgkGFb5/UcGoXxM+FslkAm5ZVgS1So5XPirtteQpDY0oitiy/zge\nfPEbVBy341tT0/DgTfMwPiM6W8kynCVGEAQU5JhgsbvQYOn//3xdPefEGKyrTUBeejzOmZGB2qYO\nbNpVO6TncLq9+MeGg/D7RdyyrGjUVlGLBeYELa5bPAmdLh+ef+cwC5SEicPpxbMbDuH5dw9DAHDr\npUW4ZVkRtGpF1NrEcJagwZTyZM+ZvrtwPPQaBd768uiQNk7510dH0GjpxIXzc7o32CDpOntaOmZN\nSkZJtRUfbg//bP1YU1bbhgdf3I6thxowPiMeD948r7tefzQxnCVoMOudre1dPWeGc6yK06lwxbnj\n0eny4T+bywf12G+KG/Hl/uPITY3DleeOj1ALKZwEQcD3LipAvF6F9Z+Xo7qxPdpNGpX8fhEbtlTg\nsbW70NLmxLJvjcM9189Gyggtk+sPw1mCUk1aJBhUKKmy9HvdudXmglolj+rwC0XfopmZyEkx4Mv9\nxwe8YUKrzYmX3i+GSiHDrVHa8YmGJl6nwk0XFcDrE/HshoMRK0YzVrXanHji37vx5hcVMBpU+PV1\ns3DlueMl9f8B6bSEugmCgIJcE2wOD+qaO/q8r7XdxV4zQSYTcP0FkwEAaz8qhd/f94c6v1/EsxsO\nweHyYsViLpsajWZMTMaimRmoaerAm18cjXZzRo3dpU144PntKK22Ys5kMx66eR7yg6WTpYThLFGF\nOf0Pbbs9PrR3emK6pjadMCkroXtjhs/31fV53/e3VaKk2opZk5KxkMumRq3l501EikmLjduqhrQX\nfKwprrTgmTcPwOvz43tL83HbFVMlOwGS4SxRJ9Y7976FJK8306mu/vYEaFRy/Gdzea+VpCqO2/BW\ncDiPy6ZGN41KgR8uK4IgCHjunUNwOL3RbpJktbQ58de3D0AQgLuWz8DCmZmS/t1nOEuUOUGLZKMG\nJVWWXpdLWDhTm06RYFDjsrPz0OH04s3PTx/qdLq9+Md/D8IXXDYVF4EdlWhkTcg0Ytm3ctFic+Ff\nH5f+//buPSiqO0vg+LfppuUNDdKgEiARFBw0q4lW2jwUxRidpFyd1YmOEpOUFcOiGVPGWMaYmXUT\no3FSFTVZI4m7Mzq7UkteTu3uyDhqjeuiCSYT04wKGB+AzUsaGmiaR9P7B9gRbIMoem8351NFVXMv\nXRxO/YrT93fP73eVDkeV2tqd7PjsOxrt7Tw9PVmV09i9SXFWsZR4A82ODsqqPHdjWhvlyllcb/oD\ncQyLCuLINxVcrGzsce4/DpZQZW1h5qR7BuwZxEJ5T05OJDE2lP8zV1J4plrpcFTF5XKx58BZLlY2\n8sjYYUybMELpkG6KFGcVS0mIAG68ladcOQtPdFo/fjFjFC5g75/OumdeCs9Uc/SUhXhjCPMeG6ls\nkGJA6bR+LHtqDHqdH7/94xn3La/+6nB2cu5yA6cv1FFe04Stua3P5kK1O/R1BcfMldw7LJQlM0ep\neir7WrL+RsVSrmkKmzkp/rrzVtkdTNzAmMRIHkwxUnimmgJzJakJBn77x6vLpn4yqJ/97auGRQUz\nPz2J3/+pmN3/fZpV8+/vsxDZHe2UVtgoKa+npLyB8xbbdcuyNEBIkD9hQXrCgvWEdr8ODdYTdu3r\nYD2Rkerq+j97ycq+P5cQFuTPP84d61WPk5XirGKRYQHEGAIpLqvH2dmJ1q/nP1S5chY/5ufpSZw6\nV8t/HjlHrCGQZkcHS2aOZvhQdf0DFQNn2oQRfFtai/n7Oo58U0H6hDj3OZfLxRWbg5LyBkrLGygp\nr6eippmr18UaIM4YQnJcOCGB/jS2tNPY3IatuQ2bvZ36plYq+ljaOSI6mGVPjiE+JvTO/ZE3qc7m\n4IPPzQC8+PdpRIZ510WMFGeVS00wcOSvl7lQ2cjI4eE9zlmbWtH6aQgNUudSAKGsqPAAnjQl8ulf\nvsfW3Mb45K51scJ3aTQanp2dyoaPT5B7qJTIsABqGxzuK+OrH+gB9Do/RsdHkBQXwai4cO4bHk5Q\nwI+XhA5nJ432dmzNbTTa27DZ27A1t9Nob6O6voWTZ2v459+dZGFGMlP/brhiU8jtHU7e724A+8WM\nUV7RANabFGeVS+kuzmcuWq8vzo2tGEKHyMPpxQ3NnBTPib9V0dLWIcumBglD6BCeeSKFDz43817e\nKffxsCB/HhgVTXJcOElxEcTHhPR7Ryyd1g9D6JAbPqL2Qk0zv/n9SfYcOMvZS1aeeSLlru9e6HK5\n+N2Bs5y3NPJwWqzXNID1JsVZ5a5+4jtzqZ6fmn447uzspKGpjftGKPM4M+Ed/HV+vP7Mg7iAIf7e\nc79N3J4HU4z8bMp9VFtbSI6LIDkuHKMh8I5/OJs4JpZfPzeJnV8U8eXpai5UNvLinDQSYu/eNPeh\nrys49l0libGhZD4x2ms/kEpxVrnwYD0jhgZTUl5Ph7PT/UnX1txOp8sly6hEn/RSlAeln5oSFfm9\nkWEBrFk0ns/+8j3/c+ISb+45ycLpSUwdf+c3/Sguq2ffn0sIDfIne553NYD1Ji2bXiAl3kBbeyff\nX7a5j7mbwWTrTiGEyui0fsxPT+KX88cRoNeyJ7+YnV8U0dJ653Ywq7M5+OCz73C5IMsLG8B6k+Ls\nBTw939na6ABkAxIhhHqNGzmUXz07kaS4cL46U82v//Wr6zbGGQhdDWBmbPZ2fj49ySsbwHqT4uwF\nRsdHoKHnZiSyjEoI4Q0iwwJYs3A8sx9KoLq+hTf3FHL46/I+H4d7s1wuF3vyizlvsTE5LZaMB+L6\nfpMXkOLsBUIC/bnHGEJphY22dicgG5AIIbyHTuvHP0wdyS/n30+AXsee/GL+5YuiAXlQx5FvKvjf\nUxYSYkPJnOm9DWC9SXH2EikJhq6t9SoagK41zgARofLgAiGEdxg3MopfPTuR5LhwCs9U80//dnvT\n3MVl9fz7we4GsLljfar5Ubq1vURKgoH8r8o4fame1MRIrLZWNEhDmBDCu1zt5v786Hn+q+Aib+4p\nxPSTWEIC/QnQaxmi1xGg13Z/Xfv6h3N6nR/1TW188LkZlwtenJNGVLhvzSJKcfYSo++JwE+jcTeF\nWZtaCQ3W93sTASGEUJrWz4+fTRnJqHsiyPnD3zh6ytKv92s0oPXT0OF0sXB6srtp1pfcVHEuLi4m\nKyuLpUuXsnjx4h7nWltb2bBhAyUlJXz66afu4/v37+ejjz5Cp9OxcuVKpk6dOqCBDzaBQ3QkxIZy\n3mLD0daBtbFV9kgWQni1sfdF8c6Lk6ltaMHR5uz+6ujxurXdiaP1mnPtXa9b25zcnzSUjAd9owGs\ntz6Ls91uZ+PGjZhMJo/nt2zZQmpqKiUlJe5jVquV999/n08++QS73c727dulOA+AlIQIzlts/LW0\nlvaOTgwypS2E8HJD9FpGRIcoHYbq9DknqtfrycnJwWg0ejy/atUqMjIyehwrKCjAZDIREhKC0Whk\n48aNAxPtIJfaPXVTYK4CwBAmxVkIIXxRn1fOOp0One7GPxYSEkJ9fX2PY+Xl5TgcDpYvX47NZmPF\nihU3vPK+ymAIQjfAW61FRyv/2LKBZAoLRJd3iqILdQDExYTd0t/oa3kZKJIXzyQvnklePJO8eNbf\nvNyxhrD6+np27NjB5cuXyczM5PDhwz+6/sxqtQ/o74+ODqWmZuB3olFa4rAwSsu7llPp/ej33+ir\nebldkhfPJC+eSV48k7x4dqO8/FjBviOtvlFRUYwfPx6dTkd8fDzBwcHU1dXdiV816KResy3djR7b\nJoQQwrvdkeL8yCOPcPz4cTo7O7FardjtdgwG32t1V8K1SwakOAshhG/qc1rbbDazefNmKioq0Ol0\nHDhwgGnTphEXF8eMGTNYuXIllZWVnD9/niVLlrBgwQKeeuopZs6cyYIFCwBYv349fn6yHncgJI0I\nQ6f1o8PZKcVZCCF8lMY1ULuP36aBvk/hy/c+dv2hCEutnTeendjv9/pyXm6H5MUzyYtnkhfPJC+e\n3co9Z9khzAste3KMz2zuLoQQ4noy1+yFpDALIYRvk+IshBBCqIwUZyGEEEJlpDgLIYQQKiPFWQgh\nhFAZKc5CCCGEykhxFkIIIVRGirMQQgihMlKchRBCCJWR4iyEEEKojBRnIYQQQmWkOAshhBAqo5qn\nUgkhhBCii1w5CyGEECojxVkIIYRQGSnOQgghhMpIcRZCCCFURoqzEEIIoTJSnIUQQgiV0SkdwJ3w\n1ltv8e2336LRaFi3bh3jxo1TOiTFnThxgpdeeonk5GQARo0axeuvv65wVMopLi4mKyuLpUuXsnjx\nYiwWC2vWrMHpdBIdHc0777yDXq9XOsy7rnde1q5dS1FREREREQA8//zzTJ06VdkgFbBlyxZOnjxJ\nR0cHL7zwAmPHjpXxwvV5OXTo0KAfLy0tLaxdu5YrV67Q2tpKVlYWKSkp/R4vPlecv/zySy5evEhu\nbi7nzp1j3bp15ObmKh2WKkyaNIlt27YpHYbi7HY7GzduxGQyuY9t27aNRYsWMWvWLN59913y8vJY\ntGiRglHefZ7yAvDyyy+Tnp6uUFTKO378OCUlJeTm5mK1Wpk7dy4mk2nQjxdPeXnooYcG/Xg5fPgw\naWlpLFu2jIqKCp577jkmTJjQ7/Hic9PaBQUFZGRkADBy5EgaGhpoampSOCqhJnq9npycHIxGo/vY\niRMnmD59OgDp6ekUFBQoFZ5iPOVFwMSJE3nvvfcACAsLo6WlRcYLnvPidDoVjkp5s2fPZtmyZQBY\nLBZiYmJuabz4XHGura3FYDC4v4+MjKSmpkbBiNSjtLSU5cuXs3DhQo4dO6Z0OIrR6XQEBAT0ONbS\n0uKeZoqKihqUY8ZTXgD27t1LZmYmq1atoq6uToHIlKXVagkKCgIgLy+Pxx57TMYLnvOi1WoH/Xi5\n6umnn2b16tWsW7fulsaLz01r9ya7k3ZJTEwkOzubWbNmUVZWRmZmJvn5+YPyPllfZMz8YM6cOURE\nRJCamsquXbvYsWMHGzZsUDosRRw8eJC8vDx2797N448/7j4+2MfLtXkxm80yXrrt27eP06dP88or\nr/QYIzc7XnzuytloNFJbW+v+vrq6mujoaAUjUoeYmBhmz56NRqMhPj6eoUOHUlVVpXRYqhEUFITD\n4QCgqqpKpna7mUwmUlNTAZg2bRrFxcUKR6SMo0ePsnPnTnJycggNDZXx0q13XmS8gNlsxmKxAJCa\nmorT6SQ4OLjf48XnivPDDz/MgQMHACgqKsJoNBISEqJwVMrbv38/H3/8MQA1NTVcuXKFmJgYhaNS\nj8mTJ7vHTX5+Po8++qjCEanDihUrKCsrA7ruy1/t9h9MGhsb2bJlCx9++KG7C1nGi+e8yHiBwsJC\ndu/eDXTdZrXb7bc0XnzyqVRbt26lsLAQjUbDG2+8QUpKitIhKa6pqYnVq1djs9lob28nOzubKVOm\nKB2WIsxmM5s3b6aiogKdTkdMTAxbt25l7dq1tLa2Mnz4cDZt2oS/v7/Sod5VnvKyePFidu3aRWBg\nIEFBQWzatImoqCilQ72rcnNz2b59O/fee6/72Ntvv8369esH9XjxlJd58+axd+/eQT1eHA4Hr732\nGhaLBYfDQXZ2Nmlpabz66qv9Gi8+WZyFEEIIb+Zz09pCCCGEt5PiLIQQQqiMFGchhBBCZaQ4CyGE\nECojxVkIIYRQGSnOQgghhMpIcRZCCCFURoqzEEIIoTL/Dz+K4+J+R8eIAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "HzDUdTT8Agxo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Task C"
      ]
    },
    {
      "metadata": {
        "id": "Wf47-PIDAlIw",
        "colab_type": "code",
        "outputId": "781397f0-02a2-402c-8a9c-daea8a0e5fef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train = data.TabularDataset(train_fp, format='TSV', fields = \n",
        "                            data_fields, skip_header=True, filter_pred=lambda d: d.subtask_a == 'OFF' and d.subtask_b == 'TIN')\n",
        "\n",
        "train, valid = train.split(split_ratio=0.8, random_state=random.seed(SEED))\n",
        "\n",
        "print(f'Train size: {len(train)}')\n",
        "print(f'Validation size: {len(valid)}')\n",
        "\n",
        "#Now build vocab (using only the training set)\n",
        "TEXT.build_vocab(train, vectors='glove.twitter.27B.200d') #USE \"glove.840B.300d\" or glove.twitter.27B.200d\n",
        "\n",
        "LABEL.build_vocab(train.subtask_c)\n",
        "\n",
        "output_dim = len(LABEL.vocab)\n",
        "\n",
        "print(LABEL.vocab.stoi)\n",
        "\n",
        "#Create iterators\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid),\n",
        "                        batch_sizes=(BATCH_SIZE, len(valid)),  \n",
        "                        sort_key=lambda x: len(x.tweet), device=device)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 3101\n",
            "Validation size: 775\n",
            "defaultdict(<function _default_unk_index at 0x7fa37d683840>, {'IND': 0, 'GRP': 1, 'OTH': 2})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UTUbzJZBBBMr",
        "colab_type": "code",
        "outputId": "90ddda19-b833-484e-ddad-78ee5fb9b7ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6708
        }
      },
      "cell_type": "code",
      "source": [
        "#CONV with Glove\n",
        "embedding_dim = 200\n",
        "window_size = 3\n",
        "lr = 0.00025\n",
        "out_channels = 512\n",
        "dropout = 0.5\n",
        "weight = torch.tensor([1.6, 3.7 ,8.4], device = device) #deals with unbalanced classes\n",
        "\n",
        "model = SimpleClassifierGloVe(TEXT.vocab, embedding_dim, window_size, out_channels, dropout, num_classes=3)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "loss_fn = nn.CrossEntropyLoss(weight = weight)\n",
        "\n",
        "\n",
        "t_losses, v_losses = train_helper('subtask_c', model, optimizer, loss_fn = loss_fn, epochs = 20, train_loader=train_iterator, valid_loader=valid_iterator)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Iteration 0, loss = 2.1998\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 371 / 775 correct (47.87)\n",
            "[[209 234  33]\n",
            " [ 38 149  20]\n",
            " [ 23  56  13]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.44      0.56       476\n",
            "           1       0.34      0.72      0.46       207\n",
            "           2       0.20      0.14      0.16        92\n",
            "\n",
            "   micro avg       0.48      0.48      0.48       775\n",
            "   macro avg       0.44      0.43      0.40       775\n",
            "weighted avg       0.59      0.48      0.49       775\n",
            "\n",
            "Kappa 0.1654\n",
            "\n",
            "Epoch: 1\n",
            "Iteration 0, loss = 1.3425\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 445 / 775 correct (57.42)\n",
            "[[288 155  33]\n",
            " [ 45 141  21]\n",
            " [ 31  45  16]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.61      0.69       476\n",
            "           1       0.41      0.68      0.51       207\n",
            "           2       0.23      0.17      0.20        92\n",
            "\n",
            "   micro avg       0.57      0.57      0.57       775\n",
            "   macro avg       0.48      0.49      0.47       775\n",
            "weighted avg       0.62      0.57      0.58       775\n",
            "\n",
            "Kappa 0.2700\n",
            "\n",
            "Epoch: 2\n",
            "Iteration 0, loss = 1.0534\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 473 / 775 correct (61.03)\n",
            "[[324 128  24]\n",
            " [ 53 140  14]\n",
            " [ 35  48   9]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.68      0.73       476\n",
            "           1       0.44      0.68      0.54       207\n",
            "           2       0.19      0.10      0.13        92\n",
            "\n",
            "   micro avg       0.61      0.61      0.61       775\n",
            "   macro avg       0.47      0.48      0.46       775\n",
            "weighted avg       0.62      0.61      0.61       775\n",
            "\n",
            "Kappa 0.3009\n",
            "\n",
            "Epoch: 3\n",
            "Iteration 0, loss = 1.4057\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 504 / 775 correct (65.03)\n",
            "[[359 110   7]\n",
            " [ 57 140  10]\n",
            " [ 38  49   5]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.75      0.77       476\n",
            "           1       0.47      0.68      0.55       207\n",
            "           2       0.23      0.05      0.09        92\n",
            "\n",
            "   micro avg       0.65      0.65      0.65       775\n",
            "   macro avg       0.50      0.49      0.47       775\n",
            "weighted avg       0.64      0.65      0.63       775\n",
            "\n",
            "Kappa 0.3449\n",
            "\n",
            "Epoch: 4\n",
            "Iteration 0, loss = 1.0962\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 485 / 775 correct (62.58)\n",
            "[[332 101  43]\n",
            " [ 47 134  26]\n",
            " [ 32  41  19]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.70      0.75       476\n",
            "           1       0.49      0.65      0.55       207\n",
            "           2       0.22      0.21      0.21        92\n",
            "\n",
            "   micro avg       0.63      0.63      0.63       775\n",
            "   macro avg       0.50      0.52      0.50       775\n",
            "weighted avg       0.65      0.63      0.63       775\n",
            "\n",
            "Kappa 0.3385\n",
            "\n",
            "Epoch: 5\n",
            "Iteration 0, loss = 0.9045\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 477 / 775 correct (61.55)\n",
            "[[305 154  17]\n",
            " [ 31 164  12]\n",
            " [ 32  52   8]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.64      0.72       476\n",
            "           1       0.44      0.79      0.57       207\n",
            "           2       0.22      0.09      0.12        92\n",
            "\n",
            "   micro avg       0.62      0.62      0.62       775\n",
            "   macro avg       0.50      0.51      0.47       775\n",
            "weighted avg       0.65      0.62      0.61       775\n",
            "\n",
            "Kappa 0.3315\n",
            "\n",
            "Epoch: 6\n",
            "Iteration 0, loss = 1.1067\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 502 / 775 correct (64.77)\n",
            "[[351  91  34]\n",
            " [ 52 130  25]\n",
            " [ 34  37  21]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.74      0.77       476\n",
            "           1       0.50      0.63      0.56       207\n",
            "           2       0.26      0.23      0.24        92\n",
            "\n",
            "   micro avg       0.65      0.65      0.65       775\n",
            "   macro avg       0.52      0.53      0.52       775\n",
            "weighted avg       0.66      0.65      0.65       775\n",
            "\n",
            "Kappa 0.3624\n",
            "\n",
            "Epoch: 7\n",
            "Iteration 0, loss = 0.9240\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 505 / 775 correct (65.16)\n",
            "[[359  79  38]\n",
            " [ 57 123  27]\n",
            " [ 37  32  23]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.75      0.77       476\n",
            "           1       0.53      0.59      0.56       207\n",
            "           2       0.26      0.25      0.26        92\n",
            "\n",
            "   micro avg       0.65      0.65      0.65       775\n",
            "   macro avg       0.53      0.53      0.53       775\n",
            "weighted avg       0.66      0.65      0.65       775\n",
            "\n",
            "Kappa 0.3629\n",
            "\n",
            "Epoch: 8\n",
            "Iteration 0, loss = 0.8219\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 516 / 775 correct (66.58)\n",
            "[[363 101  12]\n",
            " [ 52 142  13]\n",
            " [ 37  44  11]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.76      0.78       476\n",
            "           1       0.49      0.69      0.57       207\n",
            "           2       0.31      0.12      0.17        92\n",
            "\n",
            "   micro avg       0.67      0.67      0.67       775\n",
            "   macro avg       0.53      0.52      0.51       775\n",
            "weighted avg       0.66      0.67      0.65       775\n",
            "\n",
            "Kappa 0.3781\n",
            "\n",
            "Epoch: 9\n",
            "Iteration 0, loss = 0.8320\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 506 / 775 correct (65.29)\n",
            "[[339 118  19]\n",
            " [ 35 155  17]\n",
            " [ 34  46  12]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.71      0.77       476\n",
            "           1       0.49      0.75      0.59       207\n",
            "           2       0.25      0.13      0.17        92\n",
            "\n",
            "   micro avg       0.65      0.65      0.65       775\n",
            "   macro avg       0.52      0.53      0.51       775\n",
            "weighted avg       0.67      0.65      0.65       775\n",
            "\n",
            "Kappa 0.3795\n",
            "\n",
            "Epoch: 10\n",
            "Iteration 0, loss = 0.9228\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 515 / 775 correct (66.45)\n",
            "[[375  77  24]\n",
            " [ 61 124  22]\n",
            " [ 41  35  16]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.79      0.79       476\n",
            "           1       0.53      0.60      0.56       207\n",
            "           2       0.26      0.17      0.21        92\n",
            "\n",
            "   micro avg       0.66      0.66      0.66       775\n",
            "   macro avg       0.52      0.52      0.52       775\n",
            "weighted avg       0.65      0.66      0.66       775\n",
            "\n",
            "Kappa 0.3684\n",
            "\n",
            "Epoch: 11\n",
            "Iteration 0, loss = 0.6742\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 516 / 775 correct (66.58)\n",
            "[[359  97  20]\n",
            " [ 47 144  16]\n",
            " [ 36  43  13]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.75      0.78       476\n",
            "           1       0.51      0.70      0.59       207\n",
            "           2       0.27      0.14      0.18        92\n",
            "\n",
            "   micro avg       0.67      0.67      0.67       775\n",
            "   macro avg       0.53      0.53      0.52       775\n",
            "weighted avg       0.67      0.67      0.66       775\n",
            "\n",
            "Kappa 0.3860\n",
            "\n",
            "Epoch: 12\n",
            "Iteration 0, loss = 0.6467\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 509 / 775 correct (65.68)\n",
            "[[348 100  28]\n",
            " [ 42 141  24]\n",
            " [ 34  38  20]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.73      0.77       476\n",
            "           1       0.51      0.68      0.58       207\n",
            "           2       0.28      0.22      0.24        92\n",
            "\n",
            "   micro avg       0.66      0.66      0.66       775\n",
            "   macro avg       0.53      0.54      0.53       775\n",
            "weighted avg       0.67      0.66      0.66       775\n",
            "\n",
            "Kappa 0.3836\n",
            "\n",
            "Epoch: 13\n",
            "Iteration 0, loss = 0.5908\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 513 / 775 correct (66.19)\n",
            "[[371  88  17]\n",
            " [ 59 131  17]\n",
            " [ 41  40  11]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.78      0.78       476\n",
            "           1       0.51      0.63      0.56       207\n",
            "           2       0.24      0.12      0.16        92\n",
            "\n",
            "   micro avg       0.66      0.66      0.66       775\n",
            "   macro avg       0.51      0.51      0.50       775\n",
            "weighted avg       0.65      0.66      0.65       775\n",
            "\n",
            "Kappa 0.3628\n",
            "\n",
            "Epoch: 14\n",
            "Iteration 0, loss = 0.5326\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 517 / 775 correct (66.71)\n",
            "[[358  99  19]\n",
            " [ 44 146  17]\n",
            " [ 35  44  13]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.75      0.78       476\n",
            "           1       0.51      0.71      0.59       207\n",
            "           2       0.27      0.14      0.18        92\n",
            "\n",
            "   micro avg       0.67      0.67      0.67       775\n",
            "   macro avg       0.53      0.53      0.52       775\n",
            "weighted avg       0.67      0.67      0.66       775\n",
            "\n",
            "Kappa 0.3909\n",
            "\n",
            "Epoch: 15\n",
            "Iteration 0, loss = 0.4735\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 512 / 775 correct (66.06)\n",
            "[[366  89  21]\n",
            " [ 54 131  22]\n",
            " [ 36  41  15]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.77      0.79       476\n",
            "           1       0.50      0.63      0.56       207\n",
            "           2       0.26      0.16      0.20        92\n",
            "\n",
            "   micro avg       0.66      0.66      0.66       775\n",
            "   macro avg       0.52      0.52      0.52       775\n",
            "weighted avg       0.66      0.66      0.66       775\n",
            "\n",
            "Kappa 0.3713\n",
            "\n",
            "Epoch: 16\n",
            "Iteration 0, loss = 0.5040\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 518 / 775 correct (66.84)\n",
            "[[363  93  20]\n",
            " [ 50 141  16]\n",
            " [ 36  42  14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.76      0.78       476\n",
            "           1       0.51      0.68      0.58       207\n",
            "           2       0.28      0.15      0.20        92\n",
            "\n",
            "   micro avg       0.67      0.67      0.67       775\n",
            "   macro avg       0.53      0.53      0.52       775\n",
            "weighted avg       0.67      0.67      0.66       775\n",
            "\n",
            "Kappa 0.3875\n",
            "\n",
            "Epoch: 17\n",
            "Iteration 0, loss = 0.4169\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 517 / 775 correct (66.71)\n",
            "[[362  93  21]\n",
            " [ 48 140  19]\n",
            " [ 36  41  15]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.76      0.79       476\n",
            "           1       0.51      0.68      0.58       207\n",
            "           2       0.27      0.16      0.20        92\n",
            "\n",
            "   micro avg       0.67      0.67      0.67       775\n",
            "   macro avg       0.53      0.53      0.52       775\n",
            "weighted avg       0.67      0.67      0.66       775\n",
            "\n",
            "Kappa 0.3877\n",
            "\n",
            "Epoch: 18\n",
            "Iteration 0, loss = 0.4618\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 515 / 775 correct (66.45)\n",
            "[[366  90  20]\n",
            " [ 53 136  18]\n",
            " [ 37  42  13]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.77      0.79       476\n",
            "           1       0.51      0.66      0.57       207\n",
            "           2       0.25      0.14      0.18        92\n",
            "\n",
            "   micro avg       0.66      0.66      0.66       775\n",
            "   macro avg       0.52      0.52      0.51       775\n",
            "weighted avg       0.66      0.66      0.66       775\n",
            "\n",
            "Kappa 0.3769\n",
            "\n",
            "Epoch: 19\n",
            "Iteration 0, loss = 0.5058\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 524 / 775 correct (67.61)\n",
            "[[373  85  18]\n",
            " [ 53 138  16]\n",
            " [ 38  41  13]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.78      0.79       476\n",
            "           1       0.52      0.67      0.59       207\n",
            "           2       0.28      0.14      0.19        92\n",
            "\n",
            "   micro avg       0.68      0.68      0.68       775\n",
            "   macro avg       0.53      0.53      0.52       775\n",
            "weighted avg       0.67      0.68      0.67       775\n",
            "\n",
            "Kappa 0.3936\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TSbEtokDP1Dy",
        "colab_type": "code",
        "outputId": "10f39def-a9c1-4cd6-8cfa-1b3d5c781414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "fig1, ax1 = plt.subplots()\n",
        "\n",
        "ax1.plot(t_losses, label='Training')\n",
        "ax1.plot(v_losses, label='Validation')\n",
        "\n",
        "ax1.set_title('Losses')\n",
        "ax1.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFZCAYAAACv05cWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "zza6aQ1QnMyy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9v2KyvFUQlXi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}