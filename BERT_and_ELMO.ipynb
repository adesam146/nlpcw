{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_and_ELMO.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "_i_qSkEMxlkg",
        "xGMVF5KTg-He",
        "TSaz7XNLG2f4",
        "SClCUJp08-zn",
        "u7glEGKc-rNE"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adesam146/nlpcw/blob/master/BERT_and_ELMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "dHEjHym795PB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Summary of BERT and ELMO work. We were able to get BERT to load and train but it was slow and performed poorly for a large number of epochs. ELMO just caused Colab to crash (due to GPU memory issues)."
      ]
    },
    {
      "metadata": {
        "id": "_i_qSkEMxlkg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Check GPU memory"
      ]
    },
    {
      "metadata": {
        "id": "5-XwNX-831V6",
        "colab_type": "code",
        "outputId": "4f4ed181-afd5-4767-8011-1670999d41e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "#Check GPU Memory allocation\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NOXcwqriwFsu",
        "colab_type": "code",
        "outputId": "e772c897-c69b-4a36-cf96-a19311361ee9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 12.8 GB  | Proc size: 555.5 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ecWOCoFgxS_j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#run this if GPU utilization is not 0%\n",
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wTfeo8tcxhwC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ]
    },
    {
      "metadata": {
        "id": "ePuqIHSPf554",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install spacy ftfy pytorch-pretrained-bert\n",
        "!python -m spacy download en\n",
        "!pip install torchvision torch allennlp\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Srpq8hYt4whg",
        "colab_type": "code",
        "outputId": "140fc0fb-6c43-4f72-ba68-5707125a95ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data import sampler\n",
        "import spacy\n",
        "import torchvision.datasets as dset\n",
        "from torchtext import data\n",
        "from torchtext import datasets as nlp_dset\n",
        "import random\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "\n",
        "nlp_spaCy = spacy.load('en')\n",
        "\n",
        "GPU = True\n",
        "device_idx = 0\n",
        "if GPU:\n",
        "    device = torch.device(\"cuda:\"+str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
        "else:\n",
        "    \n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)\n",
        "\n",
        "#Fix all seeds\n",
        "SEED = 0\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "\n",
        "\n",
        "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
        "#Use pretrained ELMO weights. \n",
        "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
        "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
        "\n",
        "elmo = Elmo(options_file, weight_file, 2, dropout=0)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qtiwRhtm3s87",
        "colab_type": "code",
        "outputId": "fff9afa0-b634-48d1-b2eb-a19e5b368da7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "# Load datafiles from own google drive\n",
        "\n",
        "# EDIT AS NECESSARY:\n",
        "#################\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "train_fp = \"\"\"/content/drive/My Drive/colab_data/offenseval-training-v1.tsv\"\"\"\n",
        "trial_data_fp = \"\"\"/content/drive/My Drive/colab_data/offenseval-trial.txt\"\"\"\n",
        "\n",
        "train_df = pd.read_csv(train_fp, delimiter=\"\\t\")\n",
        "train_df, valid_df = train_test_split(train_df, train_size = 0.8)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "TSaz7XNLG2f4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bert Pre-processing"
      ]
    },
    {
      "metadata": {
        "id": "KlqIs-m8Eq56",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# BERT summary\n",
        "# The first token of every sequence is the classification embedding \"[CLS]\"\n",
        "# There are two types of sentence in the representation: A & B. These are used for\n",
        "# Question-answering systems. For our purposes, all sentences/tweets will be type A. \n",
        "# A and B are seperated with the special token \"[SEP]\". Again, we don't need this here.\n",
        "\n",
        "# We should use the \"bert-large-uncased\" eventually which has 1024 latent features\n",
        "# but for now use \"bert-base-uncased\" which has 768\n",
        "\n",
        "\n",
        "#The model returns the embedded representations in the form:\n",
        "# encoded_layers, pooled_output\n",
        "# encoded_layers: The activations of each of the 12 layers (or 24 layers in BERT-large)\n",
        "#                 list of length 12/24 where each element is a tensor of dimensions:\n",
        "#                 (B, L, F) for Batch size B, sequence length L and number feautures F\n",
        "\n",
        "#if you want the output embeddings per word, use encoded_layers[-1]\n",
        "#if you want to use the BERT sentence embedding use pooled_output\n",
        "# We will use pooled_output for now\n",
        "\n",
        "#encoded_layers, pooled_output = model(tokens_tensor, segments_tensor)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "def convert_tweets_to_features(tweet_list, seq_length, tokenizer):\n",
        "    \"\"\"returns the BERT features\"\"\"\n",
        "\n",
        "    tokenized_tweets = []\n",
        "    input_ids_list = []\n",
        "    input_masks = []\n",
        "    input_type_ids_list= []\n",
        "                \n",
        "    for (index, tweet) in enumerate(tweet_list):\n",
        "       \n",
        "        \n",
        "        # The convention in BERT is:\n",
        "        # (a) For sequence pairs:\n",
        "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "        #  type_ids:   0   0  0    0    0     0      0   0    1  1  1   1  1   1\n",
        "        # (b) For single sequences:\n",
        "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "        #  type_ids:   0   0   0   0  0     0   0\n",
        "        #\n",
        "        # Where \"type_ids\" are used to indicate whether this is the first\n",
        "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "        # embedding vector (and position vector). This is not *strictly* necessary\n",
        "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "        #\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "        \n",
        "        tweet = tweet_preprocess(tweet)\n",
        "        tokens_a = tokenizer.tokenize(tweet)\n",
        "            \n",
        "        tokens = []\n",
        "        input_type_ids = []\n",
        "        tokens.append(\"[CLS]\")\n",
        "        input_type_ids.append(0)\n",
        "        for token in tokens_a:\n",
        "            tokens.append(token)\n",
        "            input_type_ids.append(0)\n",
        "        tokens.append(\"[SEP]\")\n",
        "        input_type_ids.append(0)\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "        #print(\"current length input ids:\", len(input_ids))\n",
        "        \n",
        "        # Zero-pad up to the sequence length.\n",
        "        while len(input_ids) < seq_length:\n",
        "            input_ids.append(0)\n",
        "            input_mask.append(0)\n",
        "            input_type_ids.append(0)\n",
        "\n",
        "        assert len(input_ids) == seq_length, \"{} should = {}\".format(len(input_ids), seq_length)\n",
        "        assert len(input_mask) == seq_length\n",
        "        assert len(input_type_ids) == seq_length\n",
        "        \n",
        "        input_ids_tensor = torch.tensor(input_ids)\n",
        "        input_mask_tensor = torch.tensor(input_mask)\n",
        "        input_type_ids_tensor = torch.tensor(input_type_ids)\n",
        "    \n",
        "        tokenized_tweets.append(tokens)\n",
        "        input_ids_list.append(input_ids_tensor)\n",
        "        input_masks.append(input_mask_tensor)\n",
        "        input_type_ids_list.append(input_type_ids_tensor)\n",
        "        \n",
        "    results = (tokenized_tweets, input_ids_list, input_masks, input_type_ids_list)\n",
        "    return results\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aMY0mUyknLDu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tweet_preprocess(tweet_text):\n",
        "  \"\"\"Add tweet specific preprocessing steps here\"\"\"\n",
        "  \n",
        "  #Remove 'USER' (but leave '@')\n",
        "  tweet_text = tweet_text.replace(\"@USER\", \"@\") \n",
        "  \n",
        "  return tweet_text\n",
        "\n",
        "def convert_labels_A(labels):\n",
        "    \"\"\"Preproceses and return labels\"\"\"\n",
        "\n",
        "    final_labels = []\n",
        "    for label in labels:\n",
        "        assert label == \"OFF\" or label == \"NOT\", \"Label should not be: {}\".format(label)\n",
        "    \n",
        "        if label == \"OFF\":\n",
        "            res = 1\n",
        "        elif label == \"NOT\":\n",
        "            res = 0        \n",
        "        label = torch.tensor([res])\n",
        "        final_labels.append(label)\n",
        "    return final_labels\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FGXJkLCmL2Ip",
        "colab_type": "code",
        "outputId": "e00a0129-114c-4309-d407-5e2ce04aef34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "#Preprocessing BERT\n",
        "\n",
        "tweet_train_list = train_df[\"tweet\"].tolist()\n",
        "tweet_valid_list = valid_df[\"tweet\"].tolist()\n",
        "#find max length of tokens \n",
        "max_len = 0\n",
        "longest_tokens = None\n",
        "for tweet in (tweet_train_list + tweet_valid_list):\n",
        "    tweet = tweet_preprocess(tweet)\n",
        "    tokens = tokenizer.tokenize(tweet)\n",
        "    tokens_len = len(tokens)\n",
        "    if tokens_len > max_len:\n",
        "        max_len = tokens_len\n",
        "        longest_tokens = tokens \n",
        "        \n",
        "print(\"Max token length is\", max_len)\n",
        "\n",
        "#Add an extra few symbols in case the tweets in the test-set are longer\n",
        "MAX_SEQ = max_len + 4 \n",
        "\n",
        "#preprocess tweets and extract labels\n",
        "LABELS_TRAIN = convert_labels_A(train_df[\"subtask_a\"].tolist())\n",
        "FEATURES_TRAIN = convert_tweets_to_features(tweet_train_list, MAX_SEQ, tokenizer)\n",
        "\n",
        "LABELS_VALID = convert_labels_A(valid_df[\"subtask_a\"].tolist())\n",
        "FEATURES_VALID = convert_tweets_to_features(tweet_valid_list, MAX_SEQ, tokenizer)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max token length is 119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4LYWiFk4IzGx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#BERT Data Loaders \n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def train_loader(batch_size = BATCH_SIZE, labels = LABELS_TRAIN, features = FEATURES_TRAIN):\n",
        "    \"\"\"Training Loader\"\"\"\n",
        "    return loader_gen(batch_size = BATCH_SIZE, labels = LABELS_TRAIN, features = FEATURES_TRAIN)\n",
        "def valid_loader():\n",
        "    \"\"\"Validate set loader\"\"\"\n",
        "    return loader_gen(batch_size = BATCH_SIZE, labels = LABELS_VALID, features = FEATURES_VALID)\n",
        "    \n",
        "def loader_gen(batch_size = BATCH_SIZE, labels = LABELS_TRAIN, features = FEATURES_TRAIN):\n",
        "    \"\"\"Generator - bespoke loader. \n",
        "    yields an output of (data, label).\n",
        "        data is a torch tensor of shape (B, L, 2)\n",
        "            where B is batch size, L is number of tokens per tweet and the final \n",
        "            dimension holds the BERT token indexes and the BERT token masks \n",
        "            in the first and second components respectively\"\"\"\n",
        "    \n",
        "    (tokenized_tweets, tweet_ids_list, input_masks, input_type_ids_list) = features \n",
        "    \n",
        "    batch_id_tensor_list = []\n",
        "    batch_mask_tensor_list = []\n",
        "    batch_labels_tensor_list = []\n",
        "    for (idx, tweet_ids) in enumerate(tweet_ids_list):\n",
        "\n",
        "        batch_id_tensor_list.append(tweet_ids)\n",
        "        batch_mask_tensor_list.append(input_masks[idx])\n",
        "        batch_labels_tensor_list.append(labels[idx])\n",
        "        \n",
        "        if len(batch_id_tensor_list) == BATCH_SIZE:\n",
        "            \n",
        "            #Then produce and yield an output batch tensor and label\n",
        "            batch_id_tensor = torch.stack(batch_id_tensor_list)\n",
        "            batch_mask_tensor = torch.stack(batch_mask_tensor_list)\n",
        "            input_tensor = torch.stack((batch_id_tensor, batch_mask_tensor), dim=2)\n",
        "            batch_labels_tensor = torch.stack(batch_labels_tensor_list)\n",
        "            \n",
        "            assert batch_mask_tensor.shape == (batch_size, MAX_SEQ)\n",
        "            assert batch_id_tensor.shape == (batch_size, MAX_SEQ)\n",
        "            assert input_tensor.shape == (batch_size, MAX_SEQ, 2)\n",
        "            assert batch_labels_tensor.shape == (batch_size, 1)\n",
        "            \n",
        "            \n",
        "            yield (input_tensor, batch_labels_tensor)\n",
        "            batch_id_tensor_list = []\n",
        "            batch_mask_tensor_list = []\n",
        "            batch_labels_tensor_list = []\n",
        "    \n",
        "    #check if there is a small batch left...\n",
        "    if len(batch_id_tensor_list) > 0:\n",
        "        batch_id_tensor = torch.stack(batch_id_tensor_list)\n",
        "        batch_mask_tensor = torch.stack(batch_mask_tensor_list)\n",
        "        input_tensor = torch.stack((batch_id_tensor, batch_mask_tensor), dim=2)\n",
        "        batch_labels_tensor = torch.stack(batch_labels_tensor_list)\n",
        "        \n",
        "        yield (input_tensor, batch_labels_tensor)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_nFNFYJhl4cA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "PRINT_EVERY = 50\n",
        "\n",
        "def check_accuracy(loader, model, conf=False): \n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    TP, TN, FP, FN = 0, 0, 0, 0\n",
        "    \n",
        "    model.eval()  # set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for idx, (x, y) in enumerate(loader()):\n",
        "            x = x.to(device=device, dtype=torch.long)  # move to  GPU\n",
        "            y = y.to(device=device, dtype=torch.long)\n",
        "            pred_prob = model(x)\n",
        "            pred_1 = (pred_prob > 0.5).type(torch.long)\n",
        "            num_correct += (pred_1 == y).sum()\n",
        "            num_samples += pred_prob.size(0)\n",
        "            \n",
        "            if conf:\n",
        "                #find confusion matrix\n",
        "                \n",
        "                #find number correct class 1\n",
        "                TP += ((pred_1 == 1) & (y == 1)).sum()\n",
        "                FP += ((pred_1 == 1) & (y == 0)).sum()\n",
        "                TN += ((pred_1 == 0) & (y == 0)).sum()\n",
        "                FN += ((pred_1 == 0) & (y == 1)).sum()\n",
        "            \n",
        "            x = x.to(device=\"cpu\", dtype=torch.long)  # move to CPU to prevent memory overflow\n",
        "            y = y.to(device=\"cpu\", dtype=torch.long)\n",
        "            \n",
        "        acc = float(num_correct) / num_samples\n",
        "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
        "        if conf:\n",
        "            F1_0 = 2 * TP / (2.0 * TP + FN + FP)\n",
        "            F1_1 = 2 * TN / (2 * TN + FN + FP)\n",
        "            print(\"TP = {}, TN = {}, FP = {}, FN = {}\".format(TP, TN, FP, FN))\n",
        "            print(\"F1_0 = {:.4f}, F1_1 = {:.4f}, F1_macro = {:.4f}\".format(F1_0, F1_1, 0.5 * (F1_0 + F1_0)))\n",
        "\n",
        "def train_part(model, optimizer, epochs=1, loss_fn = F.binary_cross_entropy, print_every=PRINT_EVERY):\n",
        "    \"\"\"\n",
        "    Train a model\n",
        "    \n",
        "    Inputs:\n",
        "    - model: A PyTorch Module giving the model to train.\n",
        "    - optimizer: An Optimizer object we will use to train the model\n",
        "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
        "    \n",
        "    Returns: Nothing, but prints model accuracies during training.\n",
        "    \"\"\"\n",
        "    model = model.to(device=device)  # move the model parameters to GPU\n",
        "    try:\n",
        "        for e in range(epochs):\n",
        "            for batch_idx, (inputs, targets) in enumerate(train_loader()):\n",
        "\n",
        "                model.train()  # put model to training mode\n",
        "\n",
        "                x = inputs.to(device=device, dtype=torch.long)  # move to device, e.g. GPU\n",
        "                y = targets.to(device=device, dtype=torch.float) #this should be a float cross entropy\n",
        "                #x = inputs\n",
        "                #y = targets\n",
        "                prob = model(x)\n",
        "                y = y.type(torch.float)\n",
        "                loss = loss_fn(prob, y)\n",
        "                # Zero out all of the gradients for the variables which the optimizer\n",
        "                # will update.\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # This is the backwards pass: compute the gradient of the loss with\n",
        "                # respect to each  parameter of the model.\n",
        "                loss.backward()\n",
        "\n",
        "                # Actually update the parameters of the model using the gradients\n",
        "                # computed by the backwards pass.\n",
        "                optimizer.step()\n",
        "\n",
        "                x = x.to(device=\"cpu\", dtype=torch.long)  # move to CPU to prevent memory overflow\n",
        "                y = y.to(device=\"cpu\", dtype=torch.long)\n",
        "\n",
        "                if batch_idx % print_every == 0:\n",
        "                    print('Iteration %d, loss = %.4f' % (batch_idx, loss.item()))\n",
        "                    check_accuracy(valid_loader, model, conf=True)\n",
        "            print()\n",
        "            print(\"Validation Accuracy:\")\n",
        "            check_accuracy(valid_loader, model, conf=True)\n",
        "            print()\n",
        "\n",
        "    except Exception as e:\n",
        "        #Attempt to prevent GPU memory overflow by transferring model back to cpu\n",
        "        #model = model.to(device=\"cpu\")\n",
        "        raise e"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ZiirMXp9Dq9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test BERT Loader"
      ]
    },
    {
      "metadata": {
        "id": "RfabnjsCcUse",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Test loader\n",
        "#get first batch from train loader\n",
        "#Now compare with values obtained w/o loader\n",
        "(_, tweet_ids_list, input_masks, _) = FEATURES_TRAIN\n",
        "\n",
        "tweet_ids_tensor = tweet_ids_list[0].view((1, -1)) #use just first value\n",
        "input_masks = input_masks[0].view((1, -1))\n",
        "with torch.no_grad():\n",
        "    bert.eval()\n",
        "    encoded_2, pooled_output_2 = bert(tweet_ids_tensor, output_all_encoded_layers=False,\n",
        "                                               attention_mask=input_masks, )\n",
        "    \n",
        "for idx, (inputs, targets) in enumerate(train_loader()):\n",
        "\n",
        "    input_ids = inputs[:, :, 0] #token IDs\n",
        "    attention_mask = inputs[:, :, 1]  #attention mask (to ignore padding)\n",
        "    with torch.no_grad():\n",
        "        bert.eval()\n",
        "        encoded_layers, pooled_output = bert(input_ids, output_all_encoded_layers=False,\n",
        "                                                       attention_mask=attention_mask, )\n",
        "    break #to stop full loop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SClCUJp08-zn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BERT Models"
      ]
    },
    {
      "metadata": {
        "id": "hbv8FFf8Ixah",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FcnnBertEmbeddingBinary(nn.Module):\n",
        "    \"Bert with fully connected NN\"\n",
        "    def __init__(self, embedding_dim, hidden_dim_1, hidden_dim_2, hidden_dim_3, max_len):\n",
        "        \n",
        "        super(FcnnBertEmbeddingBinary, self).__init__()\n",
        "        \n",
        "        #embedding (lookup layer) layer\n",
        "        self.embedding = BertModel.from_pretrained('bert-base-uncased')\n",
        "        \n",
        "        #hidden layers\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim_1, bias=True)\n",
        "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2, bias = True) \n",
        "        self.fc3 = nn.Linear(hidden_dim_2, hidden_dim_3, bias = True)\n",
        "        \n",
        "        #output layer\n",
        "        self.fc4 = nn.Linear(hidden_dim_3, 1, bias = True)\n",
        "        \n",
        "        #Kaming normalization\n",
        "        nn.init.kaiming_normal_(self.fc1.weight)\n",
        "        nn.init.kaiming_normal_(self.fc2.weight)\n",
        "        nn.init.kaiming_normal_(self.fc3.weight)\n",
        "        nn.init.kaiming_normal_(self.fc4.weight)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #Put into .eval mode to export exact weights\n",
        "        self.eval()\n",
        "        input_ids = x[:, :, 0] #token IDs\n",
        "        attention_mask = x[:, :, 1]  #attention mask (to ignore padding)\n",
        "        \n",
        "        \n",
        "        encoded_layers, pooled_output = self.embedding(input_ids, \n",
        "                                                     attention_mask=attention_mask)\n",
        "        self.train()\n",
        "        \n",
        "        #Use 'pooled output' as the overall embedding of the sentence.\n",
        "        #This is recommended in the BERT paper for classification tasks\n",
        "        \n",
        "        #A bit of background on what we are doing here:\n",
        "        #BERT creates its vectors by taking context before and context \n",
        "        #after every token in the sequence. The pooled_output is the \n",
        "        #resultant vector for the first token and is (according to the paper)\n",
        "        #the best representation of the sentence as a whole\n",
        "        h = F.relu(pooled_output)\n",
        "        h = F.relu(self.fc1(h))\n",
        "        h = F.relu(self.fc2(h))\n",
        "        h = F.relu(self.fc3(h))\n",
        "        h = torch.sigmoid(self.fc4(h))\n",
        "        \n",
        "        return h\n",
        "\n",
        "class SimpleClassifierWBert(nn.Module):\n",
        "    \"\"\"Bert w. 2d conv\"\"\"\n",
        "    def __init__(self, out_channels, window_size, dropout):\n",
        "        super(SimpleClassifierWBert, self).__init__()\n",
        "        \n",
        "        self.embedding = BertModel.from_pretrained('bert-base-uncased')\n",
        "        embedding_dim = 768\n",
        "        \n",
        "        self.conv = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(window_size, embedding_dim))\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(out_channels, 1)\n",
        "        \n",
        "        #Kaming normalization\n",
        "        nn.init.kaiming_normal_(self.conv.weight)\n",
        "        nn.init.kaiming_normal_(self.fc.weight)\n",
        "\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        self.eval()\n",
        "        input_ids = x[:, :, 0] #token IDs\n",
        "        attention_mask = x[:, :, 1]  #attention mask (to ignore padding)\n",
        "        \n",
        "        encoded_layers, pooled_output = self.embedding(input_ids, output_all_encoded_layers=False,\n",
        "                                                       attention_mask=attention_mask )\n",
        "        self.train()\n",
        "        \n",
        "        #Use 'final encoded layer' which is of size:\n",
        "            #[batch_size, sequence_length, embedding_dim]\n",
        "        \n",
        "        embedded = encoded_layers.unsqueeze(1)\n",
        "                \n",
        "        #(batch size, 1, max sent length, embedding dim)\n",
        "        \n",
        "        feature_maps =  F.relu(self.conv(embedded).squeeze(3))\n",
        "        # (batch size, out_channels, max sent length - window size +1, 1)\n",
        "        # -> (batch size, out_channels, max sent length - window size +1)\n",
        "           \n",
        "        #the max pooling layer\n",
        "        pooled = F.max_pool1d(feature_maps, feature_maps.shape[2]).squeeze(2)\n",
        "        # (batch size, out_channels)      \n",
        " \n",
        "        return self.fc( self.dropout(pooled))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sTt9LkEcNIq9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "outputId": "95c8c612-668f-48ac-e5cf-1f99426aaf56"
      },
      "cell_type": "code",
      "source": [
        "#CONV with bert\n",
        "embedding_dim = 768\n",
        "max_len = MAX_SEQ\n",
        "hidden_dim_1 = 128\n",
        "hidden_dim_2 = 16\n",
        "hidden_dim_3 = 4\n",
        "lr = 0.00025\n",
        "\n",
        "model = SimpleClassifierWBert(out_channels=100, window_size=3, dropout=0)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "\n",
        "pos_weight = torch.tensor([2.], device = device) #deals with unbalanced classes\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight = pos_weight)\n",
        "\n",
        "\n",
        "train_part(model, optimizer, loss_fn = loss_fn, epochs = 5)\n",
        "\n",
        "#Note: the calculations of F1 are incorrect here. \n",
        "\n",
        "#It takes half an hour to run a single epoch and in that time, the system is \n",
        "#only slightly better than the baseline. If we had more computing resource, \n",
        "#we would love to explore this more but it is not practical to wait this long\n",
        "#everytime we want to run a model.\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0, loss = 3.5033\n",
            "Got 842 / 2648 correct (31.80)\n",
            "TP = 842, TN = 0, FP = 1806, FN = 0\n",
            "F1_0 = 0.0000, F1_1 = 0.0000, F1_macro = 0.0000\n",
            "Iteration 50, loss = 0.9679\n",
            "Got 1806 / 2648 correct (68.20)\n",
            "TP = 0, TN = 1806, FP = 0, FN = 842\n",
            "F1_0 = 0.0000, F1_1 = 0.0000, F1_macro = 0.0000\n",
            "Iteration 100, loss = 0.9420\n",
            "Got 1806 / 2648 correct (68.20)\n",
            "TP = 0, TN = 1806, FP = 0, FN = 842\n",
            "F1_0 = 0.0000, F1_1 = 0.0000, F1_macro = 0.0000\n",
            "Iteration 150, loss = 0.9111\n",
            "Got 1806 / 2648 correct (68.20)\n",
            "TP = 0, TN = 1806, FP = 0, FN = 842\n",
            "F1_0 = 0.0000, F1_1 = 0.0000, F1_macro = 0.0000\n",
            "Iteration 200, loss = 0.8768\n",
            "Got 1806 / 2648 correct (68.20)\n",
            "TP = 0, TN = 1806, FP = 0, FN = 842\n",
            "F1_0 = 0.0000, F1_1 = 0.0000, F1_macro = 0.0000\n",
            "Iteration 250, loss = 0.8443\n",
            "Got 1806 / 2648 correct (68.20)\n",
            "TP = 0, TN = 1806, FP = 0, FN = 842\n",
            "F1_0 = 0.0000, F1_1 = 0.0000, F1_macro = 0.0000\n",
            "Iteration 300, loss = 0.8631\n",
            "Got 1856 / 2648 correct (70.09)\n",
            "TP = 52, TN = 1804, FP = 2, FN = 790\n",
            "F1_0 = 0.0000, F1_1 = 0.0000, F1_macro = 0.0000\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1847 / 2648 correct (69.75)\n",
            "TP = 46, TN = 1801, FP = 5, FN = 796\n",
            "F1_0 = 0.0000, F1_1 = 0.0000, F1_macro = 0.0000\n",
            "\n",
            "Iteration 0, loss = 0.9436\n",
            "Got 1849 / 2648 correct (69.83)\n",
            "TP = 48, TN = 1801, FP = 5, FN = 794\n",
            "F1_0 = 0.0000, F1_1 = 0.0000, F1_macro = 0.0000\n",
            "Iteration 50, loss = 0.7763\n",
            "Got 1807 / 2648 correct (68.24)\n",
            "TP = 583, TN = 1224, FP = 582, FN = 259\n",
            "F1_0 = 0.0000, F1_1 = 0.0000, F1_macro = 0.0000\n",
            "Iteration 100, loss = 1.0128\n",
            "Got 1806 / 2648 correct (68.20)\n",
            "TP = 0, TN = 1806, FP = 0, FN = 842\n",
            "F1_0 = 0.0000, F1_1 = 0.0000, F1_macro = 0.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-MhweQ2jVFT4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Fully connected w. Bert sentence embeddings\n",
        "embedding_dim = 768\n",
        "max_len = MAX_SEQ\n",
        "hidden_dim_1 = 128\n",
        "hidden_dim_2 = 16\n",
        "hidden_dim_3 = 4\n",
        "lr = 0.00025\n",
        "\n",
        "model = FcnnBertEmbeddingBinary(embedding_dim, hidden_dim_1, hidden_dim_2, hidden_dim_3, max_len)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "\n",
        "train_part(model, optimizer, epochs = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xGMVF5KTg-He",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test ELMO"
      ]
    },
    {
      "metadata": {
        "id": "3LeaTI5U7x5N",
        "colab_type": "code",
        "outputId": "01f205c3-7403-4c89-bcd1-6c1ac48a257a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "# ELMO takes a list of parsed sentences as an input\n",
        "# It generates an embedding of length 1024 per word\n",
        "# We then need to find a good method of combining the word vecs to create \n",
        "# a sentence embedding (this article is good: https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a). \n",
        "\n",
        "\n",
        "#Elmo test\n",
        "sentences = [['First', 'sentence', '.'], ['Another', '.'], \n",
        "             [\"Oh\", \"here\", \"we\", \"Go\", \"now\", \"you\", \"fool\", \".\"], \n",
        "             [\"meaninglesswordnotinvocab\"]]\n",
        "             \n",
        "character_ids = batch_to_ids(sentences)\n",
        "\n",
        "embeddings = elmo(character_ids)\n",
        "\n",
        "print(character_ids.shape)\n",
        "embed = embeddings[\"elmo_representations\"]\n",
        "print(len(embed))\n",
        "print(embed[0].shape)\n",
        "print(embed[1].shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 8, 50])\n",
            "2\n",
            "torch.Size([4, 8, 1024])\n",
            "torch.Size([4, 8, 1024])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NQ4enEK_-GsZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Misc"
      ]
    },
    {
      "metadata": {
        "id": "EquNq-KbeEvT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Save intermediate results to CSV so we can use the nice torchtext .TabularDataset loader\n",
        "train_df.to_csv(path_or_buf=PREPROCESSED_FP, sep=',', na_rep='', float_format=None, \n",
        "                header=True, index=True, )\n",
        "\n",
        "#train = pd.read_csv(\"offenseval-training-v1.tsv\", delimiter=\"\\t\")\n",
        "#test = pd.read_csv(\"offenseval-trial (2).txt\", delimiter=\"\\t\")\n",
        "train = None\n",
        "\n",
        "#define our batch size\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "text_field = data.Field(sequential = False, use_vocab = False, dtype = torch.long)\n",
        "label_field = data.LabelField(sequential= False, dtype=torch.float, use_vocab = False)\n",
        "\n",
        "\n",
        "#text_field.build_vocab(train, max_size=25000, vectors=\"glove.6B.100d\")\n",
        "#label_field.build_vocab(train)\n",
        "\n",
        "train = data.TabularDataset(PREPROCESSED_FP, 'CSV', fields = \n",
        "                            [('TWEET_IDS', text_field), ('LABEL_A', label_field)], \n",
        "                            skip_header=False)\n",
        "#train = data.Dataset(examples=tweets_IDs, fields= [(\"tweet\", text_field)])\n",
        "\n",
        "\n",
        "train_iterator = data.Iterator(train, batch_size = BATCH_SIZE, )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "3f22e5dc-13a8-4c8c-8c1d-589c73efea4b",
        "id": "BRvWwHbUgkWf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "cell_type": "code",
      "source": [
        "#OLD BERT METHOD\n",
        "#train = pd.read_csv(\"offenseval-training-v1.tsv\", delimiter=\"\\t\")\n",
        "#test = pd.read_csv(\"offenseval-trial (2).txt\", delimiter=\"\\t\")\n",
        "train = None\n",
        "test = None\n",
        "\n",
        "#define our batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "text_field = data.Field(tokenize=BERT_tokenize, preprocessing = BERT_retrieve_ID, use_vocab = True, dtype = torch.long)\n",
        "label_field = data.LabelField(sequential= False, preprocessing = section_a_labels, dtype=torch.float, use_vocab = False)\n",
        "\n",
        "\n",
        "#text_field.build_vocab(train, max_size=25000, vectors=\"glove.6B.100d\")\n",
        "#label_field.build_vocab(train)\n",
        "\n",
        "train = data.TabularDataset(\"offenseval-training-v1.tsv\", 'TSV', fields = \n",
        "                            { \"tweet\": (\"tweet\", text_field), \"subtask_a\": (\"LabelA\", label_field),}, skip_header=False)\n",
        "\n",
        "text_field.build_vocab(train, vectors=\"glove.6B.50d\") #USE \"glove.840B.300d\" or glove.twitter.27B.200d\n",
        "label_field.build_vocab(train)\n",
        "\n",
        "glove_dim = 50\n",
        "\n",
        "#define our batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "#define types of data and their preprocessing\n",
        "\n",
        "#get pre-defined split\n",
        "#train = text_field.preprocess(train.iloc[0][\"tweet\"])\n",
        "#print(train)\n",
        "\n",
        "train_iterator = data.Iterator(train, batch_size = BATCH_SIZE, device=\"cuda\")\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-fa18f344874d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtext_field\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBERT_tokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERT_retrieve_ID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlabel_field\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLabelField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msection_a_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BERT_tokenize' is not defined"
          ]
        }
      ]
    }
  ]
}