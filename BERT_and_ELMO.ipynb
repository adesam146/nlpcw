{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_and_ELMO.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "_i_qSkEMxlkg",
        "xGMVF5KTg-He",
        "TSaz7XNLG2f4",
        "SClCUJp08-zn",
        "u7glEGKc-rNE"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adesam146/nlpcw/blob/master/BERT_and_ELMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "_i_qSkEMxlkg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Check GPU memory"
      ]
    },
    {
      "metadata": {
        "id": "5-XwNX-831V6",
        "colab_type": "code",
        "outputId": "4f4ed181-afd5-4767-8011-1670999d41e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "#Check GPU Memory allocation\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NOXcwqriwFsu",
        "colab_type": "code",
        "outputId": "e772c897-c69b-4a36-cf96-a19311361ee9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 12.8 GB  | Proc size: 555.5 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ecWOCoFgxS_j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#run this if GPU utilization is not 0%\n",
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wTfeo8tcxhwC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ]
    },
    {
      "metadata": {
        "id": "ePuqIHSPf554",
        "colab_type": "code",
        "outputId": "c94a6205-5093-4423-e96f-2087d27779c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3936
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install spacy ftfy pytorch-pretrained-bert\n",
        "!python -m spacy download en\n",
        "!pip install torchvision torch allennlp\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.0.18)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (5.5.1)\n",
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.6/dist-packages (0.6.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.16.2)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.18.4)\n",
            "Requirement already satisfied: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy) (2018.1.10)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (6.12.1)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.35)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.9)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy) (0.1.7)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.0.1.post2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.103)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.11.29)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.5.6)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.9.0.1)\n",
            "Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.4.3.2)\n",
            "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.10.11)\n",
            "Requirement already satisfied: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.11.0)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.103 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.103)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.0)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy) (0.9.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.103->boto3->pytorch-pretrained-bert) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.103->boto3->pytorch-pretrained-bert) (0.14)\n",
            "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1.post2)\n",
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/32/d6d0a93a23763f366df2dbd4e007e45ce4d2ad97e6315506db9da8af7731/allennlp-0.8.2-py3-none-any.whl (5.6MB)\n",
            "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.6MB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.16.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/5e/e91792f198bbc5a0d7d3055ad552bc4062942d27eaf75c3e2783cf64eae5/Pillow-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.0MB 14.1MB/s \n",
            "\u001b[?25hCollecting moto==1.3.4 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/8f/7b36e81ff067d0e7bf90f7210b351c0cfe6657f79fa4dcb0cb4787462e05/moto-1.3.4-py2.py3-none-any.whl (548kB)\n",
            "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 552kB 25.7MB/s \n",
            "\u001b[?25hCollecting overrides (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/de/55/3100c6d14c1ed177492fcf8f07c4a7d2d6c996c0a7fc6a9a0a41308e7eec/overrides-1.9.tar.gz\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.18.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.20.2)\n",
            "Collecting conllu==0.11 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/d4/2c/856344d9b69baf5b374c395b4286626181a80f0c2b2f704914d18a1cea47/conllu-0.11-py2.py3-none-any.whl\n",
            "Collecting matplotlib==2.2.3 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/59/f235ab21bbe7b7c6570c4abf17ffb893071f4fa3b9cf557b09b60359ad9a/matplotlib-2.2.3-cp36-cp36m-manylinux1_x86_64.whl (12.6MB)\n",
            "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12.6MB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Collecting parsimonious==0.8.0 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/89/32c55944cd30dff856f16859ee325b13c83c260d0c56c0eed511e8063c87/parsimonious-0.8.0.tar.gz\n",
            "Requirement already satisfied: pytorch-pretrained-bert>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.6.1)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (from allennlp) (5.5.1)\n",
            "Collecting pytz==2017.3 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/7f/e7d1acbd433b929168a4fb4182a2ff3c33653717195a26c1de099ad1ef29/pytz-2017.3-py2.py3-none-any.whl (511kB)\n",
            "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 512kB 10.5MB/s \n",
            "\u001b[?25hCollecting tensorboardX==1.2 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/22/43f4f0318f7c68a1000dbb700a353b745584bc2397437832d15ba69ea5f1/tensorboardX-1.2-py2.py3-none-any.whl (44kB)\n",
            "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 20.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlparse==0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.2.4)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.0.23)\n",
            "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.6)\n",
            "Collecting flask-cors==3.0.7 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/65/cb/683f71ff8daa3aea0a5cbb276074de39f9ab66d3fbb8ad5efb5bb83e90d2/Flask_Cors-3.0.7-py2.py3-none-any.whl\n",
            "Collecting numpydoc==0.8.0 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/95/a8/b4706a6270f0475541c5c1ee3373c7a3b793936ec1f517f1a1dab4f896c0/numpydoc-0.8.0.tar.gz\n",
            "Collecting flaky (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/02/42/cca66659a786567c8af98587d66d75e7d2b6e65662f8daab75db708ac35b/flaky-3.5.3-py2.py3-none-any.whl\n",
            "Collecting awscli>=1.11.91 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/5b/ca70b0804813dda500736b0854ba15145442fa0a3ce3382d7688359fdd27/awscli-1.16.116-py2.py3-none-any.whl (1.5MB)\n",
            "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.5MB 17.0MB/s \n",
            "\u001b[?25hCollecting jsonnet==0.10.0; sys_platform != \"win32\" (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/83/d49904ee98dd4fbba6a003938e30e76251951c4bdb49628b4f92e5009a42/jsonnet-0.10.0.tar.gz (124kB)\n",
            "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133kB 32.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n",
            "Requirement already satisfied: spacy<2.1,>=2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.0.18)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.10.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.9.103)\n",
            "Requirement already satisfied: flask==1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.0.2)\n",
            "Collecting gevent==1.3.6 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/3d/a19fece28ba1b5133cf74bd22a229d77b4d9cc4b24aa8f263cca2845c555/gevent-1.3.6-cp36-cp36m-manylinux1_x86_64.whl (4.5MB)\n",
            "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.5MB 7.7MB/s \n",
            "\u001b[?25hCollecting responses>=0.7 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/d8/a77cbd4cb8366ad0e275f2c642b50b401da22a2f5714e003e499fddca106/responses-0.10.5-py2.py3-none-any.whl\n",
            "Collecting docker>=2.5.1 (from moto==1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/3c/b610f22b170b0f8fe4d8f78974878e116562389f666f99e6549567eb9d87/docker-3.7.0-py2.py3-none-any.whl (133kB)\n",
            "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143kB 34.1MB/s \n",
            "\u001b[?25hCollecting python-jose<3.0.0 (from moto==1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/5c/5fa238c0c5b0656994b52721dd8b1d7bf52ebd8786518dde794f44de86b6/python_jose-2.0.2-py2.py3-none-any.whl\n",
            "Collecting cryptography>=2.0.0 (from moto==1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/12/b0409a94dad366d98a8eee2a77678c7a73aafd8c0e4b835abea634ea3896/cryptography-2.6.1-cp34-abi3-manylinux1_x86_64.whl (2.3MB)\n",
            "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.3MB 12.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from moto==1.3.4->allennlp) (2.10)\n",
            "Collecting jsondiff==1.1.1 (from moto==1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/bd/5f/13e28a2f9abeda2ffb3f44f2f809b01b52bc02cdb63816e05b8c9cbbdfc5/jsondiff-1.1.1.tar.gz\n",
            "Collecting pyaml (from moto==1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/c5/e1/1523fb1dab744e2c6b1f02446f2139a78726c18c062a8ddd53875abb20f8/pyaml-18.11.0-py2.py3-none-any.whl\n",
            "Collecting xmltodict (from moto==1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/28/fd/30d5c1d3ac29ce229f6bdc40bbc20b28f716e8b363140c26eff19122d8a5/xmltodict-0.12.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: boto>=2.36.0 in /usr/local/lib/python3.6/dist-packages (from moto==1.3.4->allennlp) (2.49.0)\n",
            "Requirement already satisfied: werkzeug in /usr/local/lib/python3.6/dist-packages (from moto==1.3.4->allennlp) (0.14.1)\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.6/dist-packages (from moto==1.3.4->allennlp) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from moto==1.3.4->allennlp) (2.5.3)\n",
            "Collecting aws-xray-sdk<0.96,>=0.93 (from moto==1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/a5/da7887285564f9e0ae5cd25a453cca36e2cd43d8ccc9effde260b4d80904/aws_xray_sdk-0.95-py2.py3-none-any.whl (52kB)\n",
            "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61kB 20.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: botocore>=1.9.16 in /usr/local/lib/python3.6/dist-packages (from moto==1.3.4->allennlp) (1.12.103)\n",
            "Collecting cookies (from moto==1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6a/60/557f84aa2db629e5124aa05408b975b1b5d0e1cec16cde0bfa06aae097d3/cookies-2.2.1-py2.py3-none-any.whl (44kB)\n",
            "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 20.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2018.11.29)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.2.3->allennlp) (2.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.2.3->allennlp) (1.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.0->allennlp) (2018.1.10)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.7)\n",
            "Requirement already satisfied: protobuf>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.2->allennlp) (3.6.1)\n",
            "Requirement already satisfied: sphinx>=1.2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc==0.8.0->allennlp) (1.8.4)\n",
            "Collecting rsa<=3.5.0,>=3.1.2 (from awscli>=1.11.91->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/ae/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e/rsa-3.4.2-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (0.14)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (0.2.0)\n",
            "Requirement already satisfied: PyYAML<=3.13,>=3.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (3.13)\n",
            "Collecting colorama<=0.3.9,>=0.2.5 (from awscli>=1.11.91->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/db/c8/7dcf9dbcb22429512708fe3a547f8b6101c0d02137acbd892505aee57adf/colorama-0.3.9-py2.py3-none-any.whl\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (0.9.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (0.2.9)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (2.0.1)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (1.35)\n",
            "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (6.12.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (2.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (40.8.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (6.0.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (18.2.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n",
            "Requirement already satisfied: pluggy>=0.7 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.9.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.4)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->allennlp) (7.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->allennlp) (1.1.0)\n",
            "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent==1.3.6->allennlp) (0.4.15)\n",
            "Collecting docker-pycreds>=0.4.0 (from docker>=2.5.1->moto==1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting websocket-client>=0.32.0 (from docker>=2.5.1->moto==1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/54/684db2ba1b7a203602808446b8686ee786f93b4a7e080cdc440cc7e06e56/websocket_client-0.55.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204kB 28.8MB/s \n",
            "\u001b[?25hCollecting pycryptodome<4.0.0,>=3.3.1 (from python-jose<3.0.0->moto==1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/cf/4b66bf1ac2484ca39599b4576d681186b61b543c2d2c29f9aa4ba3cc53b5/pycryptodome-3.7.3-cp36-cp36m-manylinux1_x86_64.whl (7.5MB)\n",
            "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.5MB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: future<1.0 in /usr/local/lib/python3.6/dist-packages (from python-jose<3.0.0->moto==1.3.4->allennlp) (0.16.0)\n",
            "Collecting ecdsa<1.0 (from python-jose<3.0.0->moto==1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/f4/73669d51825516ce8c43b816c0a6b64cd6eb71d08b99820c00792cb42222/ecdsa-0.13-py2.py3-none-any.whl (86kB)\n",
            "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92kB 18.6MB/s \n",
            "\u001b[?25hCollecting asn1crypto>=0.21.0 (from cryptography>=2.0.0->moto==1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/cd/35485615f45f30a510576f1a56d1e0a7ad7bd8ab5ed7cdc600ef7cd06222/asn1crypto-0.24.0-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102kB 30.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.0.0->moto==1.3.4->allennlp) (1.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7.3->moto==1.3.4->allennlp) (1.1.1)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock->moto==1.3.4->allennlp) (5.1.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from aws-xray-sdk<0.96,>=0.93->moto==1.3.4->allennlp) (1.10.11)\n",
            "Collecting jsonpickle (from aws-xray-sdk<0.96,>=0.93->moto==1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/dc/12/8c44eabb501e2bc0aec0dd152b328074d98a50968d3a02be28f6037f0c6a/jsonpickle-1.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (1.1.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (19.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (1.2.1)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (2.6.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<=3.5.0,>=3.1.2->awscli>=1.11.91->allennlp) (0.4.5)\n",
            "Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy<2.1,>=2.0->allennlp) (0.4.3.2)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy<2.1,>=2.0->allennlp) (0.9.0.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0.0->moto==1.3.4->allennlp) (2.19)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy<2.1,>=2.0->allennlp) (0.9.0)\n",
            "Building wheels for collected packages: overrides, parsimonious, numpydoc, jsonnet, jsondiff\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/8d/52/86/e5a83b1797e7d263b458d2334edd2704c78508b3eea9323718\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/bb/51/82/ae9b22a790f11e7be918939d01aa397c545ebb3723453c5fb4\n",
            "  Building wheel for numpydoc (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ea/55/7f/3e25d754760ccd62d6796e5b2cfe25629346f52ea00753d549\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a0/aa/f0/b4ab8854cf00f922a87787425cfbb789aac01ab2c2cd1b4ca4\n",
            "  Building wheel for jsondiff (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/68/08/07/69d839606fb7fdc778fa86476abc0a864693d45969a0c1936c\n",
            "Successfully built overrides parsimonious numpydoc jsonnet jsondiff\n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "\u001b[31mawscli 1.16.116 has requirement botocore==1.12.106, but you'll have botocore 1.12.103 which is incompatible.\u001b[0m\n",
            "Installing collected packages: docker-pycreds, websocket-client, docker, pycryptodome, ecdsa, python-jose, asn1crypto, cryptography, jsondiff, responses, pyaml, xmltodict, jsonpickle, aws-xray-sdk, pytz, cookies, moto, overrides, conllu, matplotlib, parsimonious, tensorboardX, flask-cors, numpydoc, flaky, rsa, colorama, awscli, jsonnet, gevent, allennlp, pillow\n",
            "  Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "  Found existing installation: matplotlib 3.0.2\n",
            "    Uninstalling matplotlib-3.0.2:\n",
            "      Successfully uninstalled matplotlib-3.0.2\n",
            "  Found existing installation: rsa 4.0\n",
            "    Uninstalling rsa-4.0:\n",
            "      Successfully uninstalled rsa-4.0\n",
            "  Found existing installation: gevent 1.4.0\n",
            "    Uninstalling gevent-1.4.0:\n",
            "      Successfully uninstalled gevent-1.4.0\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed allennlp-0.8.2 asn1crypto-0.24.0 aws-xray-sdk-0.95 awscli-1.16.116 colorama-0.3.9 conllu-0.11 cookies-2.2.1 cryptography-2.6.1 docker-3.7.0 docker-pycreds-0.4.0 ecdsa-0.13 flaky-3.5.3 flask-cors-3.0.7 gevent-1.3.6 jsondiff-1.1.1 jsonnet-0.10.0 jsonpickle-1.1 matplotlib-2.2.3 moto-1.3.4 numpydoc-0.8.0 overrides-1.9 parsimonious-0.8.0 pillow-5.4.1 pyaml-18.11.0 pycryptodome-3.7.3 python-jose-2.0.2 pytz-2017.3 responses-0.10.5 rsa-3.4.2 tensorboardX-1.2 websocket-client-0.55.0 xmltodict-0.12.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "matplotlib",
                  "mpl_toolkits",
                  "pytz",
                  "rsa"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Srpq8hYt4whg",
        "colab_type": "code",
        "outputId": "140fc0fb-6c43-4f72-ba68-5707125a95ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data import sampler\n",
        "import spacy\n",
        "import torchvision.datasets as dset\n",
        "from torchtext import data\n",
        "from torchtext import datasets as nlp_dset\n",
        "import random\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "\n",
        "nlp_spaCy = spacy.load('en')\n",
        "\n",
        "GPU = True\n",
        "device_idx = 0\n",
        "if GPU:\n",
        "    device = torch.device(\"cuda:\"+str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
        "else:\n",
        "    \n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)\n",
        "\n",
        "#Fix all seeds\n",
        "SEED = 0\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "\n",
        "\n",
        "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
        "#Use pretrained ELMO weights. \n",
        "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
        "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
        "\n",
        "elmo = Elmo(options_file, weight_file, 2, dropout=0)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qtiwRhtm3s87",
        "colab_type": "code",
        "outputId": "fff9afa0-b634-48d1-b2eb-a19e5b368da7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "# Load datafiles from own google drive\n",
        "\n",
        "# EDIT AS NECESSARY:\n",
        "#################\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "train_fp = \"\"\"/content/drive/My Drive/colab_data/offenseval-training-v1.tsv\"\"\"\n",
        "trial_data_fp = \"\"\"/content/drive/My Drive/colab_data/offenseval-trial.txt\"\"\"\n",
        "\n",
        "train_df = pd.read_csv(train_fp, delimiter=\"\\t\")\n",
        "train_df, valid_df = train_test_split(train_df, train_size = 0.8)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "TSaz7XNLG2f4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bert Pre-processing"
      ]
    },
    {
      "metadata": {
        "id": "KlqIs-m8Eq56",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# BERT summary\n",
        "# The first token of every sequence is the classification embedding \"[CLS]\"\n",
        "# There are two types of sentence in the representation: A & B. These are used for\n",
        "# Question-answering systems. For our purposes, all sentences/tweets will be type A. \n",
        "# A and B are seperated with the special token \"[SEP]\". Again, we don't need this here.\n",
        "\n",
        "# We should use the \"bert-large-uncased\" eventually which has 1024 latent features\n",
        "# but for now use \"bert-base-uncased\" which has 768\n",
        "\n",
        "\n",
        "#The model returns the embedded representations in the form:\n",
        "# encoded_layers, pooled_output\n",
        "# encoded_layers: The activations of each of the 12 layers (or 24 layers in BERT-large)\n",
        "#                 list of length 12/24 where each element is a tensor of dimensions:\n",
        "#                 (B, L, F) for Batch size B, sequence length L and number feautures F\n",
        "\n",
        "#if you want the output embeddings per word, use encoded_layers[-1]\n",
        "#if you want to use the BERT sentence embedding use pooled_output\n",
        "# We will use pooled_output for now\n",
        "\n",
        "#encoded_layers, pooled_output = model(tokens_tensor, segments_tensor)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "def convert_tweets_to_features(tweet_list, seq_length, tokenizer):\n",
        "    \"\"\"returns the BERT features\"\"\"\n",
        "\n",
        "    tokenized_tweets = []\n",
        "    input_ids_list = []\n",
        "    input_masks = []\n",
        "    input_type_ids_list= []\n",
        "                \n",
        "    for (index, tweet) in enumerate(tweet_list):\n",
        "       \n",
        "        \n",
        "        # The convention in BERT is:\n",
        "        # (a) For sequence pairs:\n",
        "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "        #  type_ids:   0   0  0    0    0     0      0   0    1  1  1   1  1   1\n",
        "        # (b) For single sequences:\n",
        "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "        #  type_ids:   0   0   0   0  0     0   0\n",
        "        #\n",
        "        # Where \"type_ids\" are used to indicate whether this is the first\n",
        "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "        # embedding vector (and position vector). This is not *strictly* necessary\n",
        "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "        #\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "        \n",
        "        tweet = tweet_preprocess(tweet)\n",
        "        tokens_a = tokenizer.tokenize(tweet)\n",
        "            \n",
        "        tokens = []\n",
        "        input_type_ids = []\n",
        "        tokens.append(\"[CLS]\")\n",
        "        input_type_ids.append(0)\n",
        "        for token in tokens_a:\n",
        "            tokens.append(token)\n",
        "            input_type_ids.append(0)\n",
        "        tokens.append(\"[SEP]\")\n",
        "        input_type_ids.append(0)\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "        #print(\"current length input ids:\", len(input_ids))\n",
        "        \n",
        "        # Zero-pad up to the sequence length.\n",
        "        while len(input_ids) < seq_length:\n",
        "            input_ids.append(0)\n",
        "            input_mask.append(0)\n",
        "            input_type_ids.append(0)\n",
        "\n",
        "        assert len(input_ids) == seq_length, \"{} should = {}\".format(len(input_ids), seq_length)\n",
        "        assert len(input_mask) == seq_length\n",
        "        assert len(input_type_ids) == seq_length\n",
        "        \n",
        "        input_ids_tensor = torch.tensor(input_ids)\n",
        "        input_mask_tensor = torch.tensor(input_mask)\n",
        "        input_type_ids_tensor = torch.tensor(input_type_ids)\n",
        "    \n",
        "        tokenized_tweets.append(tokens)\n",
        "        input_ids_list.append(input_ids_tensor)\n",
        "        input_masks.append(input_mask_tensor)\n",
        "        input_type_ids_list.append(input_type_ids_tensor)\n",
        "        \n",
        "    results = (tokenized_tweets, input_ids_list, input_masks, input_type_ids_list)\n",
        "    return results\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aMY0mUyknLDu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tweet_preprocess(tweet_text):\n",
        "  \"\"\"Add tweet specific preprocessing steps here\"\"\"\n",
        "  \n",
        "  #Remove 'USER' (but leave '@')\n",
        "  tweet_text = tweet_text.replace(\"@USER\", \"@\") \n",
        "  \n",
        "  return tweet_text\n",
        "\n",
        "def convert_labels_A(labels):\n",
        "    \"\"\"Preproceses and return labels\"\"\"\n",
        "\n",
        "    final_labels = []\n",
        "    for label in labels:\n",
        "        assert label == \"OFF\" or label == \"NOT\", \"Label should not be: {}\".format(label)\n",
        "    \n",
        "        if label == \"OFF\":\n",
        "            res = 1\n",
        "        elif label == \"NOT\":\n",
        "            res = 0        \n",
        "        label = torch.tensor([res])\n",
        "        final_labels.append(label)\n",
        "    return final_labels\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FGXJkLCmL2Ip",
        "colab_type": "code",
        "outputId": "e00a0129-114c-4309-d407-5e2ce04aef34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "#Preprocessing BERT\n",
        "\n",
        "tweet_train_list = train_df[\"tweet\"].tolist()\n",
        "tweet_valid_list = valid_df[\"tweet\"].tolist()\n",
        "#find max length of tokens \n",
        "max_len = 0\n",
        "longest_tokens = None\n",
        "for tweet in (tweet_train_list + tweet_valid_list):\n",
        "    tweet = tweet_preprocess(tweet)\n",
        "    tokens = tokenizer.tokenize(tweet)\n",
        "    tokens_len = len(tokens)\n",
        "    if tokens_len > max_len:\n",
        "        max_len = tokens_len\n",
        "        longest_tokens = tokens \n",
        "        \n",
        "print(\"Max token length is\", max_len)\n",
        "\n",
        "#Add an extra few symbols in case the tweets in the test-set are longer\n",
        "MAX_SEQ = max_len + 4 \n",
        "\n",
        "#preprocess tweets and extract labels\n",
        "LABELS_TRAIN = convert_labels_A(train_df[\"subtask_a\"].tolist())\n",
        "FEATURES_TRAIN = convert_tweets_to_features(tweet_train_list, MAX_SEQ, tokenizer)\n",
        "\n",
        "LABELS_VALID = convert_labels_A(valid_df[\"subtask_a\"].tolist())\n",
        "FEATURES_VALID = convert_tweets_to_features(tweet_valid_list, MAX_SEQ, tokenizer)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max token length is 119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4LYWiFk4IzGx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#BERT Data Loaders \n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def train_loader(batch_size = BATCH_SIZE, labels = LABELS_TRAIN, features = FEATURES_TRAIN):\n",
        "    \"\"\"Training Loader\"\"\"\n",
        "    return loader_gen(batch_size = BATCH_SIZE, labels = LABELS_TRAIN, features = FEATURES_TRAIN)\n",
        "def valid_loader():\n",
        "    \"\"\"Validate set loader\"\"\"\n",
        "    return loader_gen(batch_size = BATCH_SIZE, labels = LABELS_VALID, features = FEATURES_VALID)\n",
        "    \n",
        "def loader_gen(batch_size = BATCH_SIZE, labels = LABELS_TRAIN, features = FEATURES_TRAIN):\n",
        "    \"\"\"Generator - bespoke loader. \n",
        "    yields an output of (data, label).\n",
        "        data is a torch tensor of shape (B, L, 2)\n",
        "            where B is batch size, L is number of tokens per tweet and the final \n",
        "            dimension holds the BERT token indexes and the BERT token masks \n",
        "            in the first and second components respectively\"\"\"\n",
        "    \n",
        "    (tokenized_tweets, tweet_ids_list, input_masks, input_type_ids_list) = features \n",
        "    \n",
        "    batch_id_tensor_list = []\n",
        "    batch_mask_tensor_list = []\n",
        "    batch_labels_tensor_list = []\n",
        "    for (idx, tweet_ids) in enumerate(tweet_ids_list):\n",
        "\n",
        "        batch_id_tensor_list.append(tweet_ids)\n",
        "        batch_mask_tensor_list.append(input_masks[idx])\n",
        "        batch_labels_tensor_list.append(labels[idx])\n",
        "        \n",
        "        if len(batch_id_tensor_list) == BATCH_SIZE:\n",
        "            \n",
        "            #Then produce and yield an output batch tensor and label\n",
        "            batch_id_tensor = torch.stack(batch_id_tensor_list)\n",
        "            batch_mask_tensor = torch.stack(batch_mask_tensor_list)\n",
        "            input_tensor = torch.stack((batch_id_tensor, batch_mask_tensor), dim=2)\n",
        "            batch_labels_tensor = torch.stack(batch_labels_tensor_list)\n",
        "            \n",
        "            assert batch_mask_tensor.shape == (batch_size, MAX_SEQ)\n",
        "            assert batch_id_tensor.shape == (batch_size, MAX_SEQ)\n",
        "            assert input_tensor.shape == (batch_size, MAX_SEQ, 2)\n",
        "            assert batch_labels_tensor.shape == (batch_size, 1)\n",
        "            \n",
        "            \n",
        "            yield (input_tensor, batch_labels_tensor)\n",
        "            batch_id_tensor_list = []\n",
        "            batch_mask_tensor_list = []\n",
        "            batch_labels_tensor_list = []\n",
        "    \n",
        "    #check if there is a small batch left...\n",
        "    if len(batch_id_tensor_list) > 0:\n",
        "        batch_id_tensor = torch.stack(batch_id_tensor_list)\n",
        "        batch_mask_tensor = torch.stack(batch_mask_tensor_list)\n",
        "        input_tensor = torch.stack((batch_id_tensor, batch_mask_tensor), dim=2)\n",
        "        batch_labels_tensor = torch.stack(batch_labels_tensor_list)\n",
        "        \n",
        "        yield (input_tensor, batch_labels_tensor)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_nFNFYJhl4cA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "PRINT_EVERY = 50\n",
        "\n",
        "def check_accuracy(loader, model, conf=False): \n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    TP, TN, FP, FN = 0, 0, 0, 0\n",
        "    \n",
        "    model.eval()  # set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for idx, (x, y) in enumerate(loader()):\n",
        "            x = x.to(device=device, dtype=torch.long)  # move to  GPU\n",
        "            y = y.to(device=device, dtype=torch.long)\n",
        "            pred_prob = model(x)\n",
        "            pred_1 = (pred_prob > 0.5).type(torch.long)\n",
        "            num_correct += (pred_1 == y).sum()\n",
        "            num_samples += pred_prob.size(0)\n",
        "            \n",
        "            if conf:\n",
        "                #find confusion matrix\n",
        "                \n",
        "                #find number correct class 1\n",
        "                TP += ((pred_1 == 1) & (y == 1)).sum()\n",
        "                FP += ((pred_1 == 1) & (y == 0)).sum()\n",
        "                TN += ((pred_1 == 0) & (y == 0)).sum()\n",
        "                FN += ((pred_1 == 0) & (y == 1)).sum()\n",
        "            \n",
        "            x = x.to(device=\"cpu\", dtype=torch.long)  # move to CPU to prevent memory overflow\n",
        "            y = y.to(device=\"cpu\", dtype=torch.long)\n",
        "            \n",
        "        acc = float(num_correct) / num_samples\n",
        "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
        "        if conf:\n",
        "            F1_0 = 2 * TP / (2.0 * TP + FN + FP)\n",
        "            F1_1 = 2 * TN / (2 * TN + FN + FP)\n",
        "            print(\"TP = {}, TN = {}, FP = {}, FN = {}\".format(TP, TN, FP, FN))\n",
        "            print(\"F1_0 = {:.4f}, F1_1 = {:.4f}, F1_macro = {:.4f}\".format(F1_0, F1_1, 0.5 * (F1_0 + F1_0)))\n",
        "\n",
        "def train_part(model, optimizer, epochs=1, loss_fn = F.binary_cross_entropy, print_every=PRINT_EVERY):\n",
        "    \"\"\"\n",
        "    Train a model\n",
        "    \n",
        "    Inputs:\n",
        "    - model: A PyTorch Module giving the model to train.\n",
        "    - optimizer: An Optimizer object we will use to train the model\n",
        "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
        "    \n",
        "    Returns: Nothing, but prints model accuracies during training.\n",
        "    \"\"\"\n",
        "    model = model.to(device=device)  # move the model parameters to GPU\n",
        "    try:\n",
        "        for e in range(epochs):\n",
        "            for batch_idx, (inputs, targets) in enumerate(train_loader()):\n",
        "\n",
        "                model.train()  # put model to training mode\n",
        "\n",
        "                x = inputs.to(device=device, dtype=torch.long)  # move to device, e.g. GPU\n",
        "                y = targets.to(device=device, dtype=torch.float) #this should be a float cross entropy\n",
        "                #x = inputs\n",
        "                #y = targets\n",
        "                prob = model(x)\n",
        "                y = y.type(torch.float)\n",
        "                loss = loss_fn(prob, y)\n",
        "                # Zero out all of the gradients for the variables which the optimizer\n",
        "                # will update.\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # This is the backwards pass: compute the gradient of the loss with\n",
        "                # respect to each  parameter of the model.\n",
        "                loss.backward()\n",
        "\n",
        "                # Actually update the parameters of the model using the gradients\n",
        "                # computed by the backwards pass.\n",
        "                optimizer.step()\n",
        "\n",
        "                x = x.to(device=\"cpu\", dtype=torch.long)  # move to CPU to prevent memory overflow\n",
        "                y = y.to(device=\"cpu\", dtype=torch.long)\n",
        "\n",
        "                if batch_idx % print_every == 0:\n",
        "                    print('Iteration %d, loss = %.4f' % (batch_idx, loss.item()))\n",
        "                    check_accuracy(valid_loader, model, conf=True)\n",
        "            print()\n",
        "            print(\"Validation Accuracy:\")\n",
        "            check_accuracy(valid_loader, model, conf=True)\n",
        "            print()\n",
        "\n",
        "    except Exception as e:\n",
        "        #Attempt to prevent GPU memory overflow by transferring model back to cpu\n",
        "        #model = model.to(device=\"cpu\")\n",
        "        raise e"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ZiirMXp9Dq9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test BERT Loader"
      ]
    },
    {
      "metadata": {
        "id": "RfabnjsCcUse",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Test loader\n",
        "#get first batch from train loader\n",
        "#Now compare with values obtained w/o loader\n",
        "(_, tweet_ids_list, input_masks, _) = FEATURES_TRAIN\n",
        "\n",
        "tweet_ids_tensor = tweet_ids_list[0].view((1, -1)) #use just first value\n",
        "input_masks = input_masks[0].view((1, -1))\n",
        "with torch.no_grad():\n",
        "    bert.eval()\n",
        "    encoded_2, pooled_output_2 = bert(tweet_ids_tensor, output_all_encoded_layers=False,\n",
        "                                               attention_mask=input_masks, )\n",
        "    \n",
        "for idx, (inputs, targets) in enumerate(train_loader()):\n",
        "\n",
        "    input_ids = inputs[:, :, 0] #token IDs\n",
        "    attention_mask = inputs[:, :, 1]  #attention mask (to ignore padding)\n",
        "    with torch.no_grad():\n",
        "        bert.eval()\n",
        "        encoded_layers, pooled_output = bert(input_ids, output_all_encoded_layers=False,\n",
        "                                                       attention_mask=attention_mask, )\n",
        "    break #to stop full loop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SClCUJp08-zn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BERT Models"
      ]
    },
    {
      "metadata": {
        "id": "hbv8FFf8Ixah",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FcnnBertEmbeddingBinary(nn.Module):\n",
        "    \"Bert with fully connected NN\"\n",
        "    def __init__(self, embedding_dim, hidden_dim_1, hidden_dim_2, hidden_dim_3, max_len):\n",
        "        \n",
        "        super(FcnnBertEmbeddingBinary, self).__init__()\n",
        "        \n",
        "        #embedding (lookup layer) layer\n",
        "        self.embedding = BertModel.from_pretrained('bert-base-uncased')\n",
        "        \n",
        "        #hidden layers\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim_1, bias=True)\n",
        "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2, bias = True) \n",
        "        self.fc3 = nn.Linear(hidden_dim_2, hidden_dim_3, bias = True)\n",
        "        \n",
        "        #output layer\n",
        "        self.fc4 = nn.Linear(hidden_dim_3, 1, bias = True)\n",
        "        \n",
        "        #Kaming normalization\n",
        "        nn.init.kaiming_normal_(self.fc1.weight)\n",
        "        nn.init.kaiming_normal_(self.fc2.weight)\n",
        "        nn.init.kaiming_normal_(self.fc3.weight)\n",
        "        nn.init.kaiming_normal_(self.fc4.weight)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #Put into .eval mode to export exact weights\n",
        "        self.eval()\n",
        "        input_ids = x[:, :, 0] #token IDs\n",
        "        attention_mask = x[:, :, 1]  #attention mask (to ignore padding)\n",
        "        \n",
        "        \n",
        "        encoded_layers, pooled_output = self.embedding(input_ids, \n",
        "                                                     attention_mask=attention_mask)\n",
        "        self.train()\n",
        "        \n",
        "        #Use 'pooled output' as the overall embedding of the sentence.\n",
        "        #This is recommended in the BERT paper for classification tasks\n",
        "        \n",
        "        #A bit of background on what we are doing here:\n",
        "        #BERT creates its vectors by taking context before and context \n",
        "        #after every token in the sequence. The pooled_output is the \n",
        "        #resultant vector for the first token and is (according to the paper)\n",
        "        #the best representation of the sentence as a whole\n",
        "        h = F.relu(pooled_output)\n",
        "        h = F.relu(self.fc1(h))\n",
        "        h = F.relu(self.fc2(h))\n",
        "        h = F.relu(self.fc3(h))\n",
        "        h = torch.sigmoid(self.fc4(h))\n",
        "        \n",
        "        return h\n",
        "\n",
        "class SimpleClassifierWBert(nn.Module):\n",
        "    \"\"\"Bert w. 2d conv\"\"\"\n",
        "    def __init__(self, out_channels, window_size, dropout):\n",
        "        super(SimpleClassifierWBert, self).__init__()\n",
        "        \n",
        "        self.embedding = BertModel.from_pretrained('bert-base-uncased')\n",
        "        embedding_dim = 768\n",
        "        \n",
        "        self.conv = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(window_size, embedding_dim))\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(out_channels, 1)\n",
        "        \n",
        "        #Kaming normalization\n",
        "        nn.init.kaiming_normal_(self.conv.weight)\n",
        "        nn.init.kaiming_normal_(self.fc.weight)\n",
        "\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        self.eval()\n",
        "        input_ids = x[:, :, 0] #token IDs\n",
        "        attention_mask = x[:, :, 1]  #attention mask (to ignore padding)\n",
        "        \n",
        "        encoded_layers, pooled_output = self.embedding(input_ids, output_all_encoded_layers=False,\n",
        "                                                       attention_mask=attention_mask )\n",
        "        self.train()\n",
        "        \n",
        "        #Use 'final encoded layer' which is of size:\n",
        "            #[batch_size, sequence_length, embedding_dim]\n",
        "        \n",
        "        embedded = encoded_layers.unsqueeze(1)\n",
        "                \n",
        "        #(batch size, 1, max sent length, embedding dim)\n",
        "        \n",
        "        feature_maps =  F.relu(self.conv(embedded).squeeze(3))\n",
        "        # (batch size, out_channels, max sent length - window size +1, 1)\n",
        "        # -> (batch size, out_channels, max sent length - window size +1)\n",
        "           \n",
        "        #the max pooling layer\n",
        "        pooled = F.max_pool1d(feature_maps, feature_maps.shape[2]).squeeze(2)\n",
        "        # (batch size, out_channels)      \n",
        " \n",
        "        return self.fc( self.dropout(pooled))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sTt9LkEcNIq9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "95c8c612-668f-48ac-e5cf-1f99426aaf56"
      },
      "cell_type": "code",
      "source": [
        "#CONV with bert\n",
        "embedding_dim = 768\n",
        "max_len = MAX_SEQ\n",
        "hidden_dim_1 = 128\n",
        "hidden_dim_2 = 16\n",
        "hidden_dim_3 = 4\n",
        "lr = 0.00025\n",
        "\n",
        "model = SimpleClassifierWBert(out_channels=100, window_size=3, dropout=0)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "\n",
        "pos_weight = torch.tensor([2.], device = device) #deals with unbalanced classes\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight = pos_weight)\n",
        "\n",
        "\n",
        "train_part(model, optimizer, loss_fn = loss_fn, epochs = 5)\n",
        "\n",
        "#Note: the calculations of F1 are incorerect here. \n",
        "#All values are being assigned to a single class \n",
        "#(and 5 epochs takes almost an hour)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0, loss = 3.5033\n",
            "Got 842 / 2648 correct (31.80)\n",
            "TP = 842, TN = 0, FP = 1806, FN = 0\n",
            "F1_0 = 0.0000, F1_1 = 0.0000, F1_macro = 0.0000\n",
            "Iteration 50, loss = 0.9679\n",
            "Got 1806 / 2648 correct (68.20)\n",
            "TP = 0, TN = 1806, FP = 0, FN = 842\n",
            "F1_0 = 0.0000, F1_1 = 0.0000, F1_macro = 0.0000\n",
            "Iteration 100, loss = 0.9420\n",
            "Got 1806 / 2648 correct (68.20)\n",
            "TP = 0, TN = 1806, FP = 0, FN = 842\n",
            "F1_0 = 0.0000, F1_1 = 0.0000, F1_macro = 0.0000\n",
            "Iteration 150, loss = 0.9111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-MhweQ2jVFT4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Fully connected w. Bert sentence embeddings\n",
        "embedding_dim = 768\n",
        "max_len = MAX_SEQ\n",
        "hidden_dim_1 = 128\n",
        "hidden_dim_2 = 16\n",
        "hidden_dim_3 = 4\n",
        "lr = 0.00025\n",
        "\n",
        "model = FcnnBertEmbeddingBinary(embedding_dim, hidden_dim_1, hidden_dim_2, hidden_dim_3, max_len)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "\n",
        "train_part(model, optimizer, epochs = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xGMVF5KTg-He",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test ELMO"
      ]
    },
    {
      "metadata": {
        "id": "3LeaTI5U7x5N",
        "colab_type": "code",
        "outputId": "01f205c3-7403-4c89-bcd1-6c1ac48a257a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "# ELMO takes a list of parsed sentences as an input\n",
        "# It generates an embedding of length 1024 per word\n",
        "# We then need to find a good method of combining the word vecs to create \n",
        "# a sentence embedding (this article is good: https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a). \n",
        "\n",
        "\n",
        "#Elmo test\n",
        "sentences = [['First', 'sentence', '.'], ['Another', '.'], \n",
        "             [\"Oh\", \"here\", \"we\", \"Go\", \"now\", \"you\", \"fool\", \".\"], \n",
        "             [\"meaninglesswordnotinvocab\"]]\n",
        "             \n",
        "character_ids = batch_to_ids(sentences)\n",
        "\n",
        "embeddings = elmo(character_ids)\n",
        "\n",
        "print(character_ids.shape)\n",
        "embed = embeddings[\"elmo_representations\"]\n",
        "print(len(embed))\n",
        "print(embed[0].shape)\n",
        "print(embed[1].shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 8, 50])\n",
            "2\n",
            "torch.Size([4, 8, 1024])\n",
            "torch.Size([4, 8, 1024])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NQ4enEK_-GsZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Misc"
      ]
    },
    {
      "metadata": {
        "id": "EquNq-KbeEvT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Save intermediate results to CSV so we can use the nice torchtext .TabularDataset loader\n",
        "train_df.to_csv(path_or_buf=PREPROCESSED_FP, sep=',', na_rep='', float_format=None, \n",
        "                header=True, index=True, )\n",
        "\n",
        "#train = pd.read_csv(\"offenseval-training-v1.tsv\", delimiter=\"\\t\")\n",
        "#test = pd.read_csv(\"offenseval-trial (2).txt\", delimiter=\"\\t\")\n",
        "train = None\n",
        "\n",
        "#define our batch size\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "text_field = data.Field(sequential = False, use_vocab = False, dtype = torch.long)\n",
        "label_field = data.LabelField(sequential= False, dtype=torch.float, use_vocab = False)\n",
        "\n",
        "\n",
        "#text_field.build_vocab(train, max_size=25000, vectors=\"glove.6B.100d\")\n",
        "#label_field.build_vocab(train)\n",
        "\n",
        "train = data.TabularDataset(PREPROCESSED_FP, 'CSV', fields = \n",
        "                            [('TWEET_IDS', text_field), ('LABEL_A', label_field)], \n",
        "                            skip_header=False)\n",
        "#train = data.Dataset(examples=tweets_IDs, fields= [(\"tweet\", text_field)])\n",
        "\n",
        "\n",
        "train_iterator = data.Iterator(train, batch_size = BATCH_SIZE, )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "3f22e5dc-13a8-4c8c-8c1d-589c73efea4b",
        "id": "BRvWwHbUgkWf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "cell_type": "code",
      "source": [
        "#OLD BERT METHOD\n",
        "#train = pd.read_csv(\"offenseval-training-v1.tsv\", delimiter=\"\\t\")\n",
        "#test = pd.read_csv(\"offenseval-trial (2).txt\", delimiter=\"\\t\")\n",
        "train = None\n",
        "test = None\n",
        "\n",
        "#define our batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "text_field = data.Field(tokenize=BERT_tokenize, preprocessing = BERT_retrieve_ID, use_vocab = True, dtype = torch.long)\n",
        "label_field = data.LabelField(sequential= False, preprocessing = section_a_labels, dtype=torch.float, use_vocab = False)\n",
        "\n",
        "\n",
        "#text_field.build_vocab(train, max_size=25000, vectors=\"glove.6B.100d\")\n",
        "#label_field.build_vocab(train)\n",
        "\n",
        "train = data.TabularDataset(\"offenseval-training-v1.tsv\", 'TSV', fields = \n",
        "                            { \"tweet\": (\"tweet\", text_field), \"subtask_a\": (\"LabelA\", label_field),}, skip_header=False)\n",
        "\n",
        "text_field.build_vocab(train, vectors=\"glove.6B.50d\") #USE \"glove.840B.300d\" or glove.twitter.27B.200d\n",
        "label_field.build_vocab(train)\n",
        "\n",
        "glove_dim = 50\n",
        "\n",
        "#define our batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "#define types of data and their preprocessing\n",
        "\n",
        "#get pre-defined split\n",
        "#train = text_field.preprocess(train.iloc[0][\"tweet\"])\n",
        "#print(train)\n",
        "\n",
        "train_iterator = data.Iterator(train, batch_size = BATCH_SIZE, device=\"cuda\")\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-fa18f344874d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtext_field\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBERT_tokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERT_retrieve_ID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlabel_field\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLabelField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msection_a_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BERT_tokenize' is not defined"
          ]
        }
      ]
    }
  ]
}