{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/adesam146/nlpcw/blob/sam_preprocessing/NLP_CW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3584
    },
    "colab_type": "code",
    "id": "wxCJbS2h4jfG",
    "outputId": "d95428fb-29de-4218-8134-c5e850692cd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: allennlp in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: sqlparse==0.2.4 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (0.2.4)\n",
      "Requirement already satisfied: editdistance in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (0.5.2)\n",
      "Requirement already satisfied: cffi==1.11.5 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (1.11.5)\n",
      "Requirement already satisfied: scipy in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (1.2.1)\n",
      "Requirement already satisfied: unidecode in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (1.0.23)\n",
      "Requirement already satisfied: torch>=0.4.1 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (1.0.1)\n",
      "Requirement already satisfied: awscli>=1.11.91 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (1.16.102)\n",
      "Requirement already satisfied: numpydoc==0.8.0 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (0.8.0)\n",
      "Requirement already satisfied: requests>=2.18 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (2.21.0)\n",
      "Requirement already satisfied: spacy<2.1,>=2.0 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (2.0.18)\n",
      "Requirement already satisfied: tqdm>=4.19 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (4.31.1)\n",
      "Requirement already satisfied: overrides in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (1.9)\n",
      "Requirement already satisfied: flask-cors==3.0.7 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (3.0.7)\n",
      "Requirement already satisfied: matplotlib==2.2.3 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (2.2.3)\n",
      "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (0.5.6)\n",
      "Requirement already satisfied: responses>=0.7 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (0.10.5)\n",
      "Requirement already satisfied: numpy in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (1.16.1)\n",
      "Requirement already satisfied: gevent==1.3.6 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (1.3.6)\n",
      "Requirement already satisfied: parsimonious==0.8.0 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (0.8.0)\n",
      "Requirement already satisfied: pytorch-pretrained-bert==0.3.0 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (0.3.0)\n",
      "Requirement already satisfied: nltk in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (3.4)\n",
      "Requirement already satisfied: boto3 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (1.9.92)\n",
      "Requirement already satisfied: flask==1.0.2 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (1.0.2)\n",
      "Requirement already satisfied: tensorboardX==1.2 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (1.2)\n",
      "Requirement already satisfied: flaky in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (3.5.3)\n",
      "Requirement already satisfied: h5py in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (2.9.0)\n",
      "Requirement already satisfied: moto==1.3.4 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (1.3.4)\n",
      "Requirement already satisfied: pytz==2017.3 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (2017.3)\n",
      "Requirement already satisfied: ftfy in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (5.5.1)\n",
      "Requirement already satisfied: pytest in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (4.2.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (0.20.2)\n",
      "Requirement already satisfied: conllu==0.11 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from allennlp) (0.11)\n",
      "Requirement already satisfied: pycparser in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from cffi==1.11.5->allennlp) (2.19)\n",
      "Requirement already satisfied: PyYAML<=3.13,>=3.10 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from awscli>=1.11.91->allennlp) (3.13)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from awscli>=1.11.91->allennlp) (0.2.0)\n",
      "Requirement already satisfied: docutils>=0.10 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from awscli>=1.11.91->allennlp) (0.14)\n",
      "Requirement already satisfied: rsa<=3.5.0,>=3.1.2 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from awscli>=1.11.91->allennlp) (3.4.2)\n",
      "Requirement already satisfied: colorama<=0.3.9,>=0.2.5 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from awscli>=1.11.91->allennlp) (0.3.9)\n",
      "Requirement already satisfied: botocore==1.12.92 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from awscli>=1.11.91->allennlp) (1.12.92)\n",
      "Requirement already satisfied: sphinx>=1.2.3 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from numpydoc==0.8.0->allennlp) (1.8.4)\n",
      "Requirement already satisfied: Jinja2>=2.3 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from numpydoc==0.8.0->allennlp) (2.10)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from requests>=2.18->allennlp) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from requests>=2.18->allennlp) (2018.11.29)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from requests>=2.18->allennlp) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from requests>=2.18->allennlp) (2.8)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from spacy<2.1,>=2.0->allennlp) (0.2.9)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from spacy<2.1,>=2.0->allennlp) (0.9.6)\n",
      "Requirement already satisfied: ujson>=1.35 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from spacy<2.1,>=2.0->allennlp) (1.35)\n",
      "Requirement already satisfied: regex==2018.01.10 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from spacy<2.1,>=2.0->allennlp) (2018.1.10)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from spacy<2.1,>=2.0->allennlp) (2.0.1)\n",
      "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from spacy<2.1,>=2.0->allennlp) (6.12.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from spacy<2.1,>=2.0->allennlp) (1.0.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from spacy<2.1,>=2.0->allennlp) (2.0.2)\n",
      "Requirement already satisfied: Six in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from flask-cors==3.0.7->allennlp) (1.12.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from matplotlib==2.2.3->allennlp) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from matplotlib==2.2.3->allennlp) (2.8.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from matplotlib==2.2.3->allennlp) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from matplotlib==2.2.3->allennlp) (2.3.1)\n",
      "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from gevent==1.3.6->allennlp) (0.4.15)\n",
      "Requirement already satisfied: singledispatch in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from nltk->allennlp) (3.4.0.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from boto3->allennlp) (0.9.3)\n",
      "Requirement already satisfied: click>=5.1 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from flask==1.0.2->allennlp) (7.0)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from flask==1.0.2->allennlp) (1.1.0)\n",
      "Requirement already satisfied: Werkzeug>=0.14 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from flask==1.0.2->allennlp) (0.14.1)\n",
      "Requirement already satisfied: protobuf>=0.3.2 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from tensorboardX==1.2->allennlp) (3.6.1)\n",
      "Requirement already satisfied: cookies in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from moto==1.3.4->allennlp) (2.2.1)\n",
      "Requirement already satisfied: python-jose<3.0.0 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from moto==1.3.4->allennlp) (2.0.2)\n",
      "Requirement already satisfied: xmltodict in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from moto==1.3.4->allennlp) (0.12.0)\n",
      "Requirement already satisfied: jsondiff==1.1.1 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from moto==1.3.4->allennlp) (1.1.1)\n",
      "Requirement already satisfied: mock in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from moto==1.3.4->allennlp) (2.0.0)\n",
      "Requirement already satisfied: docker>=2.5.1 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from moto==1.3.4->allennlp) (3.7.0)\n",
      "Requirement already satisfied: aws-xray-sdk<0.96,>=0.93 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from moto==1.3.4->allennlp) (0.95)\n",
      "Requirement already satisfied: cryptography>=2.0.0 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from moto==1.3.4->allennlp) (2.5)\n",
      "Requirement already satisfied: boto>=2.36.0 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from moto==1.3.4->allennlp) (2.49.0)\n",
      "Requirement already satisfied: pyaml in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from moto==1.3.4->allennlp) (18.11.0)\n",
      "Requirement already satisfied: wcwidth in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from ftfy->allennlp) (0.1.7)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from pytest->allennlp) (6.0.0)\n",
      "Requirement already satisfied: py>=1.5.0 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from pytest->allennlp) (1.7.0)\n",
      "Requirement already satisfied: pluggy>=0.7 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from pytest->allennlp) (0.8.1)\n",
      "Requirement already satisfied: setuptools in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from pytest->allennlp) (28.8.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from pytest->allennlp) (18.2.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from pytest->allennlp) (1.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from rsa<=3.5.0,>=3.1.2->awscli>=1.11.91->allennlp) (0.4.5)\n",
      "Requirement already satisfied: imagesize in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (1.1.0)\n",
      "Requirement already satisfied: sphinxcontrib-websupport in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (1.1.0)\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (1.2.1)\n",
      "Requirement already satisfied: babel!=2.0,>=1.3 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (2.6.0)\n",
      "Requirement already satisfied: packaging in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (19.0)\n",
      "Requirement already satisfied: Pygments>=2.0 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (2.3.1)\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (0.7.12)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from Jinja2>=2.3->numpydoc==0.8.0->allennlp) (1.1.0)\n",
      "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from thinc<6.13.0,>=6.12.1->spacy<2.1,>=2.0->allennlp) (0.9.0.1)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from thinc<6.13.0,>=6.12.1->spacy<2.1,>=2.0->allennlp) (1.10.11)\n",
      "Requirement already satisfied: msgpack-numpy<0.4.4 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from thinc<6.13.0,>=6.12.1->spacy<2.1,>=2.0->allennlp) (0.4.3.2)\n",
      "Requirement already satisfied: pycryptodome<4.0.0,>=3.3.1 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from python-jose<3.0.0->moto==1.3.4->allennlp) (3.7.3)\n",
      "Requirement already satisfied: ecdsa<1.0 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from python-jose<3.0.0->moto==1.3.4->allennlp) (0.13)\n",
      "Requirement already satisfied: future<1.0 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from python-jose<3.0.0->moto==1.3.4->allennlp) (0.17.1)\n",
      "Requirement already satisfied: pbr>=0.11 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from mock->moto==1.3.4->allennlp) (5.1.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from docker>=2.5.1->moto==1.3.4->allennlp) (0.4.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from docker>=2.5.1->moto==1.3.4->allennlp) (0.54.0)\n",
      "Requirement already satisfied: pypiwin32==223; sys_platform == \"win32\" and python_version >= \"3.6\" in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from docker>=2.5.1->moto==1.3.4->allennlp) (223)\n",
      "Requirement already satisfied: jsonpickle in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from aws-xray-sdk<0.96,>=0.93->moto==1.3.4->allennlp) (1.1)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from cryptography>=2.0.0->moto==1.3.4->allennlp) (0.24.0)\n",
      "Requirement already satisfied: toolz>=0.8.0 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy<2.1,>=2.0->allennlp) (0.9.0)\n",
      "Requirement already satisfied: pywin32>=223 in c:\\projects\\nlpcw\\myvenv\\lib\\site-packages (from pypiwin32==223; sys_platform == \"win32\" and python_version >= \"3.6\"->docker>=2.5.1->moto==1.3.4->allennlp) (224)\n"
     ]
    }
   ],
   "source": [
    "!pip install allennlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Srpq8hYt4whg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "8iTnEx03ZO2q",
    "outputId": "04f6974b-2e0f-4d22-c360-62527b24417e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/19/2019 10:13:18 - INFO - allennlp.modules.elmo -   Initializing ELMo\n"
     ]
    }
   ],
   "source": [
    "#Use pretrained ELMO weights. \n",
    "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "\n",
    "elmo = Elmo(options_file, weight_file, 2, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "3LeaTI5U7x5N",
    "outputId": "6fa6acca-c206-4f96-a2b5-cc7eb15063a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 50])\n",
      "dict_keys(['elmo_representations', 'mask'])\n",
      "2\n",
      "torch.Size([4, 8, 1024])\n",
      "torch.Size([4, 8, 1024])\n"
     ]
    }
   ],
   "source": [
    "#Elmo test\n",
    "sentences = [['First', 'sentence', '.'], ['Another', '.'], \n",
    "             [\"Oh\", \"here\", \"we\", \"Go\", \"now\", \"you\", \"fool\", \".\"], \n",
    "             [\"meaninglesswordnotinvocab\"]]\n",
    "             \n",
    "character_ids = batch_to_ids(sentences)\n",
    "\n",
    "# The shape would be (#sentences, length_of_longest_sentence, len of longest word(in original corpus?)=50)\n",
    "# Note if a word has n characters the first n+2 entries in its size 50 embedding would be used to\n",
    "# represent it. '+2' because it seems like they implicitly add a start and end of word\n",
    "# 'character'\n",
    "print(character_ids.shape)\n",
    "\n",
    "embeddings = elmo(character_ids)\n",
    "print(embeddings.keys())\n",
    "embed = embeddings[\"elmo_representations\"]\n",
    "\n",
    "# The length of embed is given by the num_output_representations we specfied\n",
    "# when creating the ELMO class, I believe this are just the number of different\n",
    "# weighted combination of the 3 layers in the \"ELMO network\" so could theortically\n",
    "# be any integer but is usually 1 or 2\n",
    "print(len(embed))\n",
    "print(embed[0].shape)\n",
    "print(embed[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4N-meDamEjF7"
   },
   "outputs": [],
   "source": [
    "# ELMO takes a list of parsed sentences as an input\n",
    "# It generates an embedding of length 1024 per word\n",
    "# We then need to find a good method of combining the word vecs to create \n",
    "# a sentence embedding (this article is good: https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "rirRoy-obq7J",
    "outputId": "51a799d6-8a71-4478-e97e-1296cb6bcd9a"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "#File upload instructions:\n",
    "  #1. Run this cell\n",
    "  #2. Press \"Choose Files\" at the bottom of this cell\n",
    "  #3. Select offenseval-training-v1.tsv locally to load it as the variable noisy_values  \n",
    "  \n",
    "file1 = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "id": "EquNq-KbeEvT",
    "outputId": "a45f53ac-3dba-40ce-c4b1-9c437988bbc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13240, 5)\n",
      "      id                                              tweet subtask_a  \\\n",
      "0  86426  @USER She should ask a few native Americans wh...       OFF   \n",
      "1  90194  @USER @USER Go home youâ€™re drunk!!! @USER #MAG...       OFF   \n",
      "2  16820  Amazon is investigating Chinese employees who ...       NOT   \n",
      "3  62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
      "4  43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
      "\n",
      "  subtask_b subtask_c  \n",
      "0       UNT       NaN  \n",
      "1       TIN       IND  \n",
      "2       NaN       NaN  \n",
      "3       UNT       NaN  \n",
      "4       NaN       NaN  \n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"OffensEval_task_data/start-kit/training-v1/offenseval-training-v1.tsv\", delimiter=\"\\t\")\n",
    "print(train.shape)\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           13240\n",
       "tweet        13240\n",
       "subtask_a    13240\n",
       "subtask_b     4400\n",
       "subtask_c     3876\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of offensive 4400\n",
      "Number of inoffensive 8840\n"
     ]
    }
   ],
   "source": [
    "total = train['id'].count().item()\n",
    "off_count = train[train['subtask_a'] == \"OFF\"]['id'].count()\n",
    "\n",
    "print(\"Number of offensive\", off_count)\n",
    "print(\"Number of inoffensive\", total - off_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above shows that the training dataset is not very balanced (in offensive is about twice as much). How could this be addressed. Get more data? Augment offensive comments by adding neutral words to create more data or concat offensive and inoffensive comments to make new offensive comments?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\projects\\nlpcw\\myvenv\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "c:\\projects\\nlpcw\\myvenv\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "training_percent = 0.8\n",
    "training_size = int(training_percent * total)\n",
    "validation_size = total - training_size\n",
    "\n",
    "corpus = train['tweet'].to_numpy()\n",
    "labels = train['subtask_a']\n",
    "labels[labels == 'OFF'] = 1.\n",
    "labels[labels == 'NOT'] = 0.\n",
    "labels = labels.to_numpy(dtype=np.double).reshape(-1, 1)\n",
    "\n",
    "# train_dataset, validation_dataset = torch.utils.data.random_split(TensorDataset(corpus, labels), [training_size, validation_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.shuffle(list(range(total)))\n",
    "\n",
    "training_sents = corpus[:training_size]\n",
    "training_labels = labels[:training_size]\n",
    "\n",
    "validation_sents = corpus[training_size:]\n",
    "validation_labels = labels[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_tokenised_corpus(corpus):\n",
    "    \"\"\"\n",
    "    This assumes the corpus can be iterated through and\n",
    "    retains the order in which the sentences appeared in the corpus\n",
    "    \"\"\"\n",
    "    tokenized_corpus = [] # Let us put the tokenized corpus in a list\n",
    "    for sentence in corpus:\n",
    "        tokenized_sentence = []\n",
    "        for token in re.split(r'\\s', sentence.lower()): # simplest split is \n",
    "            if token:\n",
    "              # To avoid the empty string\n",
    "              tokenized_sentence.append(token)\n",
    "        tokenized_corpus.append(tokenized_sentence)\n",
    "    \n",
    "    return tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[259,  65, 118,  ..., 261, 261, 261],\n",
      "         [259, 116, 105,  ..., 261, 261, 261],\n",
      "         [259, 116, 105,  ..., 261, 261, 261],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[259,  65, 118,  ..., 261, 261, 261],\n",
      "         [259,  65, 118,  ..., 261, 261, 261],\n",
      "         [259, 104, 112,  ..., 261, 261, 261],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]]])\n"
     ]
    }
   ],
   "source": [
    "training_ids = batch_to_ids(get_tokenised_corpus(training_sents))\n",
    "validation_ids = batch_to_ids(get_tokenised_corpus(validation_sents))\n",
    "\n",
    "print(training_ids[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, out_channels, window_size, dropout):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.embeddings = Elmo(options_file, weight_file, 1, dropout=0)\n",
    "        embedding_dim = 1024\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(window_size, embedding_dim))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(out_channels, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is the output from batch_to_ids\n",
    "        \n",
    "        \n",
    "        # Only looking at one (the first) layer from elmo for now \n",
    "        # which is my I am indexing at 0\n",
    "        embedded = self.embeddings(x)['elmo_representations'][0]\n",
    "        \n",
    "        #images have 3 RGB channels \n",
    "        #for the text we add 1 channel\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #(batch size, 1, max sent length, embedding dim)\n",
    "        \n",
    "        feature_maps = self.conv(embedded)\n",
    "        # (batch size, out_channels, max sent length - window size +1, 1)\n",
    "        \n",
    "        feature_maps = feature_maps.squeeze(3)\n",
    "        feature_maps = F.relu(feature_maps)\n",
    "        \n",
    "        #the max pooling layer\n",
    "        pooled = F.max_pool1d(feature_maps, feature_maps.shape[2])\n",
    "        pooled = pooled.squeeze(2)\n",
    "        # (batch size, out_channels)\n",
    "        \n",
    "        dropped = self.dropout(pooled)\n",
    " \n",
    "        return self.fc(dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(TensorDataset(training_ids, torch.from_numpy(training_labels)), batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " def accuracy(output, target):\n",
    "    with torch.no_grad():\n",
    "        prob_output = torch.sigmoid(output)\n",
    "\n",
    "        prob_output[prob_output > 0.5] = 1.\n",
    "        prob_output[prob_output <= 0.5] = 0.\n",
    "\n",
    "        acc = (prob_output == target).sum(dtype=torch.float) / output.shape[0]\n",
    " \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/19/2019 10:21:45 - INFO - allennlp.modules.elmo -   Initializing ELMo\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "model = SimpleClassifier(out_channels=100, window_size=3, dropout=0.5)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n",
      "Got here 2\n",
      "Got here 3\n",
      "Got here 1\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    losses = []\n",
    "    accs = []\n",
    "    for sent_ids, target in data_loader:\n",
    "        print(\"Got here 1\")\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(sent_ids)\n",
    "        logits = logits.type(torch.DoubleTensor)\n",
    "        print(\"Got here 2\")\n",
    "\n",
    "        target = target.type(torch.DoubleTensor)\n",
    "        loss = loss_fn(logits, target)\n",
    "        \n",
    "        print(\"Got here 3\")\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             logits = model(validation_ids)\n",
    "#             losses.append(lost_fn(logits, validation_labels).item())\n",
    "#             accs.append(accuracy(logits, target))\n",
    "            \n",
    "    print(f'| Epoch: {epoch:02} | Val. Loss: {np.mean(losses):.3f} | Val. Acc: {np.mean(accs)*100:.2f}% |')\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "NLP_CW.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
