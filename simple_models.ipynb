{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_CW.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adesam146/nlpcw/blob/rest_of_tasks/simple_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Srpq8hYt4whg",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import svm, metrics\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EquNq-KbeEvT",
        "outputId": "1c3a4fb7-33f9-475d-9cb7-08f539afdb95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"./offenseval-training-v1.tsv\", delimiter=\"\\t\")\n",
        "print(train.shape)\n",
        "print(train.head())"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13240, 5)\n",
            "      id                                              tweet subtask_a  \\\n",
            "0  86426  @USER She should ask a few native Americans wh...       OFF   \n",
            "1  90194  @USER @USER Go home youâ€™re drunk!!! @USER #MAG...       OFF   \n",
            "2  16820  Amazon is investigating Chinese employees who ...       NOT   \n",
            "3  62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
            "4  43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
            "\n",
            "  subtask_b subtask_c  \n",
            "0       UNT       NaN  \n",
            "1       TIN       IND  \n",
            "2       NaN       NaN  \n",
            "3       UNT       NaN  \n",
            "4       NaN       NaN  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "u8yrV9tCWJrT",
        "outputId": "2af9ed2f-ecda-4f4f-8498-895ccdbd6242",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "cell_type": "code",
      "source": [
        "train.count()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id           13240\n",
              "tweet        13240\n",
              "subtask_a    13240\n",
              "subtask_b     4400\n",
              "subtask_c     3876\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "af5bww85WJrW",
        "outputId": "07bdc559-bcb0-4306-a384-5fcc9b63765a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "total = train['id'].count().item()\n",
        "off_count = train[train['subtask_a'] == \"OFF\"]['id'].count()\n",
        "\n",
        "print(\"Number of offensive\", off_count)\n",
        "print(\"Number of inoffensive\", total - off_count)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of offensive 4400\n",
            "Number of inoffensive 8840\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K8qdnuP2C5Zb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TASK A"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "XxJeNqSbWJrZ"
      },
      "cell_type": "markdown",
      "source": [
        "**The above shows that the training dataset is not very balanced (in offensive is about twice as much). How could this be addressed. Get more data? Augment offensive comments by adding neutral words to create more data or concat offensive and inoffensive comments to make new offensive comments?**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OkVNhZ3hWJra",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_percent = 0.8\n",
        "training_size = int(training_percent * total)\n",
        "validation_size = total - training_size\n",
        "\n",
        "corpus = train['tweet'].to_numpy()\n",
        "labels = train['subtask_a'].to_numpy()\n",
        "labels[labels == 'OFF'] = 1\n",
        "labels[labels == 'NOT'] = 0\n",
        "\n",
        "labels = labels.astype(float)\n",
        "\n",
        "indices = list(range(total))\n",
        "np.random.shuffle(indices)\n",
        "training_sents = corpus[indices[:training_size]]\n",
        "training_labels = labels[indices[:training_size]]\n",
        "\n",
        "validation_sents = corpus[indices[training_size:]]\n",
        "validation_labels = labels[indices[training_size:]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Ji6AkNbLWJrf",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def get_tokenised_corpus(corpus):\n",
        "    \"\"\"\n",
        "    This assumes the corpus can be iterated through and\n",
        "    retains the order in which the sentences appeared in the corpus\n",
        "    \"\"\"\n",
        "    tokenized_corpus = [] # Let us put the tokenized corpus in a list\n",
        "    for sentence in corpus:\n",
        "        tokenized_sentence = []\n",
        "        for token in re.split(r'\\s', sentence.lower()): # simplest split is \n",
        "            if token:\n",
        "              # To avoid the empty string\n",
        "              tokenized_sentence.append(token)\n",
        "        tokenized_corpus.append(tokenized_sentence)\n",
        "    \n",
        "    return tokenized_corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d5NEltnwCkiD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "vec_training = vectorizer.fit_transform(training_sents)\n",
        "\n",
        "tf_transformer = TfidfTransformer(use_idf=False)\n",
        "vec_training = tf_transformer.fit_transform(vec_training)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bld3qbEpCkiR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "61488dff-4b46-4055-e5a1-2b8728139e0d"
      },
      "cell_type": "code",
      "source": [
        "clf = SGDClassifier(loss='hinge', penalty='l1',\n",
        "                          alpha=1e-3, random_state=42,\n",
        "                          max_iter=5, tol=None, class_weight={1.0: 2})\n",
        "\n",
        "clf.fit(vec_training, training_labels)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(alpha=0.001, average=False, class_weight={1.0: 2},\n",
              "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
              "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,\n",
              "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l1',\n",
              "       power_t=0.5, random_state=42, shuffle=True, tol=None,\n",
              "       validation_fraction=0.1, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "metadata": {
        "id": "3QbVqHdUCkio",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vec_valid = tf_transformer.transform(vectorizer.transform(validation_sents))\n",
        "predictions = clf.predict(vec_valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5pn-eu6JCkiw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "dd037b0c-fa2d-4f17-b9b7-bc78e607c681"
      },
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(validation_labels, predictions))\n",
        "print(\"Accuracy:\", metrics.accuracy_score(validation_labels, predictions))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.74      0.88      0.80      1761\n",
            "         1.0       0.62      0.38      0.47       887\n",
            "\n",
            "   micro avg       0.71      0.71      0.71      2648\n",
            "   macro avg       0.68      0.63      0.64      2648\n",
            "weighted avg       0.70      0.71      0.69      2648\n",
            "\n",
            "Accuracy: 0.7137462235649547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B9f0yiTlCki6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "train_pos_sents = []\n",
        "train_pos_labels = []\n",
        "train_neg_sents = []\n",
        "train_neg_labels = []\n",
        "\n",
        "for sent, label in zip(training_sents, training_labels):\n",
        "    if TextBlob(sent).sentiment.polarity > -0.2:\n",
        "        train_pos_sents.append(sent)\n",
        "        train_pos_labels.append(label)\n",
        "    else:\n",
        "        train_neg_sents.append(sent)\n",
        "        train_neg_labels.append(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1wIJFw6tCkjB",
        "colab_type": "code",
        "colab": {},
        "outputId": "bb07b64a-4088-4973-eb0b-1fea61b63d81"
      },
      "cell_type": "code",
      "source": [
        "clf1 = SGDClassifier(loss='hinge', penalty='l1',\n",
        "                          alpha=1e-3, random_state=42,\n",
        "                          max_iter=5, tol=None, class_weight={1.0: 2})\n",
        "clf1.fit(tf_transformer.transform(vectorizer.transform(train_pos_sents)), train_pos_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(alpha=0.001, average=False, class_weight={1.0: 2},\n",
              "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
              "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,\n",
              "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l1',\n",
              "       power_t=0.5, random_state=42, shuffle=True, tol=None,\n",
              "       validation_fraction=0.1, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "metadata": {
        "id": "88Atez8_CkjG",
        "colab_type": "code",
        "colab": {},
        "outputId": "e5c1aae3-7184-484c-c319-2188b63050eb"
      },
      "cell_type": "code",
      "source": [
        "clf2 = SGDClassifier(loss='hinge', penalty='l1',\n",
        "                          alpha=1e-3, random_state=42,\n",
        "                          max_iter=5, tol=None, class_weight={0.0: 1.15})\n",
        "clf2.fit(tf_transformer.transform(vectorizer.transform(train_neg_sents)), train_neg_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(alpha=0.001, average=False, class_weight={0.0: 1.15},\n",
              "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
              "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,\n",
              "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l1',\n",
              "       power_t=0.5, random_state=42, shuffle=True, tol=None,\n",
              "       validation_fraction=0.1, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "metadata": {
        "id": "sxwKzUPkCkjj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "valid_pos_sents = []\n",
        "valid_pos_labels = []\n",
        "valid_neg_sents = []\n",
        "valid_neg_labels = []\n",
        "\n",
        "for sent, label in zip(validation_sents, validation_labels):\n",
        "    if TextBlob(sent).sentiment.polarity > -0.2:\n",
        "        valid_pos_sents.append(sent)\n",
        "        valid_pos_labels.append(label)\n",
        "    else:\n",
        "        valid_neg_sents.append(sent)\n",
        "        valid_neg_labels.append(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QSTPIMEJCkjp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preds1 = clf1.predict(tf_transformer.transform(vectorizer.transform(valid_pos_sents)))\n",
        "preds2 = clf2.predict(tf_transformer.transform(vectorizer.transform(valid_neg_sents)))\n",
        "\n",
        "predictions_joined = np.concatenate((preds1, preds2), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NZqJFnueCkjx",
        "colab_type": "code",
        "colab": {},
        "outputId": "08eff541-428f-4b8d-fa4d-84153022b9db"
      },
      "cell_type": "code",
      "source": [
        "print(\"Classifier for positive sentiments\")\n",
        "print(metrics.classification_report(valid_pos_labels, preds1))\n",
        "print(\"Accuracy:\", metrics.accuracy_score(valid_pos_labels, preds1))\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"Classifier for negative sentiments\")\n",
        "print(metrics.classification_report(valid_neg_labels, preds2))\n",
        "print(\"Accuracy:\", metrics.accuracy_score(valid_neg_labels, preds2))\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"Overall classifier\")\n",
        "print(metrics.classification_report(validation_labels, predictions_joined))\n",
        "print(\"Accuracy:\", metrics.accuracy_score(validation_labels, predictions_joined))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier for positive sentiments\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.97      0.85      1604\n",
            "         1.0       0.75      0.21      0.32       648\n",
            "\n",
            "   micro avg       0.75      0.75      0.75      2252\n",
            "   macro avg       0.75      0.59      0.59      2252\n",
            "weighted avg       0.75      0.75      0.70      2252\n",
            "\n",
            "Accuracy: 0.7517761989342806\n",
            "\n",
            "Classifier for negative sentiments\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.58      0.79      0.67       153\n",
            "         1.0       0.83      0.64      0.72       243\n",
            "\n",
            "   micro avg       0.70      0.70      0.70       396\n",
            "   macro avg       0.71      0.72      0.70       396\n",
            "weighted avg       0.73      0.70      0.70       396\n",
            "\n",
            "Accuracy: 0.6994949494949495\n",
            "\n",
            "Overall classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.87      0.75      1757\n",
            "         1.0       0.36      0.15      0.21       891\n",
            "\n",
            "   micro avg       0.62      0.62      0.62      2648\n",
            "   macro avg       0.51      0.51      0.48      2648\n",
            "weighted avg       0.56      0.62      0.57      2648\n",
            "\n",
            "Accuracy: 0.6238670694864048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AIVC7KehDGU0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TASK B"
      ]
    },
    {
      "metadata": {
        "id": "lRM9EYtxDI6I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "total_b = train.count()['subtask_b'].item()\n",
        "training_percent = 0.8\n",
        "training_size = int(training_percent * total_b)\n",
        "validation_size = total_b - training_size\n",
        "\n",
        "train_b = train[train.subtask_a == 'OFF']\n",
        "corpus = train_b['tweet'].to_numpy()\n",
        "labels = train_b['subtask_b'].to_numpy()\n",
        "labels[labels == 'TIN'] = 0\n",
        "labels[labels == 'UNT'] = 1\n",
        "labels = labels.astype(float)\n",
        "\n",
        "indices = list(range(total_b))\n",
        "np.random.shuffle(indices)\n",
        "training_sents = corpus[indices[:training_size]]\n",
        "training_labels = labels[indices[:training_size]]\n",
        "\n",
        "validation_sents = corpus[indices[training_size:]]\n",
        "validation_labels = labels[indices[training_size:]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XBZsUuEdHrYT",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "vec_training = vectorizer.fit_transform(training_sents)\n",
        "\n",
        "tf_transformer = TfidfTransformer(use_idf=False)\n",
        "vec_training = tf_transformer.fit_transform(vec_training)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qK9oyn2XHstx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "82a43d03-33ae-4c17-f623-e27676545104"
      },
      "cell_type": "code",
      "source": [
        "clf = SGDClassifier(loss='hinge', penalty='l1',\n",
        "                          alpha=1e-3, random_state=42,\n",
        "                          max_iter=5, tol=None, class_weight={1.0: 2})\n",
        "\n",
        "clf.fit(vec_training, training_labels)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(alpha=0.001, average=False, class_weight={1.0: 2},\n",
              "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
              "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,\n",
              "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l1',\n",
              "       power_t=0.5, random_state=42, shuffle=True, tol=None,\n",
              "       validation_fraction=0.1, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "metadata": {
        "id": "faReJkFpH1nV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vec_valid = tf_transformer.transform(vectorizer.transform(validation_sents))\n",
        "predictions = clf.predict(vec_valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6JeaeTjOH9E7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "15236f67-e67e-4111-c609-1da8ffa1b48d"
      },
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(validation_labels, predictions))\n",
        "print(\"Accuracy:\", metrics.accuracy_score(validation_labels, predictions))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.89      0.98      0.94       784\n",
            "         1.0       0.18      0.03      0.05        96\n",
            "\n",
            "   micro avg       0.88      0.88      0.88       880\n",
            "   macro avg       0.53      0.51      0.49       880\n",
            "weighted avg       0.81      0.88      0.84       880\n",
            "\n",
            "Accuracy: 0.8784090909090909\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}