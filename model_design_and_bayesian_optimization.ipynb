{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_design_and_bayesian_optimization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adesam146/nlpcw/blob/master/model_design_and_bayesian_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "cnBfwWnDYg_n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "Note to marker: Please display this in colab rather than as a pdf or in jupyter.\n",
        "\n",
        "This document gives an overview of the thought process behind our final model. To see the model itself skip to the \"Final Model\" subsection in the table of contents. \n",
        "\n",
        "We used GloVe embeddings and a CNN for our final classifiers but we also did some preliminary investigation into BERT and ELMO embeddings as well as traditional machine learning classifiers. This work would break up the flow of this notebook but can be found in BERT_and_ELMO.ipynb and SVMs_vs_NN.ipynb. "
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "wTfeo8tcxhwC"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ePuqIHSPf554",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U spacy ftfy torchtext\n",
        "!python -m spacy download en\n",
        "!pip install -U textblob #Sentiment analysis\n",
        "!python -m textblob.download_corpora\n",
        "!pip install scikit-optimize\n",
        "!pip install pandas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6NVQcb0MKUCh",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#DONWLOAD GLOVE vectors and unzip\n",
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "!unzip glove.twitter.27B.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Srpq8hYt4whg",
        "outputId": "4b9eb803-4b1b-4d88-fded-9865de38dd4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data import sampler\n",
        "from torch import autograd\n",
        "import spacy\n",
        "from torchtext import data\n",
        "from torchtext import datasets as nlp_dset\n",
        "import random\n",
        "from sklearn.utils import resample\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import string\n",
        "from sklearn import metrics\n",
        "from skopt import gp_minimize #Beyesian optimization\n",
        "import torchvision.transforms as T\n",
        "import nltk\n",
        "\n",
        "#initialize spaCy\n",
        "nlp_spaCy = spacy.load('en', disable=[\"tagger\", \"parser\", \"ner\"])\n",
        "\n",
        "\n",
        "#Create list of stopwords\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "stops_nltk = list(stopwords.words('english'))\n",
        "stops_sklearn = list(ENGLISH_STOP_WORDS)\n",
        "STOPWORDS = list(set(stops_nltk + stops_sklearn))\n",
        "\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "#Fix all seeds\n",
        "SEED = 0\n",
        "\n",
        "def set_seed(seed=SEED):\n",
        "    \"\"\"Helper function to set all seeds to make results reproducible\"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "set_seed()\n",
        "\n",
        "\n",
        "#Set device = cuda \n",
        "GPU = True\n",
        "device_idx = 0\n",
        "if GPU:\n",
        "    device = torch.device(\"cuda:\"+str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"Using device:\", device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Using device: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qtiwRhtm3s87",
        "outputId": "bd243f56-1472-40f3-d686-2fde15aefee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "# Load datafiles from own google drive (or local)\n",
        "\n",
        "# EDIT AS NECESSARY:\n",
        "#################\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "One_drive = \"\"\"/content/drive/My Drive/colab_data/\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "train_fp = One_drive + \"offenseval-training-v1.tsv\"\n",
        "\n",
        "#Store data in dataframe for later\n",
        "train_df = pd.read_csv(train_fp, delimiter=\"\\t\")\n",
        "\n",
        "#Import test datasets:\n",
        "test_fp_A = One_drive + \"testset-taska.tsv\"\n",
        "test_fp_B = One_drive + \"testset-taskb.tsv\"\n",
        "test_fp_C = One_drive + \"test_set_taskc.tsv\"\n",
        "\n",
        "#################"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "t9Zt3py7E1ep"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aMY0mUyknLDu",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenize_params(text, params):\n",
        "    \"\"\"tokenizes, and optionally also:\n",
        "        1) replaces words with their lemmatized forms\n",
        "        2) removes punctuation\n",
        "        3) removes english stopwords. \n",
        "    \"\"\"\n",
        "    \n",
        "    lemmatize = params.get(\"lemmatize\")\n",
        "    rem_punct = params.get(\"rem_punct\")\n",
        "    rem_stopwords = params.get(\"rem_stopwords\")\n",
        "    \n",
        "    #deal with stopwords\n",
        "    if rem_stopwords:\n",
        "        stopwords = STOPWORDS\n",
        "    else:\n",
        "        stopwords = []\n",
        "    \n",
        "    #deal with no punctuation\n",
        "    if rem_punct:\n",
        "        stoptokens = [x for x in list(string.punctuation) if x not in list(\"#$&*@\")]\n",
        "        stoptokens += stopwords\n",
        "    else:\n",
        "        stoptokens = stopwords \n",
        "    #stoptokens will be removed from tokens\n",
        "        \n",
        "    #replace each sentence with its lemmatized counterpart\n",
        "    if not lemmatize:\n",
        "        result = [tok.text for tok in nlp_spaCy.tokenizer(text) if tok.text not in stoptokens]\n",
        "    else:\n",
        "        #otherwise: lemmatize\n",
        "        tweet = nlp_spaCy(text)  #SpaCy tokenizes and does POS and lemmatization on tokens \n",
        "        tokens = []\n",
        "        for counter, token in enumerate(tweet):\n",
        "            if token.lemma_ == \"-PRON-\":         #treat pronouns differently as SpaCy replaces all of them with \"-PRON-\"\n",
        "                tokens.append(token.text)        #which therefore becomes a common token that biases results\n",
        "            #For everything else, add the lemma:\n",
        "            else:\n",
        "                tokens.append(token.lemma_)\n",
        "        result = [tok for tok in tokens if tok not in stoptokens]\n",
        "    return result\n",
        "\n",
        "def tokenizer(text):\n",
        "    \"\"\"wrapper for tokenize so it can be used in torchtext Field creation. It takes the \n",
        "    current global varaiable TOKENIZE_PARAMS\"\"\"\n",
        "    try:\n",
        "        params = TOKENIZE_PARAMS\n",
        "    except NameError:\n",
        "        print(\"You must initialize the global variable TOKENIZE_PARAMS\")\n",
        "        raise NameError\n",
        "    return tokenize_params(text, params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "BEwuaMk_UCCv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Add sentiment\n",
        "def add_sentiment(h, ids):\n",
        "    \"\"\"Calculate tweet sentiment and concatenate this feature onto the \n",
        "    feature matrix (typically just before the fully connected layers). \n",
        "    The feature matrix dimensions will be changed as follows: \n",
        "        (B, O) -> (B, O + 2)\n",
        "        where B = batch size, O = out channels\n",
        "    \"\"\"\n",
        "\n",
        "    assert type(ids) == torch.Tensor, \"If sentiment == True, ids must be of type tensor\"\n",
        "\n",
        "    #retrieve tweets using id:\n",
        "    tweets = train_df[train_df[\"id\"].isin(ids.cpu().numpy())]\n",
        "\n",
        "    sentiments = []\n",
        "    subjectivities = []\n",
        "    \n",
        "    #extract \"sentiment\" and \"subjectivity\" according to TextBlob:\n",
        "    sentiments, subjectivities = get_sentiment_v(tweets[\"tweet\"].values)\n",
        "    \n",
        "    sentiments = torch.cuda.FloatTensor(sentiments, device=device).unsqueeze(1)\n",
        "    subjectivities = torch.cuda.FloatTensor(subjectivities, device=device).unsqueeze(1)\n",
        "    \n",
        "    \n",
        "    h = torch.cat([h, sentiments, subjectivities], dim=1)\n",
        "    #(batch size, out channels + 2)\n",
        "\n",
        "    \n",
        "    return h\n",
        "\n",
        "def get_sentiment(text):\n",
        "    \"\"\"Gets sentiment and subjectivity of text\"\"\"\n",
        "    blob = TextBlob(text)\n",
        "    sentiment = blob.sentiment.polarity\n",
        "    subjectivity = blob.sentiment.subjectivity\n",
        "    return sentiment, subjectivity\n",
        "\n",
        "#create vectorized implementation for speed\n",
        "get_sentiment_v = np.vectorize(get_sentiment, otypes = [\"float\", \"float\"], doc= \"vectorized version of get_sentiment()\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9gUDCLc_A7uR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Candidate Models"
      ]
    },
    {
      "metadata": {
        "id": "P8IJVE3DePL_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We had a series of candidate NN classifiers which are detailed here - see the report for more details. \n",
        "\n",
        "For the final classifier, skip to the \"Final Model\" subsection."
      ]
    },
    {
      "metadata": {
        "id": "6-FK3OFvGMhw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#embedding (lookup layer) layer\n",
        "class BidirectionalGRU(nn.Module):\n",
        "    \"\"\"Classifier consisting of Bidirectional GRU (i.e. RNN with memory) and \n",
        "    followed by two fully connected layers.\n",
        "    \n",
        "    NOTE: this model was not used in any of the subtasks (see the report for more detail)\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab, embedding_dim, window_size, out_channels = 128, \n",
        "                 dropout=0.5, n_hidden = 64, num_classes=2, batch_size = BATCH_SIZE, \n",
        "                 sentiment= False, ids=None):\n",
        "        \n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "        \n",
        "        self.sentiment = sentiment\n",
        "        if sentiment == True:\n",
        "            added_features = 2 #This will update number of fully connected neurons\n",
        "        else:\n",
        "            added_features = 0\n",
        "            \n",
        "            \n",
        "        self.n_hidden = n_hidden\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.num_layers = 1 #number of GRU layers\n",
        "        \n",
        "        self.vocab = vocab\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
        "        self.embedding.weight.data.copy_(vocab.vectors) # copies pre-trained word vectors\n",
        "        \n",
        "        self.bi_gru =  torch.nn.GRU(input_size=embedding_dim, hidden_size=n_hidden, \n",
        "                                    num_layers= self.num_layers, batch_first=False, \n",
        "                                    bidirectional=True)\n",
        "        self.hidden = self.init_hidden(self.batch_size)\n",
        "    \n",
        "        #Fully connected layer will convert GRU output into a label\n",
        "        # number of input features is 2 * n_hidden since GRU is bidirectional\n",
        "        \n",
        "        self.fc1 = nn.Linear( 2 * n_hidden + added_features, 16)\n",
        "        self.fc2 = nn.Linear(16, 1 if num_classes == 2 else num_classes)\n",
        "\n",
        "        #Activation: use leaky relu\n",
        "        self.lReLU = nn.LeakyReLU(negative_slope=0.05, inplace=False)\n",
        "        \n",
        "        #Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        #Kaming normalization\n",
        "        nn.init.kaiming_normal_(self.fc1.weight)\n",
        "        nn.init.kaiming_normal_(self.fc2.weight)\n",
        "\n",
        "    def init_hidden(self, batch_size = BATCH_SIZE):\n",
        "        return torch.zeros((self.num_layers * 2, batch_size, self.n_hidden), device=device)\n",
        "        \n",
        "        \n",
        "    def forward(self, x, ids=None):\n",
        "        embedded = self.embedding(x)\n",
        "        #(batch size, max sent length, embedding dim)\n",
        "        embedded = self.embedding(x).view((embedded.shape[1], embedded.shape[0], -1))\n",
        "        #(max sent length, batch size, embedding dim)\n",
        "        \n",
        "        bi_output, self.hidden = self.bi_gru(embedded, self.hidden)\n",
        "        \n",
        "        # add sentiment?\n",
        "        \n",
        "        #Just take final value of bi_output:\n",
        "        h = self.lReLU(bi_output[-1])\n",
        "        \n",
        "        if self.sentiment: #then add sentiment\n",
        "            h = add_sentiment(h, ids)\n",
        "            \n",
        "        h = self.fc1( self.dropout(h))\n",
        "        \n",
        "        h = self.fc2( self.dropout(h))\n",
        "        \n",
        "        #h = self.fc2(self.dropout(h))\n",
        "        return h\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GlyEWT07M6j7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#embedding (lookup layer) layer\n",
        "class OriginalClassifierGloVe(nn.Module):\n",
        "    \"\"\"Glove Embeddings w. 2d convolution.\n",
        "    \n",
        "    NOTE: This model was used for subtask C but a more complex version was \n",
        "    used for subtasks A and B\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab, embedding_dim, window_size, out_channels, dropout, num_classes=2):\n",
        "        \n",
        "        super(OriginalClassifierGloVe, self).__init__()\n",
        "        \n",
        "        \n",
        "        self.vocab = vocab\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
        "        self.embedding.weight.data.copy_(vocab.vectors) # copies pre-trained word vectors\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(window_size, embedding_dim))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc = nn.Linear(out_channels, 1 if num_classes == 2 else num_classes)\n",
        "        \n",
        "        self.lReLU = nn.LeakyReLU(negative_slope=0.05, inplace=False)\n",
        "        \n",
        "        #Kaming normalization\n",
        "        nn.init.kaiming_normal_(self.conv.weight)\n",
        "        nn.init.kaiming_normal_(self.fc.weight)\n",
        "\n",
        "        \n",
        "        \n",
        "    def forward(self, x, ids=None):\n",
        "        \n",
        "        embedded = self.embedding(x)\n",
        "                \n",
        "        #(batch size, max sent length, embedding dim)\n",
        "        \n",
        "        #images have 3 RGB channels \n",
        "        #for the text we add 1 channel\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        \n",
        "        #(batch size, 1, max sent length, embedding dim)\n",
        "        \n",
        "        feature_maps =  self.lReLU(self.conv(embedded).squeeze(3))\n",
        "        # (batch size, out_channels, max sent length - window size +1, 1)\n",
        "        # -> (batch size, out_channels, max sent length - window size +1)\n",
        "           \n",
        "        #the max pooling layer\n",
        "        pooled = F.max_pool1d(feature_maps, feature_maps.shape[2]).squeeze(2)\n",
        "        # (batch size, out_channels)      \n",
        "        \n",
        "        # Do batch normalize pooled then at sentiment\n",
        "        \n",
        "        return self.fc(self.dropout(pooled))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PeisH53s6cfR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#embedding (lookup layer) layer\n",
        "class SimpleClassifierGloVe(nn.Module):\n",
        "    \"\"\"Iteration on OriginalClassifierGloVe(). This has a single convolution \n",
        "    and two FC layers for classification. \n",
        "    \n",
        "    It also allows the ability to concatenate the tweet sentiment to the input to the \n",
        "    FC layers. Other than this, it is identical to OriginalClassifierGloVe\n",
        "    \n",
        "    NOTE: This model was one of the two candidates in the Bayesian optimization below \n",
        "    (although it was not used for any of the subtasks).\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab, embedding_dim, window_size, out_channels, dropout, \n",
        "                 num_classes=2, sentiment=False, n_hidden = 64):\n",
        "        \n",
        "        super(SimpleClassifierGloVe, self).__init__()\n",
        "        self.sentiment = sentiment\n",
        "        if sentiment == True:\n",
        "            added_features = 2\n",
        "        else:\n",
        "            added_features = 0\n",
        "            \n",
        "        self.vocab = vocab\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
        "        self.embedding.weight.data.copy_(vocab.vectors) # copies pre-trained word vectors\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(window_size, embedding_dim))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.fc1 = nn.Linear(out_channels + added_features, n_hidden)\n",
        "        self.fc2 = nn.Linear(n_hidden, 1 if num_classes == 2 else num_classes)\n",
        "        \n",
        "        \n",
        "        self.lReLU = nn.LeakyReLU(negative_slope=0.05, inplace=False)\n",
        "        \n",
        "        #Kaming normalization\n",
        "        nn.init.kaiming_normal_(self.conv.weight)\n",
        "        nn.init.kaiming_normal_(self.fc1.weight)\n",
        "        nn.init.kaiming_normal_(self.fc2.weight)\n",
        "        \n",
        "        \n",
        "    def forward(self, x, ids = None):\n",
        "        \n",
        "        embedded = self.embedding(x)\n",
        "            \n",
        "        #(batch size, max sent length, embedding dim)\n",
        "        \n",
        "        \n",
        "        #images have 3 RGB channels \n",
        "        #for the text we add 1 channel\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        #(batch size, 1, max sent length, embedding dim)\n",
        "        \n",
        "        feature_maps =  self.lReLU(self.conv(embedded).squeeze(3))\n",
        "        # (batch size, out_channels, max sent length - window size +1, 1)\n",
        "        # -> (batch size, out_channels, max sent length - window size +1)\n",
        "           \n",
        "        #the max pooling layer\n",
        "        h = F.max_pool1d(feature_maps, feature_maps.shape[2]).squeeze(2)\n",
        "        # (batch size, out_channels) \n",
        "        \n",
        "        if self.sentiment: #then add sentiment\n",
        "            h = add_sentiment(h, ids)\n",
        "            \n",
        "        h = self.lReLU(self.fc1( self.dropout(h)))\n",
        "        \n",
        "        h = self.fc2(self.dropout(h))\n",
        "       \n",
        "        \n",
        "        return h\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sFC6hi13DGnK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ClassifierGloVeDeep(nn.Module):\n",
        "    \"\"\"Alteration to SimpleClassifierGloVe().\n",
        "    This has a single 2d convolution and 5 fully connected layers rather than two\n",
        "    \n",
        "    Other than this, it is identical to SimpleClassifierGloVe()\n",
        "    \n",
        "    NOTE: This model was one of the two candidates in the Bayesian optimization below \n",
        "    (although it was not used for any of the subtasks).\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab, embedding_dim, window_size, out_channels = 128, \n",
        "                 dropout=0.5, n_hidden = 64, num_classes=2, sentiment=False):\n",
        "        \n",
        "        super(ClassifierGloVeDeep, self).__init__()\n",
        "        self.sentiment = sentiment\n",
        "        if sentiment == True:\n",
        "            added_features = 2 #This will update number of fully connected neurons\n",
        "        else:\n",
        "            added_features = 0\n",
        "            \n",
        "        self.vocab = vocab\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
        "        self.embedding.weight.data.copy_(vocab.vectors) # copies pre-trained word vectors\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(window_size, embedding_dim))\n",
        "        \n",
        "        self.fc1 = nn.Linear(out_channels + added_features, n_hidden[0])\n",
        "        self.fc2 = nn.Linear(n_hidden[0], n_hidden[1])\n",
        "        self.fc3 = nn.Linear(n_hidden[1], n_hidden[2])\n",
        "        self.fc4 = nn.Linear(n_hidden[2], n_hidden[3])\n",
        "        self.fc5 = nn.Linear(n_hidden[3], n_hidden[4])\n",
        "        self.fc6 = nn.Linear(n_hidden[4], 1 if num_classes == 2 else num_classes)\n",
        "        \n",
        "        #Activation: use leaky relu\n",
        "        self.lReLU = nn.LeakyReLU(negative_slope=0.05, inplace=False)\n",
        "        \n",
        "        #Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        #Kaming normalization\n",
        "        nn.init.kaiming_normal_(self.conv1.weight)\n",
        "        nn.init.kaiming_normal_(self.fc1.weight)\n",
        "        nn.init.kaiming_normal_(self.fc2.weight)\n",
        "        \n",
        "        \n",
        "    def forward(self, x, ids=None):\n",
        "        \n",
        "        embedded = self.embedding(x)\n",
        "                \n",
        "       \n",
        "        embedded = embedded.unsqueeze(1)\n",
        "       \n",
        "        \n",
        "        feature_maps =  self.lReLU(self.conv1(embedded).squeeze(3))\n",
        "        \n",
        "       \n",
        "        h = F.max_pool1d(feature_maps, feature_maps.shape[2]).squeeze(2)\n",
        "        \n",
        "        if self.sentiment: #then add sentiment\n",
        "            h = add_sentiment(h, ids)\n",
        "        \n",
        "        h = self.lReLU(self.fc1( self.dropout(h)))\n",
        "        h = self.lReLU(self.fc2( self.dropout(h)))\n",
        "        h = self.lReLU(self.fc3( self.dropout(h)))\n",
        "        h = self.lReLU(self.fc4( self.dropout(h)))\n",
        "        h = self.lReLU(self.fc5( self.dropout(h)))\n",
        "        h = self.fc6( self.dropout(h))\n",
        "        \n",
        "        \n",
        "        return h\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4_shpcBQjuU4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "k4UHz12y6L7m",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Helper functions to run training routine, calculate metrics etc.\n",
        "def check_accuracy(task_header, loader, model, conf=False, RNN=False, verbose = True, ret_optim_metric=False):\n",
        "    \"\"\"\n",
        "    Note at the moment this function assumes the batch size is equal to the \n",
        "    number of data in the loader when calculating the confusion matrix\n",
        "    \"\"\"\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    bayesian_metric = None\n",
        "    model.eval()  # set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(loader):\n",
        "            x, y = batch.tweet, getattr(batch, task_header)\n",
        "            y = y.view(-1, 1)\n",
        "                \n",
        "            x = x.to(device=device, dtype=torch.long)  # move to  GPU\n",
        "            y = y.to(device=device, dtype=torch.long)\n",
        "            \n",
        "            if RNN:\n",
        "                #Must zero all of the accumalated hidden state for the RNN\n",
        "                model.hidden = model.init_hidden(batch.batch_size)\n",
        "                \n",
        "            logits = model(x, ids = batch.id)\n",
        "            if task_header == 'subtask_c':\n",
        "                pred_prob = F.softmax(logits, dim=1)\n",
        "                pred_1 = torch.argmax(pred_prob, dim=1).view(-1, 1)\n",
        "            else:\n",
        "                pred_prob = torch.sigmoid(logits)\n",
        "                pred_1 = (pred_prob > 0.5).type(torch.long)\n",
        "              \n",
        "            num_correct += (pred_1 == y).sum()\n",
        "            num_samples += pred_prob.size(0)\n",
        "            \n",
        "            # move to CPU to prevent memory overflow and calculate metrics\n",
        "            x = x.to(device=\"cpu\", dtype=torch.long)\n",
        "            y = y.to(device=\"cpu\", dtype=torch.long).numpy()\n",
        "            pred_1 = pred_1.to(device=\"cpu\", dtype=torch.long).numpy()\n",
        "            \n",
        "            \n",
        "        acc = float(num_correct) / num_samples\n",
        "        if conf:\n",
        "            confusion = metrics.confusion_matrix(y, pred_1)\n",
        "            clas_rep = metrics.classification_report(y, pred_1, output_dict =ret_optim_metric)\n",
        "            kappa = \"Kappa: {:.4f}\".format(metrics.cohen_kappa_score(y, pred_1))\n",
        "            if ret_optim_metric:\n",
        "                bayesian_metric = optim_metric(clas_rep)\n",
        "        else:\n",
        "            total_metric = None\n",
        "        if verbose:\n",
        "            print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
        "            print(confusion)\n",
        "            print(clas_rep)\n",
        "            print(kappa)\n",
        "    return bayesian_metric\n",
        "            \n",
        "def optim_metric(clas_rep):\n",
        "    \"\"\"calculate Bayesian Optimization metric\"\"\"\n",
        "    f1_1 = clas_rep['0'][\"f1-score\"]\n",
        "    f1_2 = clas_rep['1'][\"f1-score\"]\n",
        "    total = np.sqrt(f1_1 * f1_2)\n",
        "    return total\n",
        "\n",
        "def check_loss(task_header, loader, model, loss_fn, RNN=False):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        loss = 0\n",
        "        for idx, batch in enumerate(loader):\n",
        "            x, y = batch.tweet, getattr(batch, task_header)\n",
        "\n",
        "            x = x.to(device=device, dtype=torch.long) \n",
        "            y = y.to(device=device, dtype=torch.long if task_header == 'subtask_c' else torch.float)\n",
        "            \n",
        "            if RNN:\n",
        "                #Must zero all of the accumalated hidden state for the RNN\n",
        "                model.hidden = model.init_hidden(batch.batch_size)\n",
        "                \n",
        "            logits = model(x, ids= batch.id)\n",
        "\n",
        "            loss += loss_fn(logits, y.view(-1,) if isinstance(loss_fn, nn.CrossEntropyLoss) else y.view(-1, 1))\n",
        "\n",
        "    return loss/len(loader)\n",
        "      \n",
        "\n",
        "def train_helper(task_header, model, optimizer, train_loader, \n",
        "               valid_loader, epochs=1, RNN = False, loss_fn=F.binary_cross_entropy_with_logits, \n",
        "                 print_every=50, verbose = True, ret_optim_metric=False):\n",
        "    \"\"\"\n",
        "    Train a model\n",
        "    \n",
        "    Inputs:\n",
        "    - model: A PyTorch Module giving the model to train.\n",
        "    - optimizer: An Optimizer object we will use to train the model\n",
        "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
        "    \n",
        "    Returns: Nothing, but prints model accuracies during training.\n",
        "    \"\"\"\n",
        "    #sets seeds to make results reproducible\n",
        "    set_seed()\n",
        "    \n",
        "    model = model.to(device=device)  # move the model parameters to GPU\n",
        "    \n",
        "    training_losses = []\n",
        "    validation_losses = []\n",
        "    optimizing_metric = []\n",
        "    try:\n",
        "        for epoch in range(epochs):\n",
        "            if verbose:\n",
        "                print(\"Epoch:\", epoch)\n",
        "            total_loss = 0\n",
        "            for batch_idx, batch in enumerate(train_loader):\n",
        "\n",
        "                model.train()  # put model to training mode\n",
        "                optimizer.zero_grad()\n",
        "                inputs, targets = batch.tweet, getattr(batch, task_header)\n",
        "                \n",
        "                if RNN:\n",
        "                    #Must zero all of the accumulated hidden states for the GRU\n",
        "                    model.hidden = model.init_hidden(batch.batch_size)\n",
        "                \n",
        "                \n",
        "                x = inputs.to(device=device, dtype=torch.long)  # move to device, e.g. GPU\n",
        "                y = targets.to(device=device, dtype=torch.long if task_header == 'subtask_c' else torch.float) #this should be a float cross entropy\n",
        "                #x = inputs\n",
        "                #y = targets\n",
        "                logits = model(x, ids = batch.id)\n",
        "                \n",
        "                # When using cross_entropy the targets need to have a shape (N,)\n",
        "                # However, for BCEWithLogits they just need\n",
        "                # to have the same shape as the logits\n",
        "                loss = loss_fn(logits, y.view(-1,) if isinstance(loss_fn, nn.CrossEntropyLoss) else y.view(-1, 1))\n",
        "                # Zero out all of the gradients for the variables which the optimizer\n",
        "                # will update.\n",
        "                \n",
        "\n",
        "                # This is the backwards pass: compute the gradient of the loss with\n",
        "                # respect to each  parameter of the model.\n",
        "                loss.backward()\n",
        "\n",
        "                # Actually update the parameters of the model using the gradients\n",
        "                # computed by the backwards pass.\n",
        "                optimizer.step()\n",
        "\n",
        "                x = x.to(device=\"cpu\", dtype=torch.long)  # move to CPU to prevent memory overflow\n",
        "                y = y.to(device=\"cpu\", dtype=torch.long)\n",
        "\n",
        "                total_loss += loss.detach().item()\n",
        "                \n",
        "                if batch_idx % print_every == 0 and verbose:\n",
        "                    print('Iteration %d, loss = %.4f' % (batch_idx, loss.item()))\n",
        "            \n",
        "            training_losses.append(total_loss/len(train_loader))\n",
        "            if verbose:\n",
        "                print()\n",
        "                print(\"Validation Accuracy:\")\n",
        "            optim_metric = check_accuracy(task_header, valid_loader, model, RNN=RNN, \n",
        "                                          conf=True, verbose=verbose, ret_optim_metric=ret_optim_metric)\n",
        "            optimizing_metric.append(optim_metric)\n",
        "            \n",
        "            valid_loss = check_loss(task_header, valid_loader, model, loss_fn, RNN)\n",
        "            validation_losses.append(valid_loss)\n",
        "        if ret_optim_metric:\n",
        "            return training_losses, validation_losses, optimizing_metric\n",
        "        else:\n",
        "            return training_losses, validation_losses,\n",
        "    except Exception as e:\n",
        "        #Attempt to prevent GPU memory overflow by transferring model back to cpu\n",
        "        #model = model.to(device=\"cpu\")\n",
        "        raise e    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fDt0pRgyHgxw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tuning\n",
        "There are a huge number of permutations of hyperparameters (particularly to do with preprocessing). Hence it will be useful to do a hyperparameter search for the best values. These are:\n",
        " - Lemmatize words $\\in \\{True, False\\}$\n",
        " - Remove punctuation $\\in \\{True, False\\}$\n",
        " - Remove stopwords $\\in \\{True, False\\}$\n",
        " - Add sentiment (and subjectivity) $\\in\\{True, False\\}$\n",
        " - Type of model $\\in \\{$simple_CNN, Deep_CNN, Bidirectional_GRU$\\}$\n",
        " - Model parameters (number of neurons per layers etc)\n",
        " - Window size (in CNN only) - i.e. size of kernel. \n",
        " - Learning rate $\\in [0.0001, 0.0025]$\n",
        " - weight_decay $\\in [0.0, 0.1]$\n",
        " - Use dropout $\\in \\{True, False\\}$ (since dropout rate =0.5 is recommended when dropout is used)\n",
        " \n",
        "\n",
        "We will conduct Bayesian optimization on these parameters. \n",
        "### Choice of optimization metric\n",
        "We want to avoid the model predicting all True or all False so will maximize the product of the f1 scores for each class (as this will be zero if either class is never correctly predicted by the model).  To avoid complications due to different combinations of parameters affecting the speed at which the network learns, we will use a large number of epochs but take the maximum value (not including the first five epochs). In an ideal world, we would conduct a full scale Bayesian optimization for all three subtasks but due to the constraints of Colab, it will not be possible in this case. Hence we will optimize the values for subtask A and then do some basic checks for the latter parts. \n",
        " \n",
        "Note: It was clear from preliminary investigation that our implementation of the Bidirectional_GRU network was not performing well. As such, it was not included in the Bayesian Optimization. \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "7uXTKBmINouS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Bayesian optimization for hyperparameters\n",
        "\n",
        "#NOTE: This cell takes approximately 10 hours to run.\n",
        "#Uncomment the final line of this cell if you would like to run it. \n",
        "\n",
        "def hyperparam_tuning(x0=None, y0=None):\n",
        "    \"\"\"Run hyperparameter tuning routine using bayesian optimization.\n",
        "\n",
        "    The hyperparameters are as follows (in this order):\n",
        "         - Lemmatize words $\\in [True, False]$\n",
        "         - Remove punctuation $\\in [True, False]$\n",
        "         - Remove stopwords $\\in [True, False]$\n",
        "         - Add sentiment (and subjectivity) $\\in [True, False]$\n",
        "         - Type of model $\\in [$simple_CNN, Deep_CNN, Bidirectional_GRU]\n",
        "         - Learning rate $\\in [0.0001, 0.0025]$\n",
        "         - weight_decay $\\in [0.0, 0.1]$\n",
        "         - window_size $\\in [3, 5]$\n",
        "    \"\"\"\n",
        "\n",
        "    #define dimension lower/upper bound (or possible values) for each hyperparameter\n",
        "        #args are of the form:\n",
        "        #[lemma, rem_punct, rem_stopwords, sentiment, model_type, lr, weight_decay, window_size, dropout_on]\n",
        "\n",
        "    dimensions = [(True, False), (True, False), (True, False), (True, False),\n",
        "                  (\"simple_CNN\", \"Deep_CNN\"), (0.0001, 0.0015), (0.0, 0.1), \n",
        "                  (2, 3, 4, 5), (True, False)]\n",
        "                  \n",
        "\n",
        "    #It will check the initial points above and then use bayesian optimization\n",
        "    #to choose the next points to evaluate\n",
        "    #first define a function to minimize:\n",
        "    res = gp_minimize(fn_optim, dimensions, n_calls=20, n_random_starts=0,\n",
        "                acq_func='gp_hedge', x0=x0, y0=y0, \n",
        "                random_state=SEED, verbose=True, callback=None, n_points=1000,\n",
        "                n_restarts_optimizer=5, xi=0.01, kappa=1.96,\n",
        "                noise='gaussian')\n",
        "    print(\"Bayesian Optimization Results:\")\n",
        "    print(res)\n",
        "    return res\n",
        "\n",
        "#Define function which will call the training cycle with each set of parameters\n",
        "def fn_optim(args, epochs=25, verbose=False):\n",
        "    \"\"\"Helper function to run the hyperparameter optimisation. It takes the\n",
        "    hyperparameters as args and must return the quantity to be minimized\n",
        "    \"\"\"\n",
        "    [lemma, rem_punct, rem_stopwords, sentiment, model_type, lr, weight_decay, window_size, dropout_on] = args\n",
        "\n",
        "    out_channels = 128 #keep these fixed\n",
        "    embedding_dim = 200 #for glove\n",
        "    \n",
        "    if dropout_on == True:\n",
        "        dropout_rate = 0.5\n",
        "    else:\n",
        "        dropout_rate = 0\n",
        "\n",
        "    TOKENIZE_PARAMS_LCL = {\"lemmatize\": lemma, \n",
        "                       \"rem_punct\": rem_punct, #remove punctuation\n",
        "                       \"rem_stopwords\": rem_stopwords}\n",
        "    \n",
        "    #Initialize tokenizer function with these parameters \n",
        "    def tokenizer_local(text):\n",
        "        \"\"\"wrapper for tokenize so it can be used in torchtext Field creation. It takes the \n",
        "        current global varaiable TOKENIZE_PARAMS\"\"\"\n",
        "        try:\n",
        "            params = TOKENIZE_PARAMS_LCL\n",
        "        except NameError:\n",
        "            print(\"You must initialize the global variable TOKENIZE_PARAMS\")\n",
        "            raise NameError\n",
        "        return tokenize_params(text, params)\n",
        "    \n",
        "    #Initialize vocab with these parameters:\n",
        "\n",
        "    TEXT = data.Field(sequential=True, tokenize=tokenizer_local, lower=True, batch_first = True)\n",
        "    LABEL = data.LabelField(sequential=False, use_vocab=True, batch_first = True)\n",
        "    ID = data.LabelField(sequential=False, use_vocab=False, batch_first=True)\n",
        "\n",
        "    data_fields = [('id', ID), \n",
        "                   ('tweet', TEXT),\n",
        "                   ('subtask_a',LABEL),\n",
        "                   ('subtask_b',LABEL),\n",
        "                   ('subtask_c',LABEL)]\n",
        "\n",
        "    set_seed()\n",
        "    train = data.TabularDataset(train_fp, format='TSV', fields = \n",
        "                                data_fields, skip_header=True, filter_pred=None)\n",
        "\n",
        "    train, valid = train.split(split_ratio=0.8, random_state=random.seed(SEED))\n",
        "\n",
        "    #Now build vocab (using only the training set)\n",
        "    TEXT.build_vocab(train, vectors='glove.twitter.27B.200d') #USE \"glove.840B.300d\" or glove.twitter.27B.200d\n",
        "\n",
        "    LABEL.build_vocab(train.subtask_a)\n",
        "\n",
        "    output_dim = len(LABEL.vocab)\n",
        "\n",
        "    #Create iterators\n",
        "    train_iterator, valid_iterator = data.BucketIterator.splits((train, valid),\n",
        "                            batch_sizes=(BATCH_SIZE, len(valid)),  \n",
        "                            sort_key=lambda x: len(x.tweet), device=device)\n",
        "\n",
        "    # For retrieving tweet text later on\n",
        "    train_df = pd.read_csv(train_fp, delimiter=\"\\t\")\n",
        "\n",
        "    #define model\n",
        "    if model_type == \"simple_CNN\":\n",
        "        n_hidden = 64\n",
        "        model = SimpleClassifierGloVe(TEXT.vocab, embedding_dim, window_size, out_channels, \n",
        "                          dropout=True, sentiment = sentiment)\n",
        "    elif model_type == \"Deep_CNN\":\n",
        "        n_hidden = (64, 32, 16, 8, 4)\n",
        "        model = ClassifierGloVeDeep(TEXT.vocab, embedding_dim, window_size, out_channels, \n",
        "             dropout=dropout_rate, n_hidden = n_hidden, sentiment=sentiment)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid NN architecture. n_hidden must be either \\\"funnel\\\" or \\\"diamond\\\".\")\n",
        "\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr = lr, weight_decay = weight_decay)\n",
        "\n",
        "    pos_weight = torch.tensor([2.], device = device) #deals with unbalanced classes\n",
        "\n",
        "    loss_fn = nn.BCEWithLogitsLoss(pos_weight = pos_weight)\n",
        "\n",
        "    #train model\n",
        "    _, v_losses, optim_metric = train_helper('subtask_a', model, optimizer, loss_fn = loss_fn, \n",
        "                                      epochs = 25, train_loader=train_iterator, \n",
        "                                      valid_loader=valid_iterator, ret_optim_metric=True, verbose=verbose)\n",
        "\n",
        "    max_val = max(optim_metric[4:])\n",
        "\n",
        "    return - max_val #return negative value as this will minimize this\n",
        "\n",
        "#Uncomment the line below to run bayesian optimization routine\n",
        "#hyperparam_tuning()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9ryiD3YS5rYa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#The routine suggested the following hyperparameters are best (on subtask a): \n",
        "\n",
        "            #[lemma, rem_punct, rem_stopwords, sentiment, model_type,                \n",
        "best_args = [True,       True,       True,        True,    'Deep_CNN',\n",
        "             #           lr,           weight_decay,       window_size, dropout_on]\n",
        "             0.0001656545740852317, 0.0054326444080709255,        5,       False] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EXxnttXR5TUN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter intermediate conclusions \n",
        "\n",
        "* The Bayesian optimization suggested that, for our limited range of architectures, all the preprocessing steps we performed (lemmatization, removal of punctuation and removal of stopwords) had a positive impact on performance.\n",
        "* The addition of sentiment aided performance as expected. \n",
        "* The use of 7 fully connected layers was superior to 2 layers (suggesting overfitting was not a huge issue here).\n",
        "* We found that weight_decay in the optimizer was preferred to dropout as a method of regularization.\n",
        "* A window size of 5 tokens in the convolution was preferred. This is particularly interesting as it was the largest value we tried. This suggests we may have been searching in the wrong region and optimal window size is larger than five. This suggests that the model prefers to have a larger amount of context available when considering the importance of each individual token. We conducted another search on this hyperparameter with the other hyperparameters fixed. "
      ]
    },
    {
      "metadata": {
        "id": "yipQs8Gn_bTC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Window size line search"
      ]
    },
    {
      "metadata": {
        "id": "VgDEwQxh8JMj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Quick line search for best window size (keeping other best_args) fixed\n",
        "# Hack to code this quickly:\n",
        "    #We use the fn_optim() function and just pass a different value of window size\n",
        "    #at index 7 of args:\n",
        "    \n",
        "          #[lemma, rem_punct, rem_stopwords, sentiment, model_type, lr, weight_decay, window_size, dropout_on]\n",
        "best_args = [True, True, True, True, 'Deep_CNN', 0.0001656545740852317, 0.0054326444080709255, 5, False]\n",
        "\n",
        "w_sizes = [5, 6, 7, 9, 12, 20, 30, 50]\n",
        "\n",
        "for size in w_sizes:\n",
        "    print(\"For window size:\", size)\n",
        "    args = best_args\n",
        "    args[7] = size\n",
        "    optim_metric_val = - fn_optim(args, epochs=25, verbose=False)\n",
        "    print(\"Square root of product of F1-scores: \", optim_metric_val)\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9BkG64b0aFqb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The above cell had the following output when run:\n",
        "\n",
        "        For window size: 5\n",
        "        Square root of product of F1-scores:  0.743731033087325\n",
        "\n",
        "        For window size: 6\n",
        "        Square root of product of F1-scores:  0.7306656271243175\n",
        "\n",
        "        For window size: 7\n",
        "        Square root of product of F1-scores:  0.7322754673368059\n",
        "\n",
        "        For window size: 9\n",
        "        Square root of product of F1-scores:  0.7302783836834617\n",
        "\n",
        "        For window size: 12\n",
        "        Square root of product of F1-scores:  0.7369161985199361\n",
        "\n",
        "        For window size: 20\n",
        "        Square root of product of F1-scores:  0.7320976881905901\n",
        "\n",
        "        For window size: 30\n",
        "        Square root of product of F1-scores:  0.733712883624817\n",
        "\n",
        "        For window size: 50 \n",
        "        [Long cuda GPU memory error]"
      ]
    },
    {
      "metadata": {
        "id": "PG0Qy_gVC-EJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This analysis suggests that window_size = 5 was the optimal value (though the distribution is relatively flat). "
      ]
    },
    {
      "metadata": {
        "id": "Z81BR6tqSqxG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Final Model"
      ]
    },
    {
      "metadata": {
        "id": "R6Z0lOV2kzPf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In imaging, is often better to have a series of smaller convolutions than a single large convolution as this reduces the number of parameters and, in theory, allows the learning of more abstract features. In our case, an single CNN layer with a window size of 5 will have a similar effect to 2 convolutional layers with window size 3. Therefore, we created a final candidate model: a 2C-5FC and compared this to our 1C-5FC. We found that the 2C-5FC outperformed the 1C-5FC and this became our final model. In an ideal world, we would optimize hyperparameters for this new network but in practice, we did not have sufficient time to do this so simply used the optimal values for its predecessor. "
      ]
    },
    {
      "metadata": {
        "id": "N5FsTsPpBI7H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Final candidate model:\n",
        "#embedding (lookup layer) layer\n",
        "class ClassifierGloVeDeepMultiConv(nn.Module):\n",
        "    \"\"\"Glove w. two convolutional layers and 5 fc layers.\n",
        "    \n",
        "    NOTE: this was used on subtasks A and B.\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab, embedding_dim, window_size, n_hidden = (64, 32, 16, 8, 4), out_channels = (128, 256), #out_channels must now be an iterable\n",
        "                 dropout=0.5,  num_classes=2, sentiment=False):\n",
        "        \n",
        "        super(ClassifierGloVeDeepMultiConv, self).__init__()\n",
        "        self.sentiment = sentiment\n",
        "        \n",
        "        if sentiment == True:\n",
        "            added_features = 2 #This will update number of fully connected neurons\n",
        "        else:\n",
        "            added_features = 0\n",
        "            \n",
        "        self.vocab = vocab\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
        "        self.embedding.weight.data.copy_(vocab.vectors) # copies pre-trained word vectors\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=out_channels[0], kernel_size=(window_size, embedding_dim))\n",
        "        self.conv2 = nn.Conv2d(in_channels=out_channels[0], out_channels=out_channels[1], kernel_size=(window_size, 1))\n",
        "        \n",
        "        self.fc1 = nn.Linear(out_channels[1] + added_features, n_hidden[0])\n",
        "        self.fc2 = nn.Linear(n_hidden[0], n_hidden[1])\n",
        "        self.fc3 = nn.Linear(n_hidden[1], n_hidden[2])\n",
        "        self.fc4 = nn.Linear(n_hidden[2], n_hidden[3])\n",
        "        self.fc5 = nn.Linear(n_hidden[3], n_hidden[4])\n",
        "        self.fc6 = nn.Linear(n_hidden[4], 1 if num_classes == 2 else num_classes)\n",
        "        \n",
        "        #Activation: use leaky relu\n",
        "        self.lReLU = nn.LeakyReLU(negative_slope=0.05, inplace=False)\n",
        "        \n",
        "        #Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        #Kaming normalization\n",
        "        nn.init.kaiming_normal_(self.conv1.weight)\n",
        "        nn.init.kaiming_normal_(self.conv2.weight)\n",
        "        nn.init.kaiming_normal_(self.fc1.weight)\n",
        "        nn.init.kaiming_normal_(self.fc2.weight)\n",
        "        nn.init.kaiming_normal_(self.fc3.weight)\n",
        "        nn.init.kaiming_normal_(self.fc4.weight)\n",
        "        nn.init.kaiming_normal_(self.fc5.weight)\n",
        "        nn.init.kaiming_normal_(self.fc6.weight)\n",
        "\n",
        "        \n",
        "    def forward(self, x, ids=None):\n",
        "        \n",
        "        embedded = self.embedding(x)\n",
        "                \n",
        "       \n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        \n",
        "        h = self.lReLU(self.conv1(embedded))\n",
        "        h = self.lReLU(self.conv2(h).squeeze(3))\n",
        "        h = F.max_pool1d(h, h.shape[2]).squeeze(2)\n",
        "\n",
        "        if self.sentiment: #then add sentiment\n",
        "            h = add_sentiment(h, ids)\n",
        "        \n",
        "        h = self.lReLU(self.fc1( self.dropout(h)))\n",
        "        h = self.lReLU(self.fc2( self.dropout(h)))\n",
        "        h = self.lReLU(self.fc3( self.dropout(h)))\n",
        "        h = self.lReLU(self.fc4( self.dropout(h)))\n",
        "        h = self.lReLU(self.fc5( self.dropout(h)))\n",
        "        h = self.fc6( self.dropout(h))\n",
        "        \n",
        "        \n",
        "        return h\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CJWlrUsNS2JQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part A"
      ]
    },
    {
      "metadata": {
        "id": "Yr85FFhPbkgX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This section and the two following it, show the training of our models with correctly tuned hyperparameters. Running these cells should create models with similar performance to our submitted results. "
      ]
    },
    {
      "metadata": {
        "id": "0HuMlNzvLPPw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TOKENIZE_PARAMS_new = {\"lemmatize\": True, \n",
        "                   \"rem_punct\": True, #remove punctuation\n",
        "                   \"rem_stopwords\": True}\n",
        "\n",
        "#Initialize tokenizer function with these parameters - unfortuantely we need a \n",
        "#global variable here as a result of the way as our TorchText tokenizer can only\n",
        "#take one argument. \n",
        "\n",
        "def tokenizer_local(text):\n",
        "    \"\"\"wrapper for tokenize so it can be used in torchtext Field creation. It takes the \n",
        "    current global varaiable TOKENIZE_PARAMS_LCL\"\"\"\n",
        "    try:\n",
        "        params = TOKENIZE_PARAMS_new\n",
        "    except NameError:\n",
        "        print(\"You must initialize the global variable TOKENIZE_PARAMS_LCL\")\n",
        "        raise NameError\n",
        "    return tokenize_params(text, params)\n",
        "\n",
        "#Initialize vocab with these parameters:\n",
        "TEXT = data.Field(sequential=True, tokenize=tokenizer_local, lower=True, batch_first = True)\n",
        "LABEL = data.LabelField(sequential=False, use_vocab=True, batch_first = True)\n",
        "ID = data.LabelField(sequential=False, use_vocab=False, batch_first=True)\n",
        "\n",
        "data_fields = [('id', ID), \n",
        "               ('tweet', TEXT),\n",
        "               ('subtask_a',LABEL),\n",
        "               ('subtask_b',LABEL),\n",
        "               ('subtask_c',LABEL)]\n",
        "\n",
        "train = data.TabularDataset(train_fp, format='TSV', fields = \n",
        "                            data_fields, skip_header=True, filter_pred=None)\n",
        "\n",
        "set_seed() #set seed to make valid-train split deterministic\n",
        "\n",
        "train, valid = train.split(split_ratio=0.8, random_state=random.seed(SEED))\n",
        "\n",
        "#Now build vocab (using only the training set)\n",
        "TEXT.build_vocab(train, vectors='glove.twitter.27B.200d') #USE \"glove.840B.300d\" or glove.twitter.27B.200d\n",
        "\n",
        "LABEL.build_vocab(train.subtask_a)\n",
        "\n",
        "#Create iterators\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid),\n",
        "                        batch_sizes=(BATCH_SIZE, len(valid)),  \n",
        "                        sort_key=lambda x: len(x.tweet), device=device)\n",
        "\n",
        "# For retrieving tweet text later on\n",
        "train_df = pd.read_csv(train_fp, delimiter=\"\\t\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MVRjOO2zlIfG",
        "colab_type": "code",
        "outputId": "406dbbde-deb0-4128-93ed-96a492490276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1522
        }
      },
      "cell_type": "code",
      "source": [
        "#Subtask A hyperparameters\n",
        "##########################\n",
        "embedding_dim = 200\n",
        "window_size = 3\n",
        "lr = 0.0001656545740852317\n",
        "weight_decay = 0.0054326444080709255\n",
        "out_channels = (128, 256)\n",
        "dropout = 0\n",
        "pos_weight = torch.tensor([2.], device = device) #deals with unbalanced classes\n",
        "n_hidden = (64, 32, 16, 8, 4)\n",
        "sentiment = True\n",
        "#####################\n",
        "\n",
        "set_seed()\n",
        "train_df = pd.read_csv(train_fp, delimiter=\"\\t\")\n",
        "model_A = ClassifierGloVeDeepMultiConv(TEXT.vocab, embedding_dim, window_size, \n",
        "                            out_channels=out_channels, dropout=dropout, n_hidden = n_hidden, sentiment=sentiment)\n",
        "\n",
        "optimizer = optim.Adam(model_A.parameters(), lr, weight_decay=weight_decay)\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight = pos_weight)\n",
        "\n",
        "\n",
        "t_losses, v_losses = train_helper('subtask_a', model_A, optimizer, loss_fn = loss_fn,\n",
        "                                  epochs = 5, train_loader=train_iterator, valid_loader=valid_iterator)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Iteration 0, loss = 0.9433\n",
            "Iteration 50, loss = 0.8409\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1773 / 2648 correct (66.96)\n",
            "[[1174  599]\n",
            " [ 276  599]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.66      0.73      1773\n",
            "           1       0.50      0.68      0.58       875\n",
            "\n",
            "   micro avg       0.67      0.67      0.67      2648\n",
            "   macro avg       0.65      0.67      0.65      2648\n",
            "weighted avg       0.71      0.67      0.68      2648\n",
            "\n",
            "Kappa: 0.3171\n",
            "Epoch: 1\n",
            "Iteration 0, loss = 0.8261\n",
            "Iteration 50, loss = 0.7514\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1989 / 2648 correct (75.11)\n",
            "[[1488  285]\n",
            " [ 374  501]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.84      0.82      1773\n",
            "           1       0.64      0.57      0.60       875\n",
            "\n",
            "   micro avg       0.75      0.75      0.75      2648\n",
            "   macro avg       0.72      0.71      0.71      2648\n",
            "weighted avg       0.75      0.75      0.75      2648\n",
            "\n",
            "Kappa: 0.4227\n",
            "Epoch: 2\n",
            "Iteration 0, loss = 0.7247\n",
            "Iteration 50, loss = 0.7301\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1989 / 2648 correct (75.11)\n",
            "[[1401  372]\n",
            " [ 287  588]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.79      0.81      1773\n",
            "           1       0.61      0.67      0.64       875\n",
            "\n",
            "   micro avg       0.75      0.75      0.75      2648\n",
            "   macro avg       0.72      0.73      0.73      2648\n",
            "weighted avg       0.76      0.75      0.75      2648\n",
            "\n",
            "Kappa: 0.4511\n",
            "Epoch: 3\n",
            "Iteration 0, loss = 0.6770\n",
            "Iteration 50, loss = 0.6240\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2025 / 2648 correct (76.47)\n",
            "[[1456  317]\n",
            " [ 306  569]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.82      0.82      1773\n",
            "           1       0.64      0.65      0.65       875\n",
            "\n",
            "   micro avg       0.76      0.76      0.76      2648\n",
            "   macro avg       0.73      0.74      0.73      2648\n",
            "weighted avg       0.77      0.76      0.77      2648\n",
            "\n",
            "Kappa: 0.4700\n",
            "Epoch: 4\n",
            "Iteration 0, loss = 0.5960\n",
            "Iteration 50, loss = 0.6636\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2009 / 2648 correct (75.87)\n",
            "[[1412  361]\n",
            " [ 278  597]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.80      0.82      1773\n",
            "           1       0.62      0.68      0.65       875\n",
            "\n",
            "   micro avg       0.76      0.76      0.76      2648\n",
            "   macro avg       0.73      0.74      0.73      2648\n",
            "weighted avg       0.77      0.76      0.76      2648\n",
            "\n",
            "Kappa: 0.4674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oiXbnMDeath_",
        "colab_type": "code",
        "outputId": "9138e977-47df-40f5-a670-6134bbbafc17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "fig1, ax1 = plt.subplots()\n",
        "\n",
        "ax1.plot(t_losses, label='Training')\n",
        "ax1.plot(v_losses, label='Validation')\n",
        "\n",
        "ax1.set_title('Losses')\n",
        "ax1.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFZCAYAAACizedRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Mt1eluBC8rTo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part B"
      ]
    },
    {
      "metadata": {
        "id": "YF2_sJC9mQs9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Part B is similar to A with the following changes:\n",
        "* We do not remove stopwords as we found that this decreased performance. Upon reflection, this was the expected behaviour as a numbe of stopwords such as \"you\" and \"they\" are very useful in predicting the target of the offensive language."
      ]
    },
    {
      "metadata": {
        "id": "ljbIv5fj82GA",
        "colab_type": "code",
        "outputId": "4bb2f7ad-47bc-4c84-9f94-08c44be8434e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "#Repeat for B\n",
        "TOKENIZE_PARAMS_new = {\"lemmatize\": True, \n",
        "                   \"rem_punct\": True, #remove punctuation\n",
        "                   \"rem_stopwords\": False}\n",
        "\n",
        "#Initialize tokenizer function with these parameters \n",
        "def tokenizer_local(text):\n",
        "    \"\"\"wrapper for tokenize so it can be used in torchtext Field creation. It takes the \n",
        "    current global varaiable TOKENIZE_PARAMS_LCL\"\"\"\n",
        "    try:\n",
        "        params = TOKENIZE_PARAMS_new\n",
        "    except NameError:\n",
        "        print(\"You must initialize the global variable TOKENIZE_PARAMS_LCL\")\n",
        "        raise NameError\n",
        "    return tokenize_params(text, params)\n",
        "\n",
        "#The majority of what follows is repetition from part A and isn't, strictly, necessary.\n",
        "#However, colab's namespace gets cluttered so we found it easier to re-initialize\n",
        "#everything when building the vocab for each subtask by just running this cell.\n",
        "#Poor software design for which we can only apologise...\n",
        "\n",
        "TEXT = data.Field(sequential=True, tokenize=tokenizer_local, lower=True, batch_first = True)\n",
        "LABEL = data.LabelField(sequential=False, use_vocab=True, batch_first = True)\n",
        "ID = data.LabelField(sequential=False, use_vocab=False, batch_first=True)\n",
        "\n",
        "data_fields = [('id', ID), \n",
        "               ('tweet', TEXT),\n",
        "               ('subtask_a',LABEL),\n",
        "               ('subtask_b',LABEL),\n",
        "               ('subtask_c',LABEL)]\n",
        "\n",
        "set_seed()\n",
        "\n",
        "#Select data that does not have subtask_a == \"OFF\":\n",
        "train = data.TabularDataset(train_fp, format='TSV', fields = \n",
        "                            data_fields, skip_header=True, filter_pred=lambda d: d.subtask_a == 'OFF')\n",
        "\n",
        "train, valid = train.split(split_ratio=0.8, random_state=random.seed(SEED))\n",
        "\n",
        "print(f'Train size: {len(train)}')\n",
        "print(f'Validation size: {len(valid)}')\n",
        "\n",
        "#Now build vocab (using only the training set)\n",
        "TEXT.build_vocab(train, vectors='glove.twitter.27B.200d') \n",
        "\n",
        "LABEL.build_vocab(train.subtask_b)\n",
        "\n",
        "output_dim = len(LABEL.vocab)\n",
        "\n",
        "print(LABEL.vocab.stoi)\n",
        "\n",
        "#Create iterators\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid),\n",
        "                        batch_sizes=(BATCH_SIZE, len(valid)),  \n",
        "                        sort_key=lambda x: len(x.tweet), device=device)\n",
        "\n",
        "train_df = pd.read_csv(train_fp, delimiter=\"\\t\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 3520\n",
            "Validation size: 880\n",
            "defaultdict(<function _default_unk_index at 0x7f2c4ccd2400>, {'TIN': 0, 'UNT': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9KV2Yzy28xpD",
        "colab_type": "code",
        "outputId": "f0052426-2ba5-44a1-eede-825a2fd02b66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2914
        }
      },
      "cell_type": "code",
      "source": [
        "#Conv with Glove Deep\n",
        "embedding_dim = 200\n",
        "window_size = 3\n",
        "lr = 0.0001656545740852317\n",
        "weight_decay = 0.0054326444080709255\n",
        "out_channels = (128, 256)\n",
        "dropout = 0\n",
        "pos_weight = torch.tensor([6.8], device = device) #deals with unbalanced classes\n",
        "n_hidden = (64, 32, 16, 8, 4)\n",
        "sentiment = True\n",
        "\n",
        "set_seed()\n",
        "\n",
        "model_B = ClassifierGloVeDeepMultiConv(TEXT.vocab, embedding_dim, window_size = window_size, \n",
        "                            out_channels=out_channels, dropout=dropout, n_hidden = n_hidden, sentiment=sentiment)\n",
        "\n",
        "optimizer = optim.Adam(model_B.parameters(), lr, weight_decay=weight_decay)\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight = pos_weight)\n",
        "\n",
        "\n",
        "t_losses, v_losses = train_helper('subtask_b', model_B, optimizer, loss_fn = loss_fn, epochs = 10, \n",
        "                                  train_loader=train_iterator, valid_loader=valid_iterator)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Iteration 0, loss = 1.6851\n",
            "\n",
            "Validation Accuracy:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 768 / 880 correct (87.27)\n",
            "[[768   0]\n",
            " [112   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      1.00      0.93       768\n",
            "           1       0.00      0.00      0.00       112\n",
            "\n",
            "   micro avg       0.87      0.87      0.87       880\n",
            "   macro avg       0.44      0.50      0.47       880\n",
            "weighted avg       0.76      0.87      0.81       880\n",
            "\n",
            "Kappa: 0.0000\n",
            "Epoch: 1\n",
            "Iteration 0, loss = 1.3418\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 767 / 880 correct (87.16)\n",
            "[[766   2]\n",
            " [111   1]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      1.00      0.93       768\n",
            "           1       0.33      0.01      0.02       112\n",
            "\n",
            "   micro avg       0.87      0.87      0.87       880\n",
            "   macro avg       0.60      0.50      0.47       880\n",
            "weighted avg       0.80      0.87      0.81       880\n",
            "\n",
            "Kappa: 0.0108\n",
            "Epoch: 2\n",
            "Iteration 0, loss = 1.2065\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 768 / 880 correct (87.27)\n",
            "[[766   2]\n",
            " [110   2]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      1.00      0.93       768\n",
            "           1       0.50      0.02      0.03       112\n",
            "\n",
            "   micro avg       0.87      0.87      0.87       880\n",
            "   macro avg       0.69      0.51      0.48       880\n",
            "weighted avg       0.83      0.87      0.82       880\n",
            "\n",
            "Kappa: 0.0259\n",
            "Epoch: 3\n",
            "Iteration 0, loss = 0.8116\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 743 / 880 correct (84.43)\n",
            "[[718  50]\n",
            " [ 87  25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.93      0.91       768\n",
            "           1       0.33      0.22      0.27       112\n",
            "\n",
            "   micro avg       0.84      0.84      0.84       880\n",
            "   macro avg       0.61      0.58      0.59       880\n",
            "weighted avg       0.82      0.84      0.83       880\n",
            "\n",
            "Kappa: 0.1841\n",
            "Epoch: 4\n",
            "Iteration 0, loss = 1.0714\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 735 / 880 correct (83.52)\n",
            "[[700  68]\n",
            " [ 77  35]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.91      0.91       768\n",
            "           1       0.34      0.31      0.33       112\n",
            "\n",
            "   micro avg       0.84      0.84      0.84       880\n",
            "   macro avg       0.62      0.61      0.62       880\n",
            "weighted avg       0.83      0.84      0.83       880\n",
            "\n",
            "Kappa: 0.2319\n",
            "Epoch: 5\n",
            "Iteration 0, loss = 1.0100\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 659 / 880 correct (74.89)\n",
            "[[611 157]\n",
            " [ 64  48]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.80      0.85       768\n",
            "           1       0.23      0.43      0.30       112\n",
            "\n",
            "   micro avg       0.75      0.75      0.75       880\n",
            "   macro avg       0.57      0.61      0.57       880\n",
            "weighted avg       0.82      0.75      0.78       880\n",
            "\n",
            "Kappa: 0.1655\n",
            "Epoch: 6\n",
            "Iteration 0, loss = 1.0966\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 692 / 880 correct (78.64)\n",
            "[[642 126]\n",
            " [ 62  50]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.84      0.87       768\n",
            "           1       0.28      0.45      0.35       112\n",
            "\n",
            "   micro avg       0.79      0.79      0.79       880\n",
            "   macro avg       0.60      0.64      0.61       880\n",
            "weighted avg       0.83      0.79      0.81       880\n",
            "\n",
            "Kappa: 0.2270\n",
            "Epoch: 7\n",
            "Iteration 0, loss = 0.9517\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 531 / 880 correct (60.34)\n",
            "[[452 316]\n",
            " [ 33  79]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.59      0.72       768\n",
            "           1       0.20      0.71      0.31       112\n",
            "\n",
            "   micro avg       0.60      0.60      0.60       880\n",
            "   macro avg       0.57      0.65      0.52       880\n",
            "weighted avg       0.84      0.60      0.67       880\n",
            "\n",
            "Kappa: 0.1414\n",
            "Epoch: 8\n",
            "Iteration 0, loss = 1.0472\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 667 / 880 correct (75.80)\n",
            "[[612 156]\n",
            " [ 57  55]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.80      0.85       768\n",
            "           1       0.26      0.49      0.34       112\n",
            "\n",
            "   micro avg       0.76      0.76      0.76       880\n",
            "   macro avg       0.59      0.64      0.60       880\n",
            "weighted avg       0.83      0.76      0.79       880\n",
            "\n",
            "Kappa: 0.2090\n",
            "Epoch: 9\n",
            "Iteration 0, loss = 0.9559\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 671 / 880 correct (76.25)\n",
            "[[617 151]\n",
            " [ 58  54]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.80      0.86       768\n",
            "           1       0.26      0.48      0.34       112\n",
            "\n",
            "   micro avg       0.76      0.76      0.76       880\n",
            "   macro avg       0.59      0.64      0.60       880\n",
            "weighted avg       0.83      0.76      0.79       880\n",
            "\n",
            "Kappa: 0.2108\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kgzCmb7O9LmJ",
        "colab_type": "code",
        "outputId": "52722aac-c257-4270-bdcd-2275211087ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "fig1, ax1 = plt.subplots()\n",
        "\n",
        "ax1.plot(t_losses, label='Training')\n",
        "ax1.plot(v_losses, label='Validation')\n",
        "\n",
        "ax1.set_title('Losses')\n",
        "ax1.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFZCAYAAACizedRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "HzDUdTT8Agxo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Task C"
      ]
    },
    {
      "metadata": {
        "id": "0ZU22xLLni54",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Task C proved more difficult than A or B. Our ClassifierGloVeDeepMultiConv() was not able to generalization on this dataset. We eventually found that the original classifier OriginalClassifierGloVe() performed relatively well so used this instead. Post-rationalizing this: subtask C has a smaller training set so is not able to train the network that has more parameters. \n",
        "\n",
        "We found that it was not helpful to add sentiment and hypothesised that this was because a tweet's sentiment is likely independent of the who the offense is targeted at. "
      ]
    },
    {
      "metadata": {
        "id": "Wf47-PIDAlIw",
        "colab_type": "code",
        "outputId": "6be60354-8d23-4027-81b1-0a8ff05016ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "TOKENIZE_PARAMS_new = {\"lemmatize\": True, \n",
        "                   \"rem_punct\": True, #remove punctuation\n",
        "                   \"rem_stopwords\": False}\n",
        "\n",
        "#Initialize tokenizer function with these parameters \n",
        "def tokenizer_local(text):\n",
        "    \"\"\"wrapper for tokenize so it can be used in torchtext Field creation. It takes the \n",
        "    current global varaiable TOKENIZE_PARAMS_LCL\"\"\"\n",
        "    try:\n",
        "        params = TOKENIZE_PARAMS_new\n",
        "    except NameError:\n",
        "        print(\"You must initialize the global variable TOKENIZE_PARAMS_LCL\")\n",
        "        raise NameError\n",
        "    return tokenize_params(text, params)\n",
        "\n",
        "TEXT = data.Field(sequential=True, tokenize=tokenizer_local, lower=True, batch_first = True)\n",
        "LABEL = data.LabelField(sequential=False, use_vocab=True, batch_first = True)\n",
        "ID = data.LabelField(sequential=False, use_vocab=False, batch_first=True)\n",
        "\n",
        "data_fields = [('id', ID), \n",
        "               ('tweet', TEXT),\n",
        "               ('subtask_a',LABEL),\n",
        "               ('subtask_b',LABEL),\n",
        "               ('subtask_c',LABEL)]\n",
        "\n",
        "set_seed()\n",
        "\n",
        "\n",
        "train = data.TabularDataset(train_fp, format='TSV', fields = \n",
        "                            data_fields, skip_header=True, filter_pred=lambda d: d.subtask_a == 'OFF' and d.subtask_b == 'TIN')\n",
        "\n",
        "train, valid = train.split(split_ratio=0.8, random_state=random.seed(SEED))\n",
        "\n",
        "print(f'Train size: {len(train)}')\n",
        "print(f'Validation size: {len(valid)}')\n",
        "\n",
        "#Now build vocab (using only the training set)\n",
        "TEXT.build_vocab(train, vectors='glove.twitter.27B.200d') #USE \"glove.840B.300d\" or glove.twitter.27B.200d\n",
        "\n",
        "LABEL.build_vocab(train.subtask_c)\n",
        "\n",
        "output_dim = len(LABEL.vocab)\n",
        "\n",
        "print(LABEL.vocab.stoi)\n",
        "\n",
        "#Create iterators\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid),\n",
        "                        batch_sizes=(BATCH_SIZE, len(valid)),  \n",
        "                        sort_key=lambda x: len(x.tweet), device=device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 3101\n",
            "Validation size: 775\n",
            "defaultdict(<function _default_unk_index at 0x7f334c389620>, {'IND': 0, 'GRP': 1, 'OTH': 2})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UTUbzJZBBBMr",
        "colab_type": "code",
        "outputId": "13fb125a-bd93-41ff-dd60-21856dcb5295",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 12729
        }
      },
      "cell_type": "code",
      "source": [
        "#Conv with Glove Deep\n",
        "embedding_dim = 200\n",
        "window_size = 3\n",
        "num_classes = 3\n",
        "lr = 0.0001656545740852317\n",
        "weight_decay = 0.0054326444080709255\n",
        "out_channels = 128\n",
        "dropout = 0.5\n",
        "weight = torch.tensor([1.6, 3.7 ,8.4], device = device) #deals with unbalanced classes\n",
        "n_hidden = (64, 32, 16, 8, 4)\n",
        "\n",
        "sentiment = False\n",
        "\n",
        "set_seed()\n",
        "#vocab, embedding_dim, window_size, out_channels, dropout, num_classes=2, sentiment=False, n_hidden = 64\n",
        "model_C = OriginalClassifierGloVe(TEXT.vocab, embedding_dim, window_size,\n",
        "                            out_channels= out_channels, dropout=dropout, num_classes= num_classes )\n",
        "\n",
        "optimizer = optim.Adam(model_C.parameters(), lr, weight_decay=weight_decay)\n",
        "loss_fn = nn.CrossEntropyLoss(weight = weight)\n",
        "\n",
        "\n",
        "t_losses, v_losses = train_helper('subtask_c', model_C, optimizer, loss_fn = loss_fn, epochs = 40, \n",
        "                                  train_loader=train_iterator, valid_loader=valid_iterator)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Iteration 0, loss = 2.0285\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 437 / 775 correct (56.39)\n",
            "[[312 163   1]\n",
            " [ 81 125   1]\n",
            " [ 37  55   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.66      0.69       476\n",
            "           1       0.36      0.60      0.45       207\n",
            "           2       0.00      0.00      0.00        92\n",
            "\n",
            "   micro avg       0.56      0.56      0.56       775\n",
            "   macro avg       0.36      0.42      0.38       775\n",
            "weighted avg       0.54      0.56      0.54       775\n",
            "\n",
            "Kappa: 0.1934\n",
            "Epoch: 1\n",
            "Iteration 0, loss = 1.6601\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 451 / 775 correct (58.19)\n",
            "[[331 111  34]\n",
            " [ 76 104  27]\n",
            " [ 38  38  16]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.70      0.72       476\n",
            "           1       0.41      0.50      0.45       207\n",
            "           2       0.21      0.17      0.19        92\n",
            "\n",
            "   micro avg       0.58      0.58      0.58       775\n",
            "   macro avg       0.45      0.46      0.45       775\n",
            "weighted avg       0.59      0.58      0.58       775\n",
            "\n",
            "Kappa: 0.2376\n",
            "Epoch: 2\n",
            "Iteration 0, loss = 1.4935\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 442 / 775 correct (57.03)\n",
            "[[294 165  17]\n",
            " [ 54 141  12]\n",
            " [ 24  61   7]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.62      0.69       476\n",
            "           1       0.38      0.68      0.49       207\n",
            "           2       0.19      0.08      0.11        92\n",
            "\n",
            "   micro avg       0.57      0.57      0.57       775\n",
            "   macro avg       0.46      0.46      0.43       775\n",
            "weighted avg       0.61      0.57      0.57       775\n",
            "\n",
            "Kappa: 0.2504\n",
            "Epoch: 3\n",
            "Iteration 0, loss = 1.3775\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 474 / 775 correct (61.16)\n",
            "[[327 123  26]\n",
            " [ 59 135  13]\n",
            " [ 30  50  12]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.69      0.73       476\n",
            "           1       0.44      0.65      0.52       207\n",
            "           2       0.24      0.13      0.17        92\n",
            "\n",
            "   micro avg       0.61      0.61      0.61       775\n",
            "   macro avg       0.49      0.49      0.48       775\n",
            "weighted avg       0.63      0.61      0.61       775\n",
            "\n",
            "Kappa: 0.3019\n",
            "Epoch: 4\n",
            "Iteration 0, loss = 1.4578\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 474 / 775 correct (61.16)\n",
            "[[318 143  15]\n",
            " [ 50 150   7]\n",
            " [ 27  59   6]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.67      0.73       476\n",
            "           1       0.43      0.72      0.54       207\n",
            "           2       0.21      0.07      0.10        92\n",
            "\n",
            "   micro avg       0.61      0.61      0.61       775\n",
            "   macro avg       0.48      0.49      0.46       775\n",
            "weighted avg       0.63      0.61      0.60       775\n",
            "\n",
            "Kappa: 0.3081\n",
            "Epoch: 5\n",
            "Iteration 0, loss = 1.3220\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 494 / 775 correct (63.74)\n",
            "[[348 111  17]\n",
            " [ 61 140   6]\n",
            " [ 34  52   6]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.73      0.76       476\n",
            "           1       0.46      0.68      0.55       207\n",
            "           2       0.21      0.07      0.10        92\n",
            "\n",
            "   micro avg       0.64      0.64      0.64       775\n",
            "   macro avg       0.48      0.49      0.47       775\n",
            "weighted avg       0.63      0.64      0.62       775\n",
            "\n",
            "Kappa: 0.3286\n",
            "Epoch: 6\n",
            "Iteration 0, loss = 1.3234\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 486 / 775 correct (62.71)\n",
            "[[331 123  22]\n",
            " [ 52 145  10]\n",
            " [ 32  50  10]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.70      0.74       476\n",
            "           1       0.46      0.70      0.55       207\n",
            "           2       0.24      0.11      0.15        92\n",
            "\n",
            "   micro avg       0.63      0.63      0.63       775\n",
            "   macro avg       0.50      0.50      0.48       775\n",
            "weighted avg       0.64      0.63      0.62       775\n",
            "\n",
            "Kappa: 0.3282\n",
            "Epoch: 7\n",
            "Iteration 0, loss = 1.4737\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 501 / 775 correct (64.65)\n",
            "[[352  88  36]\n",
            " [ 55 131  21]\n",
            " [ 36  38  18]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.74      0.77       476\n",
            "           1       0.51      0.63      0.56       207\n",
            "           2       0.24      0.20      0.22        92\n",
            "\n",
            "   micro avg       0.65      0.65      0.65       775\n",
            "   macro avg       0.51      0.52      0.52       775\n",
            "weighted avg       0.65      0.65      0.65       775\n",
            "\n",
            "Kappa: 0.3558\n",
            "Epoch: 8\n",
            "Iteration 0, loss = 1.1411\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 484 / 775 correct (62.45)\n",
            "[[327 126  23]\n",
            " [ 48 147  12]\n",
            " [ 32  50  10]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.69      0.74       476\n",
            "           1       0.46      0.71      0.55       207\n",
            "           2       0.22      0.11      0.15        92\n",
            "\n",
            "   micro avg       0.62      0.62      0.62       775\n",
            "   macro avg       0.49      0.50      0.48       775\n",
            "weighted avg       0.64      0.62      0.62       775\n",
            "\n",
            "Kappa: 0.3286\n",
            "Epoch: 9\n",
            "Iteration 0, loss = 0.9311\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 500 / 775 correct (64.52)\n",
            "[[351 105  20]\n",
            " [ 56 139  12]\n",
            " [ 34  48  10]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.74      0.77       476\n",
            "           1       0.48      0.67      0.56       207\n",
            "           2       0.24      0.11      0.15        92\n",
            "\n",
            "   micro avg       0.65      0.65      0.65       775\n",
            "   macro avg       0.50      0.51      0.49       775\n",
            "weighted avg       0.64      0.65      0.64       775\n",
            "\n",
            "Kappa: 0.3470\n",
            "Epoch: 10\n",
            "Iteration 0, loss = 1.0175\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 499 / 775 correct (64.39)\n",
            "[[347 103  26]\n",
            " [ 54 140  13]\n",
            " [ 33  47  12]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.73      0.76       476\n",
            "           1       0.48      0.68      0.56       207\n",
            "           2       0.24      0.13      0.17        92\n",
            "\n",
            "   micro avg       0.64      0.64      0.64       775\n",
            "   macro avg       0.51      0.51      0.50       775\n",
            "weighted avg       0.65      0.64      0.64       775\n",
            "\n",
            "Kappa: 0.3505\n",
            "Epoch: 11\n",
            "Iteration 0, loss = 0.9695\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 502 / 775 correct (64.77)\n",
            "[[348 108  20]\n",
            " [ 52 144  11]\n",
            " [ 35  47  10]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.73      0.76       476\n",
            "           1       0.48      0.70      0.57       207\n",
            "           2       0.24      0.11      0.15        92\n",
            "\n",
            "   micro avg       0.65      0.65      0.65       775\n",
            "   macro avg       0.51      0.51      0.49       775\n",
            "weighted avg       0.65      0.65      0.64       775\n",
            "\n",
            "Kappa: 0.3548\n",
            "Epoch: 12\n",
            "Iteration 0, loss = 1.0737\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 507 / 775 correct (65.42)\n",
            "[[353 102  21]\n",
            " [ 52 144  11]\n",
            " [ 35  47  10]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.74      0.77       476\n",
            "           1       0.49      0.70      0.58       207\n",
            "           2       0.24      0.11      0.15        92\n",
            "\n",
            "   micro avg       0.65      0.65      0.65       775\n",
            "   macro avg       0.51      0.52      0.50       775\n",
            "weighted avg       0.65      0.65      0.64       775\n",
            "\n",
            "Kappa: 0.3642\n",
            "Epoch: 13\n",
            "Iteration 0, loss = 0.9090\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 504 / 775 correct (65.03)\n",
            "[[349  92  35]\n",
            " [ 51 139  17]\n",
            " [ 33  43  16]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.73      0.77       476\n",
            "           1       0.51      0.67      0.58       207\n",
            "           2       0.24      0.17      0.20        92\n",
            "\n",
            "   micro avg       0.65      0.65      0.65       775\n",
            "   macro avg       0.52      0.53      0.52       775\n",
            "weighted avg       0.66      0.65      0.65       775\n",
            "\n",
            "Kappa: 0.3665\n",
            "Epoch: 14\n",
            "Iteration 0, loss = 0.9431\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 507 / 775 correct (65.42)\n",
            "[[351 105  20]\n",
            " [ 51 145  11]\n",
            " [ 34  47  11]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.74      0.77       476\n",
            "           1       0.49      0.70      0.58       207\n",
            "           2       0.26      0.12      0.16        92\n",
            "\n",
            "   micro avg       0.65      0.65      0.65       775\n",
            "   macro avg       0.52      0.52      0.50       775\n",
            "weighted avg       0.66      0.65      0.65       775\n",
            "\n",
            "Kappa: 0.3663\n",
            "Epoch: 15\n",
            "Iteration 0, loss = 1.0060\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 513 / 775 correct (66.19)\n",
            "[[356  98  22]\n",
            " [ 51 143  13]\n",
            " [ 33  45  14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.75      0.78       476\n",
            "           1       0.50      0.69      0.58       207\n",
            "           2       0.29      0.15      0.20        92\n",
            "\n",
            "   micro avg       0.66      0.66      0.66       775\n",
            "   macro avg       0.53      0.53      0.52       775\n",
            "weighted avg       0.66      0.66      0.66       775\n",
            "\n",
            "Kappa: 0.3800\n",
            "Epoch: 16\n",
            "Iteration 0, loss = 0.9266\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 505 / 775 correct (65.16)\n",
            "[[349  97  30]\n",
            " [ 50 141  16]\n",
            " [ 33  44  15]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.73      0.77       476\n",
            "           1       0.50      0.68      0.58       207\n",
            "           2       0.25      0.16      0.20        92\n",
            "\n",
            "   micro avg       0.65      0.65      0.65       775\n",
            "   macro avg       0.52      0.53      0.51       775\n",
            "weighted avg       0.66      0.65      0.65       775\n",
            "\n",
            "Kappa: 0.3678\n",
            "Epoch: 17\n",
            "Iteration 0, loss = 1.0480\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 508 / 775 correct (65.55)\n",
            "[[352  99  25]\n",
            " [ 50 141  16]\n",
            " [ 32  45  15]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.74      0.77       476\n",
            "           1       0.49      0.68      0.57       207\n",
            "           2       0.27      0.16      0.20        92\n",
            "\n",
            "   micro avg       0.66      0.66      0.66       775\n",
            "   macro avg       0.52      0.53      0.52       775\n",
            "weighted avg       0.66      0.66      0.65       775\n",
            "\n",
            "Kappa: 0.3728\n",
            "Epoch: 18\n",
            "Iteration 0, loss = 0.9098\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 508 / 775 correct (65.55)\n",
            "[[355  89  32]\n",
            " [ 51 136  20]\n",
            " [ 32  43  17]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.75      0.78       476\n",
            "           1       0.51      0.66      0.57       207\n",
            "           2       0.25      0.18      0.21        92\n",
            "\n",
            "   micro avg       0.66      0.66      0.66       775\n",
            "   macro avg       0.52      0.53      0.52       775\n",
            "weighted avg       0.66      0.66      0.66       775\n",
            "\n",
            "Kappa: 0.3735\n",
            "Epoch: 19\n",
            "Iteration 0, loss = 0.9788\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 510 / 775 correct (65.81)\n",
            "[[354  99  23]\n",
            " [ 50 143  14]\n",
            " [ 32  47  13]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.74      0.78       476\n",
            "           1       0.49      0.69      0.58       207\n",
            "           2       0.26      0.14      0.18        92\n",
            "\n",
            "   micro avg       0.66      0.66      0.66       775\n",
            "   macro avg       0.52      0.53      0.51       775\n",
            "weighted avg       0.66      0.66      0.65       775\n",
            "\n",
            "Kappa: 0.3751\n",
            "Epoch: 20\n",
            "Iteration 0, loss = 0.7427\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 510 / 775 correct (65.81)\n",
            "[[352  99  25]\n",
            " [ 49 143  15]\n",
            " [ 33  44  15]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.74      0.77       476\n",
            "           1       0.50      0.69      0.58       207\n",
            "           2       0.27      0.16      0.20        92\n",
            "\n",
            "   micro avg       0.66      0.66      0.66       775\n",
            "   macro avg       0.53      0.53      0.52       775\n",
            "weighted avg       0.66      0.66      0.65       775\n",
            "\n",
            "Kappa: 0.3772\n",
            "Epoch: 21\n",
            "Iteration 0, loss = 0.9490\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 507 / 775 correct (65.42)\n",
            "[[350 102  24]\n",
            " [ 47 144  16]\n",
            " [ 33  46  13]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.74      0.77       476\n",
            "           1       0.49      0.70      0.58       207\n",
            "           2       0.25      0.14      0.18        92\n",
            "\n",
            "   micro avg       0.65      0.65      0.65       775\n",
            "   macro avg       0.52      0.52      0.51       775\n",
            "weighted avg       0.66      0.65      0.65       775\n",
            "\n",
            "Kappa: 0.3718\n",
            "Epoch: 22\n",
            "Iteration 0, loss = 0.8874\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 515 / 775 correct (66.45)\n",
            "[[361  83  32]\n",
            " [ 52 134  21]\n",
            " [ 36  36  20]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.76      0.78       476\n",
            "           1       0.53      0.65      0.58       207\n",
            "           2       0.27      0.22      0.24        92\n",
            "\n",
            "   micro avg       0.66      0.66      0.66       775\n",
            "   macro avg       0.54      0.54      0.54       775\n",
            "weighted avg       0.67      0.66      0.66       775\n",
            "\n",
            "Kappa: 0.3853\n",
            "Epoch: 23\n",
            "Iteration 0, loss = 0.8121\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 508 / 775 correct (65.55)\n",
            "[[354  89  33]\n",
            " [ 50 136  21]\n",
            " [ 34  40  18]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.74      0.77       476\n",
            "           1       0.51      0.66      0.58       207\n",
            "           2       0.25      0.20      0.22        92\n",
            "\n",
            "   micro avg       0.66      0.66      0.66       775\n",
            "   macro avg       0.52      0.53      0.52       775\n",
            "weighted avg       0.66      0.66      0.66       775\n",
            "\n",
            "Kappa: 0.3742\n",
            "Epoch: 24\n",
            "Iteration 0, loss = 0.7746\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 521 / 775 correct (67.23)\n",
            "[[369  80  27]\n",
            " [ 54 135  18]\n",
            " [ 36  39  17]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.78      0.79       476\n",
            "           1       0.53      0.65      0.59       207\n",
            "           2       0.27      0.18      0.22        92\n",
            "\n",
            "   micro avg       0.67      0.67      0.67       775\n",
            "   macro avg       0.54      0.54      0.53       775\n",
            "weighted avg       0.67      0.67      0.67       775\n",
            "\n",
            "Kappa: 0.3922\n",
            "Epoch: 25\n",
            "Iteration 0, loss = 0.8359\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 515 / 775 correct (66.45)\n",
            "[[359  97  20]\n",
            " [ 51 143  13]\n",
            " [ 33  46  13]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.75      0.78       476\n",
            "           1       0.50      0.69      0.58       207\n",
            "           2       0.28      0.14      0.19        92\n",
            "\n",
            "   micro avg       0.66      0.66      0.66       775\n",
            "   macro avg       0.53      0.53      0.52       775\n",
            "weighted avg       0.66      0.66      0.66       775\n",
            "\n",
            "Kappa: 0.3825\n",
            "Epoch: 26\n",
            "Iteration 0, loss = 0.8533\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 514 / 775 correct (66.32)\n",
            "[[361  91  24]\n",
            " [ 51 139  17]\n",
            " [ 35  43  14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.76      0.78       476\n",
            "           1       0.51      0.67      0.58       207\n",
            "           2       0.25      0.15      0.19        92\n",
            "\n",
            "   micro avg       0.66      0.66      0.66       775\n",
            "   macro avg       0.52      0.53      0.52       775\n",
            "weighted avg       0.66      0.66      0.66       775\n",
            "\n",
            "Kappa: 0.3801\n",
            "Epoch: 27\n",
            "Iteration 0, loss = 0.7693\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 511 / 775 correct (65.94)\n",
            "[[356  97  23]\n",
            " [ 50 141  16]\n",
            " [ 35  43  14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.75      0.78       476\n",
            "           1       0.50      0.68      0.58       207\n",
            "           2       0.26      0.15      0.19        92\n",
            "\n",
            "   micro avg       0.66      0.66      0.66       775\n",
            "   macro avg       0.52      0.53      0.52       775\n",
            "weighted avg       0.66      0.66      0.65       775\n",
            "\n",
            "Kappa: 0.3756\n",
            "Epoch: 28\n",
            "Iteration 0, loss = 0.7953\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 517 / 775 correct (66.71)\n",
            "[[360  89  27]\n",
            " [ 50 140  17]\n",
            " [ 36  39  17]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.76      0.78       476\n",
            "           1       0.52      0.68      0.59       207\n",
            "           2       0.28      0.18      0.22        92\n",
            "\n",
            "   micro avg       0.67      0.67      0.67       775\n",
            "   macro avg       0.54      0.54      0.53       775\n",
            "weighted avg       0.67      0.67      0.66       775\n",
            "\n",
            "Kappa: 0.3890\n",
            "Epoch: 29\n",
            "Iteration 0, loss = 0.7751\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 516 / 775 correct (66.58)\n",
            "[[361  88  27]\n",
            " [ 51 138  18]\n",
            " [ 36  39  17]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.76      0.78       476\n",
            "           1       0.52      0.67      0.58       207\n",
            "           2       0.27      0.18      0.22        92\n",
            "\n",
            "   micro avg       0.67      0.67      0.67       775\n",
            "   macro avg       0.53      0.54      0.53       775\n",
            "weighted avg       0.67      0.67      0.66       775\n",
            "\n",
            "Kappa: 0.3858\n",
            "Epoch: 30\n",
            "Iteration 0, loss = 0.8079\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 509 / 775 correct (65.68)\n",
            "[[354  99  23]\n",
            " [ 50 141  16]\n",
            " [ 34  44  14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.74      0.77       476\n",
            "           1       0.50      0.68      0.57       207\n",
            "           2       0.26      0.15      0.19        92\n",
            "\n",
            "   micro avg       0.66      0.66      0.66       775\n",
            "   macro avg       0.52      0.53      0.51       775\n",
            "weighted avg       0.66      0.66      0.65       775\n",
            "\n",
            "Kappa: 0.3724\n",
            "Epoch: 31\n",
            "Iteration 0, loss = 0.7484\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 513 / 775 correct (66.19)\n",
            "[[357  93  26]\n",
            " [ 50 140  17]\n",
            " [ 35  41  16]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.75      0.78       476\n",
            "           1       0.51      0.68      0.58       207\n",
            "           2       0.27      0.17      0.21        92\n",
            "\n",
            "   micro avg       0.66      0.66      0.66       775\n",
            "   macro avg       0.53      0.53      0.52       775\n",
            "weighted avg       0.66      0.66      0.66       775\n",
            "\n",
            "Kappa: 0.3811\n",
            "Epoch: 32\n",
            "Iteration 0, loss = 0.8352\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 512 / 775 correct (66.06)\n",
            "[[356  99  21]\n",
            " [ 50 142  15]\n",
            " [ 35  43  14]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.75      0.78       476\n",
            "           1       0.50      0.69      0.58       207\n",
            "           2       0.28      0.15      0.20        92\n",
            "\n",
            "   micro avg       0.66      0.66      0.66       775\n",
            "   macro avg       0.53      0.53      0.52       775\n",
            "weighted avg       0.66      0.66      0.65       775\n",
            "\n",
            "Kappa: 0.3773\n",
            "Epoch: 33\n",
            "Iteration 0, loss = 0.7220\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 517 / 775 correct (66.71)\n",
            "[[360  91  25]\n",
            " [ 50 140  17]\n",
            " [ 36  39  17]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.76      0.78       476\n",
            "           1       0.52      0.68      0.59       207\n",
            "           2       0.29      0.18      0.23        92\n",
            "\n",
            "   micro avg       0.67      0.67      0.67       775\n",
            "   macro avg       0.54      0.54      0.53       775\n",
            "weighted avg       0.67      0.67      0.66       775\n",
            "\n",
            "Kappa: 0.3886\n",
            "Epoch: 34\n",
            "Iteration 0, loss = 0.8169\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 519 / 775 correct (66.97)\n",
            "[[362  91  23]\n",
            " [ 50 141  16]\n",
            " [ 36  40  16]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.76      0.78       476\n",
            "           1       0.52      0.68      0.59       207\n",
            "           2       0.29      0.17      0.22        92\n",
            "\n",
            "   micro avg       0.67      0.67      0.67       775\n",
            "   macro avg       0.54      0.54      0.53       775\n",
            "weighted avg       0.67      0.67      0.66       775\n",
            "\n",
            "Kappa: 0.3914\n",
            "Epoch: 35\n",
            "Iteration 0, loss = 0.7353\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 523 / 775 correct (67.48)\n",
            "[[366  88  22]\n",
            " [ 51 140  16]\n",
            " [ 37  38  17]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.77      0.79       476\n",
            "           1       0.53      0.68      0.59       207\n",
            "           2       0.31      0.18      0.23        92\n",
            "\n",
            "   micro avg       0.67      0.67      0.67       775\n",
            "   macro avg       0.55      0.54      0.54       775\n",
            "weighted avg       0.67      0.67      0.67       775\n",
            "\n",
            "Kappa: 0.3980\n",
            "Epoch: 36\n",
            "Iteration 0, loss = 0.7150\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 523 / 775 correct (67.48)\n",
            "[[368  87  21]\n",
            " [ 51 140  16]\n",
            " [ 37  40  15]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.77      0.79       476\n",
            "           1       0.52      0.68      0.59       207\n",
            "           2       0.29      0.16      0.21        92\n",
            "\n",
            "   micro avg       0.67      0.67      0.67       775\n",
            "   macro avg       0.54      0.54      0.53       775\n",
            "weighted avg       0.67      0.67      0.67       775\n",
            "\n",
            "Kappa: 0.3963\n",
            "Epoch: 37\n",
            "Iteration 0, loss = 0.7683\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 523 / 775 correct (67.48)\n",
            "[[366  85  25]\n",
            " [ 49 140  18]\n",
            " [ 37  38  17]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.77      0.79       476\n",
            "           1       0.53      0.68      0.60       207\n",
            "           2       0.28      0.18      0.22        92\n",
            "\n",
            "   micro avg       0.67      0.67      0.67       775\n",
            "   macro avg       0.54      0.54      0.54       775\n",
            "weighted avg       0.67      0.67      0.67       775\n",
            "\n",
            "Kappa: 0.4000\n",
            "Epoch: 38\n",
            "Iteration 0, loss = 0.8016\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 527 / 775 correct (68.00)\n",
            "[[370  84  22]\n",
            " [ 50 141  16]\n",
            " [ 38  38  16]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.78      0.79       476\n",
            "           1       0.54      0.68      0.60       207\n",
            "           2       0.30      0.17      0.22        92\n",
            "\n",
            "   micro avg       0.68      0.68      0.68       775\n",
            "   macro avg       0.55      0.54      0.54       775\n",
            "weighted avg       0.67      0.68      0.67       775\n",
            "\n",
            "Kappa: 0.4053\n",
            "Epoch: 39\n",
            "Iteration 0, loss = 0.7162\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 521 / 775 correct (67.23)\n",
            "[[364  90  22]\n",
            " [ 49 141  17]\n",
            " [ 36  40  16]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.76      0.79       476\n",
            "           1       0.52      0.68      0.59       207\n",
            "           2       0.29      0.17      0.22        92\n",
            "\n",
            "   micro avg       0.67      0.67      0.67       775\n",
            "   macro avg       0.54      0.54      0.53       775\n",
            "weighted avg       0.67      0.67      0.67       775\n",
            "\n",
            "Kappa: 0.3957\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TSbEtokDP1Dy",
        "colab_type": "code",
        "outputId": "557cf4fe-6837-478e-8a92-6c06a31d86a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "fig1, ax1 = plt.subplots()\n",
        "\n",
        "ax1.plot(t_losses, label='Training')\n",
        "ax1.plot(v_losses, label='Validation')\n",
        "\n",
        "ax1.set_title('Losses')\n",
        "ax1.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFZCAYAAACv05cWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSimllCG6hJVSSilDdAkrpZRShvwG0vNjbIsddlMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "dsiTUEZqga-V"
      },
      "cell_type": "markdown",
      "source": [
        "## Output predictions"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cojQod3qgW0Y",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Use this helper function to output predictions\n",
        "\n",
        "label_dict_A = {0: \"NOT\", 1: \"OFF\"}\n",
        "label_dict_B = {0: 'TIN', 1: 'UNT'} \n",
        "label_dict_C = {0: 'IND', 1: 'GRP', 2: 'OTH'}\n",
        "\n",
        "#Function to oputput predictions to CSV\n",
        "def output_prediction_csv(fp, model, test_loader, task_header, label_dict, RNN=False):\n",
        "    \"\"\"Outputs CSV to filepath fp\"\"\"\n",
        "    model.eval()  # set model to evaluation mode\n",
        "    model = model.to(device=\"cpu\")  # move the model parameters to GPU\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(test_loader):\n",
        "            x = batch.tweet\n",
        "            ids = batch.id\n",
        "            if x.device != \"cpu\":\n",
        "                x = x.to(device=\"cpu\", dtype=torch.long)  # move to  GPU\n",
        "            if ids.device != \"cpu\":\n",
        "                ids = ids.to(device=\"cpu\")\n",
        "            \n",
        "            if RNN:\n",
        "                #Must zero all of the accumalated hidden state for the RNN\n",
        "                model.hidden = model.init_hidden(batch.batch_size)\n",
        "   \n",
        "            logits = model(x, ids)\n",
        "            if task_header == 'subtask_c':\n",
        "                pred_prob = F.softmax(logits, dim=1)\n",
        "                predictions = torch.argmax(pred_prob, dim=1).view(-1, 1).numpy()\n",
        "            else:\n",
        "                pred_prob = torch.sigmoid(logits)\n",
        "                pred_1 = (pred_prob > 0.5).numpy()\n",
        "                predictions = pred_1.astype(int)\n",
        "            out_np = np.concatenate([ids.numpy().reshape((-1, 1)), predictions], axis=1)\n",
        "            df = pd.DataFrame(out_np)\n",
        "            df[1] = df[1].map(label_dict)\n",
        "            df.to_csv(fp, header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R5Zc-PNvgqKv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}