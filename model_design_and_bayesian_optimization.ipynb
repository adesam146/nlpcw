{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_design_and_bayesian_optimization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "_i_qSkEMxlkg"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adesam146/nlpcw/blob/rest_of_tasks_playground/model_design_and_bayesian_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "_i_qSkEMxlkg"
      },
      "cell_type": "markdown",
      "source": [
        "## Check GPU memory"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5-XwNX-831V6",
        "outputId": "13a98e80-4221-4a16-9a8d-040a05940c69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "#Check GPU Memory allocation\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NOXcwqriwFsu",
        "outputId": "142c5174-73eb-481d-d282-5961b6fbe93c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() "
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 11.5 GB  | Proc size: 2.4 GB\n",
            "GPU RAM Free: 11121MB | Used: 320MB | Util   3% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ecWOCoFgxS_j",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#run this if GPU utilization is not 0%\n",
        "# !kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "wTfeo8tcxhwC"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ePuqIHSPf554",
        "outputId": "2fb03d93-7086-4108-abd7-69b3aaa7fcc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U spacy ftfy torchtext\n",
        "!python -m spacy download en\n",
        "!pip install -U textblob #Sentiment analysis\n",
        "!python -m textblob.download_corpora\n",
        "!pip install scikit-optimize"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spacy in /usr/local/lib/python3.6/dist-packages (2.0.18)\n",
            "Requirement already up-to-date: ftfy in /usr/local/lib/python3.6/dist-packages (5.5.1)\n",
            "Requirement already up-to-date: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.16.2)\n",
            "Requirement already satisfied, skipping upgrade: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (6.12.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.18.4)\n",
            "Requirement already satisfied, skipping upgrade: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.9)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied, skipping upgrade: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.35)\n",
            "Requirement already satisfied, skipping upgrade: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy) (2018.1.10)\n",
            "Requirement already satisfied, skipping upgrade: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy) (0.1.7)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.0.1.post2)\n",
            "Requirement already satisfied, skipping upgrade: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.5.6)\n",
            "Requirement already satisfied, skipping upgrade: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.10.11)\n",
            "Requirement already satisfied, skipping upgrade: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.4.3.2)\n",
            "Requirement already satisfied, skipping upgrade: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.9.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.11.29)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
            "Requirement already satisfied, skipping upgrade: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy) (0.9.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n",
            "Requirement already up-to-date: textblob in /usr/local/lib/python3.6/dist-packages (0.15.3)\n",
            "Requirement already satisfied, skipping upgrade: nltk>=3.1 in /usr/local/lib/python3.6/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.1->textblob) (1.11.0)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "Finished.\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.16.2)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.20.2)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6NVQcb0MKUCh",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use two GloVe trained on two different corpuses for comparison:\n",
        "    # Glove.6B\n",
        "    # glove.twitter.27B\n",
        "#!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "!unzip glove.twitter.27B.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Srpq8hYt4whg",
        "outputId": "1725ceb9-bdd9-4157-fa07-6d72730fdaa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data import sampler\n",
        "from torch import autograd\n",
        "import spacy\n",
        "from torchtext import data\n",
        "from torchtext import datasets as nlp_dset\n",
        "import random\n",
        "from sklearn.utils import resample\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import string\n",
        "from sklearn import metrics\n",
        "from skopt import gp_minimize\n",
        "\n",
        "import torchvision.transforms as T\n",
        "\n",
        "nlp_spaCy = spacy.load('en', disable=[\"tagger\", \"parser\", \"ner\"])\n",
        "\n",
        "#stopwords\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stops_nltk = list(stopwords.words('english'))\n",
        "stops_sklearn = list(ENGLISH_STOP_WORDS)\n",
        "STOPWORDS = list(set(stops_nltk + stops_sklearn))\n",
        "\n",
        "#GPU\n",
        "GPU = True\n",
        "device_idx = 0\n",
        "if GPU:\n",
        "    device = torch.device(\"cuda:\"+str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "#Fix all seeds\n",
        "SEED = 0\n",
        "\n",
        "def set_seed(seed=SEED):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "set_seed()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Using device: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qtiwRhtm3s87",
        "outputId": "547b4e6f-8bdc-4557-abbf-9fc96ab8514d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "# Load datafiles from own google drive - EDIT AS NECESSARY\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "train_fp = \"\"\"/content/drive/My Drive/colab_data/offenseval-training-v1.tsv\"\"\"\n",
        "train_df = pd.read_csv(train_fp, delimiter=\"\\t\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "t9Zt3py7E1ep"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aMY0mUyknLDu",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenize_params(text, params):\n",
        "    \"\"\"tokenizes, and optionally also:\n",
        "        1) replaces words with their lemmatized forms\n",
        "        2) removes punctuation\n",
        "        3) removes english stopwords. \n",
        "    \"\"\"\n",
        "    \n",
        "    lemmatize = params.get(\"lemmatize\")\n",
        "    rem_punct = params.get(\"rem_punct\")\n",
        "    rem_stopwords = params.get(\"rem_stopwords\")\n",
        "    \n",
        "    #deal with stopwords\n",
        "    if rem_stopwords:\n",
        "        stopwords = STOPWORDS\n",
        "    else:\n",
        "        stopwords = []\n",
        "    \n",
        "    #deal with no punctuation\n",
        "    if rem_punct:\n",
        "        stoptokens = [x for x in list(string.punctuation) if x not in list(\"#$&*@\")]\n",
        "        stoptokens += stopwords\n",
        "    else:\n",
        "        stoptokens = stopwords \n",
        "    #stoptokens will be removed from tokens\n",
        "        \n",
        "    #replace each sentence with its lemmatized counterpart\n",
        "    if not lemmatize:\n",
        "        result = [tok.text for tok in nlp_spaCy.tokenizer(text) if tok.text not in stoptokens]\n",
        "    else:\n",
        "        #otherwise: lemmatize\n",
        "        tweet = nlp_spaCy(text)  #SpaCy tokenizes and does POS and lemmatization on tokens \n",
        "        tokens = []\n",
        "        for counter, token in enumerate(tweet):\n",
        "            if token.lemma_ == \"-PRON-\":         #treat pronouns differently as SpaCy replaces all of them with \"-PRON-\"\n",
        "                tokens.append(token.text)        #which therefore becomes a common token that biases results\n",
        "            #For everything else, add the lemma:\n",
        "            else:\n",
        "                tokens.append(token.lemma_)\n",
        "        result = [tok for tok in tokens if tok not in stoptokens]\n",
        "    return result\n",
        "\n",
        "def tokenizer(text):\n",
        "    \"\"\"wrapper for tokenize so it can be used in torchtext Field creation. It takes the \n",
        "    current global varaiable TOKENIZE_PARAMS\"\"\"\n",
        "    try:\n",
        "        params = TOKENIZE_PARAMS\n",
        "    except NameError:\n",
        "        print(\"You must initialize the global variable TOKENIZE_PARAMS\")\n",
        "        raise NameError\n",
        "    return tokenize_params(text, params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZlBy4TS6JqW1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Add sentiment\n",
        "def add_sentiment(h, ids):\n",
        "    \"\"\"Calculate tweet sentiment and concatenate this feature onto the \n",
        "    feature matrix (typically just before the fully connected layers). \n",
        "    The feature matrix dimensions will be changed as follows: \n",
        "        (B, O) -> (B, O + 2)\n",
        "        where B = batch size, O = out channels\n",
        "    \"\"\"\n",
        "\n",
        "    assert type(ids) == torch.Tensor, \"If sentiment == True, ids must be of type tensor\"\n",
        "\n",
        "    #retrieve tweets using id:\n",
        "    tweets = train_df[train_df[\"id\"].isin(ids.cpu().numpy())]\n",
        "\n",
        "    sentiments = []\n",
        "    subjectivities = []\n",
        "    \n",
        "    #extract \"sentiment\" and \"subjectivity\" according to TextBlob:\n",
        "    sentiments, subjectivities = get_sentiment_v(tweets[\"tweet\"].values)\n",
        "\n",
        "    sentiments = torch.tensor(sentiments, device=device).type(torch.float32).unsqueeze(1)\n",
        "    subjectivities = torch.tensor(subjectivities, device=device).type(torch.float32).unsqueeze(1)\n",
        "    \n",
        "    \n",
        "    h = torch.cat([h, sentiments, subjectivities], dim=1)\n",
        "    #(batch size, out channels + 2)\n",
        "\n",
        "    \n",
        "    return h\n",
        "\n",
        "def get_sentiment(text):\n",
        "    \"\"\"Gets sentiment and subjectivity of text\"\"\"\n",
        "    blob = TextBlob(text)\n",
        "    sentiment = blob.sentiment.polarity\n",
        "    subjectivity = blob.sentiment.subjectivity\n",
        "    return sentiment, subjectivity\n",
        "\n",
        "#create vectorized implementation for speed\n",
        "get_sentiment_v = np.vectorize(get_sentiment, otypes = [\"float\", \"float\"], doc= \"vectorized version of get_sentiment()\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9gUDCLc_A7uR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Models"
      ]
    },
    {
      "metadata": {
        "id": "GlyEWT07M6j7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#embedding (lookup layer) layer\n",
        "class OriginalClassifierGloVe(nn.Module):\n",
        "    \"\"\"Glove w. 2d conv\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab, embedding_dim, window_size, out_channels, dropout, num_classes=2):\n",
        "        \n",
        "        super(OriginalClassifierGloVe, self).__init__()\n",
        "        \n",
        "        \n",
        "        self.vocab = vocab\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
        "        self.embedding.weight.data.copy_(vocab.vectors) # copies pre-trained word vectors\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(window_size, embedding_dim))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc = nn.Linear(out_channels, 1 if num_classes == 2 else num_classes)\n",
        "        \n",
        "        self.lReLU = nn.LeakyReLU(negative_slope=0.05, inplace=False)\n",
        "        \n",
        "        #Kaming normalization\n",
        "        nn.init.kaiming_normal_(self.conv.weight)\n",
        "        nn.init.kaiming_normal_(self.fc.weight)\n",
        "\n",
        "        \n",
        "        \n",
        "    def forward(self, x, ids=None):\n",
        "        \n",
        "        embedded = self.embedding(x)\n",
        "                \n",
        "        #(batch size, max sent length, embedding dim)\n",
        "        \n",
        "        #images have 3 RGB channels \n",
        "        #for the text we add 1 channel\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        \n",
        "        #(batch size, 1, max sent length, embedding dim)\n",
        "        \n",
        "        feature_maps =  self.lReLU(self.conv(embedded).squeeze(3))\n",
        "        # (batch size, out_channels, max sent length - window size +1, 1)\n",
        "        # -> (batch size, out_channels, max sent length - window size +1)\n",
        "           \n",
        "        #the max pooling layer\n",
        "        pooled = F.max_pool1d(feature_maps, feature_maps.shape[2]).squeeze(2)\n",
        "        # (batch size, out_channels)      \n",
        "        \n",
        "        # Do batch normalize pooled then at sentiment\n",
        "        \n",
        "        return self.fc(self.dropout(pooled))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PeisH53s6cfR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#embedding (lookup layer) layer\n",
        "class SimpleClassifierGloVe(nn.Module):\n",
        "    \"\"\"Glove CNN w. 1 CNN layer and 2 fc layers\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab, embedding_dim, window_size, out_channels, dropout, \n",
        "                 num_classes=2, sentiment=False, n_hidden = 64):\n",
        "        \n",
        "        super(SimpleClassifierGloVe, self).__init__()\n",
        "        self.sentiment = sentiment\n",
        "        if sentiment == True:\n",
        "            added_features = 2\n",
        "        else:\n",
        "            added_features = 0\n",
        "            \n",
        "        self.vocab = vocab\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
        "        self.embedding.weight.data.copy_(vocab.vectors) # copies pre-trained word vectors\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(window_size, embedding_dim))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.fc1 = nn.Linear(out_channels + added_features, n_hidden)\n",
        "        self.fc2 = nn.Linear(n_hidden, 1 if num_classes == 2 else num_classes)\n",
        "        \n",
        "        \n",
        "        self.lReLU = nn.LeakyReLU(negative_slope=0.05, inplace=False)\n",
        "        \n",
        "        #Kaming normalization\n",
        "        nn.init.kaiming_normal_(self.conv.weight)\n",
        "        nn.init.kaiming_normal_(self.fc1.weight)\n",
        "        nn.init.kaiming_normal_(self.fc2.weight)\n",
        "        \n",
        "        \n",
        "    def forward(self, x, ids = None):\n",
        "        \n",
        "        embedded = self.embedding(x)\n",
        "            \n",
        "        #(batch size, max sent length, embedding dim)\n",
        "        \n",
        "        \n",
        "        #images have 3 RGB channels \n",
        "        #for the text we add 1 channel\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        #(batch size, 1, max sent length, embedding dim)\n",
        "        \n",
        "        feature_maps =  self.lReLU(self.conv(embedded).squeeze(3))\n",
        "        # (batch size, out_channels, max sent length - window size +1, 1)\n",
        "        # -> (batch size, out_channels, max sent length - window size +1)\n",
        "           \n",
        "        #the max pooling layer\n",
        "        h = F.max_pool1d(feature_maps, feature_maps.shape[2]).squeeze(2)\n",
        "        # (batch size, out_channels) \n",
        "        \n",
        "        if self.sentiment: #then add sentiment\n",
        "            h = add_sentiment(h, ids)\n",
        "            \n",
        "        h = self.lReLU(self.fc1( self.dropout(h)))\n",
        "        \n",
        "        h = self.fc2(self.dropout(h))\n",
        "       \n",
        "        \n",
        "        return h\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sFC6hi13DGnK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#embedding (lookup layer) layer\n",
        "class ClassifierGloVeDeep(nn.Module):\n",
        "    \"\"\"Glove w. 2d conv, 7 layers fc\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab, embedding_dim, window_size, out_channels = 128, \n",
        "                 dropout=0.5, n_hidden = 64, num_classes=2, sentiment=False):\n",
        "        \n",
        "        super(ClassifierGloVeDeep, self).__init__()\n",
        "        self.sentiment = sentiment\n",
        "        if sentiment == True:\n",
        "            added_features = 2 #This will update number of fully connected neurons\n",
        "        else:\n",
        "            added_features = 0\n",
        "            \n",
        "        self.vocab = vocab\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
        "        self.embedding.weight.data.copy_(vocab.vectors) # copies pre-trained word vectors\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(window_size, embedding_dim))\n",
        "        \n",
        "        self.fc1 = nn.Linear(out_channels + added_features, n_hidden[0])\n",
        "        self.fc2 = nn.Linear(n_hidden[0], n_hidden[1])\n",
        "        self.fc3 = nn.Linear(n_hidden[1], n_hidden[2])\n",
        "        self.fc4 = nn.Linear(n_hidden[2], n_hidden[3])\n",
        "        self.fc5 = nn.Linear(n_hidden[3], n_hidden[4])\n",
        "        self.fc6 = nn.Linear(n_hidden[4], 1 if num_classes == 2 else num_classes)\n",
        "        \n",
        "        #Activation: use leaky relu\n",
        "        self.lReLU = nn.LeakyReLU(negative_slope=0.05, inplace=False)\n",
        "        \n",
        "        #Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        #Kaming normalization\n",
        "        nn.init.kaiming_normal_(self.conv1.weight)\n",
        "        nn.init.kaiming_normal_(self.fc1.weight)\n",
        "        nn.init.kaiming_normal_(self.fc2.weight)\n",
        "        \n",
        "        \n",
        "    def forward(self, x, ids=None):\n",
        "        \n",
        "        embedded = self.embedding(x)\n",
        "                \n",
        "       \n",
        "        embedded = embedded.unsqueeze(1)\n",
        "       \n",
        "        \n",
        "        feature_maps =  self.lReLU(self.conv1(embedded).squeeze(3))\n",
        "        \n",
        "       \n",
        "        h = F.max_pool1d(feature_maps, feature_maps.shape[2]).squeeze(2)\n",
        "        \n",
        "        if self.sentiment: #then add sentiment\n",
        "            h = add_sentiment(h, ids)\n",
        "        \n",
        "        h = self.lReLU(self.fc1( self.dropout(h)))\n",
        "        h = self.lReLU(self.fc2( self.dropout(h)))\n",
        "        h = self.lReLU(self.fc3( self.dropout(h)))\n",
        "        h = self.lReLU(self.fc4( self.dropout(h)))\n",
        "        h = self.lReLU(self.fc5( self.dropout(h)))\n",
        "        h = self.fc6( self.dropout(h))\n",
        "        \n",
        "        \n",
        "        return h\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6-FK3OFvGMhw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#embedding (lookup layer) layer\n",
        "class BidirectionalGRU(nn.Module):\n",
        "    \"\"\"Classifier consisting of Bidirectional GRU (i.e. RNN with memory) and \n",
        "    followed by two fully connected layers.\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab, embedding_dim, window_size, out_channels = 128, \n",
        "                 dropout=0.5, n_hidden = 64, num_classes=2, batch_size = BATCH_SIZE, \n",
        "                 sentiment= False, ids=None):\n",
        "        \n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "        \n",
        "        self.sentiment = sentiment\n",
        "        if sentiment == True:\n",
        "            added_features = 2 #This will update number of fully connected neurons\n",
        "        else:\n",
        "            added_features = 0\n",
        "            \n",
        "            \n",
        "        self.n_hidden = n_hidden\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.num_layers = 1 #number of GRU layers\n",
        "        \n",
        "        self.vocab = vocab\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
        "        self.embedding.weight.data.copy_(vocab.vectors) # copies pre-trained word vectors\n",
        "        \n",
        "        self.bi_gru =  torch.nn.GRU(input_size=embedding_dim, hidden_size=n_hidden, \n",
        "                                    num_layers= self.num_layers, batch_first=False, \n",
        "                                    bidirectional=True)\n",
        "        self.hidden = self.init_hidden(self.batch_size)\n",
        "    \n",
        "        #Fully connected layer will convert GRU output into a label\n",
        "        # number of input features is 2 * n_hidden since GRU is bidirectional\n",
        "        \n",
        "        self.fc1 = nn.Linear( 2 * n_hidden + added_features, 16)\n",
        "        self.fc2 = nn.Linear(16, 1 if num_classes == 2 else num_classes)\n",
        "\n",
        "        #Activation: use leaky relu\n",
        "        self.lReLU = nn.LeakyReLU(negative_slope=0.05, inplace=False)\n",
        "        \n",
        "        #Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        #Kaming normalization\n",
        "        nn.init.kaiming_normal_(self.fc1.weight)\n",
        "        nn.init.kaiming_normal_(self.fc2.weight)\n",
        "\n",
        "    def init_hidden(self, batch_size = BATCH_SIZE):\n",
        "        return torch.zeros((self.num_layers * 2, batch_size, self.n_hidden), device=device)\n",
        "        \n",
        "        \n",
        "    def forward(self, x, ids=None):\n",
        "        embedded = self.embedding(x)\n",
        "        #(batch size, max sent length, embedding dim)\n",
        "        embedded = self.embedding(x).view((embedded.shape[1], embedded.shape[0], -1))\n",
        "        #(max sent length, batch size, embedding dim)\n",
        "        \n",
        "        bi_output, self.hidden = self.bi_gru(embedded, self.hidden)\n",
        "        \n",
        "        # add sentiment?\n",
        "        \n",
        "        #Just take final value of bi_output:\n",
        "        h = self.lReLU(bi_output[-1])\n",
        "        \n",
        "        if self.sentiment: #then add sentiment\n",
        "            h = add_sentiment(h, ids)\n",
        "            \n",
        "        h = self.fc1( self.dropout(h))\n",
        "        \n",
        "        h = self.fc2( self.dropout(h))\n",
        "        \n",
        "        #h = self.fc2(self.dropout(h))\n",
        "        return h\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "k4UHz12y6L7m",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Helper functions to run training routine, calculate metrics etc.\n",
        "def check_accuracy(task_header, loader, model, conf=False, RNN=False, verbose = True, ret_optim_metric=False):\n",
        "    \"\"\"\n",
        "    Note at the moment this function assumes the batch size is equal to the \n",
        "    number of data in the loader when calculating the confusion matrix\n",
        "    \"\"\"\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    bayesian_metric = None\n",
        "    model.eval()  # set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(loader):\n",
        "            x, y = batch.tweet, getattr(batch, task_header)\n",
        "            y = y.view(-1, 1)\n",
        "                \n",
        "            x = x.to(device=device, dtype=torch.long)  # move to  GPU\n",
        "            y = y.to(device=device, dtype=torch.long)\n",
        "            \n",
        "            if RNN:\n",
        "                #Must zero all of the accumalated hidden state for the RNN\n",
        "                model.hidden = model.init_hidden(batch.batch_size)\n",
        "                \n",
        "            logits = model(x, ids = batch.id)\n",
        "            if task_header == 'subtask_c':\n",
        "                pred_prob = F.softmax(logits, dim=1)\n",
        "                pred_1 = torch.argmax(pred_prob, dim=1).view(-1, 1)\n",
        "            else:\n",
        "                pred_prob = torch.sigmoid(logits)\n",
        "                pred_1 = (pred_prob > 0.5).type(torch.long)\n",
        "              \n",
        "            num_correct += (pred_1 == y).sum()\n",
        "            num_samples += pred_prob.size(0)\n",
        "            \n",
        "            # move to CPU to prevent memory overflow and calculate metrics\n",
        "            x = x.to(device=\"cpu\", dtype=torch.long)\n",
        "            y = y.to(device=\"cpu\", dtype=torch.long).numpy()\n",
        "            pred_1 = pred_1.to(device=\"cpu\", dtype=torch.long).numpy()\n",
        "            \n",
        "            \n",
        "        acc = float(num_correct) / num_samples\n",
        "        if conf:\n",
        "            confusion = metrics.confusion_matrix(y, pred_1)\n",
        "            clas_rep = metrics.classification_report(y, pred_1, output_dict =ret_optim_metric)\n",
        "            kappa = \"Kappa: {:.4f}\".format(metrics.cohen_kappa_score(y, pred_1))\n",
        "            if ret_optim_metric:\n",
        "                bayesian_metric = optim_metric(clas_rep)\n",
        "        else:\n",
        "            total_metric = None\n",
        "        if verbose:\n",
        "            print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
        "            print(confusion)\n",
        "            print(clas_rep)\n",
        "            print(kappa)\n",
        "    return bayesian_metric\n",
        "            \n",
        "def optim_metric(clas_rep):\n",
        "    \"\"\"calculate Bayesian Optimization metric\"\"\"\n",
        "    f1_1 = clas_rep['0'][\"f1-score\"]\n",
        "    f1_2 = clas_rep['1'][\"f1-score\"]\n",
        "    total = np.sqrt(f1_1 * f1_2)\n",
        "    return total\n",
        "\n",
        "def check_loss(task_header, loader, model, loss_fn, RNN=False):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        loss = 0\n",
        "        for idx, batch in enumerate(loader):\n",
        "            x, y = batch.tweet, getattr(batch, task_header)\n",
        "\n",
        "            x = x.to(device=device, dtype=torch.long) \n",
        "            y = y.to(device=device, dtype=torch.long if task_header == 'subtask_c' else torch.float)\n",
        "            \n",
        "            if RNN:\n",
        "                #Must zero all of the accumalated hidden state for the RNN\n",
        "                model.hidden = model.init_hidden(batch.batch_size)\n",
        "                \n",
        "            logits = model(x, ids= batch.id)\n",
        "\n",
        "            loss += loss_fn(logits, y.view(-1,) if isinstance(loss_fn, nn.CrossEntropyLoss) else y.view(-1, 1))\n",
        "\n",
        "    return loss/len(loader)\n",
        "      \n",
        "\n",
        "def train_helper(task_header, model, optimizer, train_loader, \n",
        "               valid_loader, epochs=1, RNN = False, loss_fn=F.binary_cross_entropy_with_logits, \n",
        "                 print_every=50, verbose = True, ret_optim_metric=False):\n",
        "    \"\"\"\n",
        "    Train a model\n",
        "    \n",
        "    Inputs:\n",
        "    - model: A PyTorch Module giving the model to train.\n",
        "    - optimizer: An Optimizer object we will use to train the model\n",
        "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
        "    \n",
        "    Returns: Nothing, but prints model accuracies during training.\n",
        "    \"\"\"\n",
        "    #sets seeds to make results reproducible\n",
        "    set_seed()\n",
        "    \n",
        "    model = model.to(device=device)  # move the model parameters to GPU\n",
        "    \n",
        "    training_losses = []\n",
        "    validation_losses = []\n",
        "    optimizing_metric = []\n",
        "    try:\n",
        "        for epoch in range(epochs):\n",
        "            if verbose:\n",
        "                print(\"Epoch:\", epoch)\n",
        "            total_loss = 0\n",
        "            for batch_idx, batch in enumerate(train_loader):\n",
        "\n",
        "                model.train()  # put model to training mode\n",
        "                optimizer.zero_grad()\n",
        "                inputs, targets = batch.tweet, getattr(batch, task_header)\n",
        "                \n",
        "                if RNN:\n",
        "                    #Must zero all of the accumulated hidden state for the RNN\n",
        "                    model.hidden = model.init_hidden(batch.batch_size)\n",
        "                \n",
        "                \n",
        "                x = inputs.to(device=device, dtype=torch.long)  # move to device, e.g. GPU\n",
        "                y = targets.to(device=device, dtype=torch.long if task_header == 'subtask_c' else torch.float) #this should be a float cross entropy\n",
        "                #x = inputs\n",
        "                #y = targets\n",
        "                logits = model(x, ids = batch.id)\n",
        "                \n",
        "                # When using cross_entropy the targets need to have a shape (N,)\n",
        "                # However, for BCEWithLogits they just need\n",
        "                # to have the same shape as the logits\n",
        "                loss = loss_fn(logits, y.view(-1,) if isinstance(loss_fn, nn.CrossEntropyLoss) else y.view(-1, 1))\n",
        "                # Zero out all of the gradients for the variables which the optimizer\n",
        "                # will update.\n",
        "                \n",
        "\n",
        "                # This is the backwards pass: compute the gradient of the loss with\n",
        "                # respect to each  parameter of the model.\n",
        "                loss.backward()\n",
        "\n",
        "                # Actually update the parameters of the model using the gradients\n",
        "                # computed by the backwards pass.\n",
        "                optimizer.step()\n",
        "\n",
        "                x = x.to(device=\"cpu\", dtype=torch.long)  # move to CPU to prevent memory overflow\n",
        "                y = y.to(device=\"cpu\", dtype=torch.long)\n",
        "\n",
        "                total_loss += loss.detach().item()\n",
        "                \n",
        "                if batch_idx % print_every == 0 and verbose:\n",
        "                    print('Iteration %d, loss = %.4f' % (batch_idx, loss.item()))\n",
        "            \n",
        "            training_losses.append(total_loss/len(train_loader))\n",
        "            if verbose:\n",
        "                print()\n",
        "                print(\"Validation Accuracy:\")\n",
        "            optim_metric = check_accuracy(task_header, valid_loader, model, RNN=RNN, \n",
        "                                          conf=True, verbose=verbose, ret_optim_metric=ret_optim_metric)\n",
        "            optimizing_metric.append(optim_metric)\n",
        "            \n",
        "            valid_loss = check_loss(task_header, valid_loader, model, loss_fn, RNN)\n",
        "            validation_losses.append(valid_loss)\n",
        "        if ret_optim_metric:\n",
        "            return training_losses, validation_losses, optimizing_metric\n",
        "        else:\n",
        "            return training_losses, validation_losses,\n",
        "    except Exception as e:\n",
        "        #Attempt to prevent GPU memory overflow by transferring model back to cpu\n",
        "        #model = model.to(device=\"cpu\")\n",
        "        raise e    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fDt0pRgyHgxw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tuning\n",
        "There are a huge number of permutations of hyperparameters (particularly to do with preprocessing). Hence it will be useful to do a hyperparameter search for the best values. These are:\n",
        " - Lemmatize words $\\in [True, False]$\n",
        " - Remove punctuation $\\in [True, False]$\n",
        " - Remove stopwords $\\in [True, False]$\n",
        " - Add sentiment (and subjectivity) $\\in [True, False]$\n",
        " - Type of model $\\in [$simple_CNN, Deep_CNN, Bidirectional_GRU]\n",
        " - Model parameters (number of neurons per layers etc)\n",
        " - Window size (in CNN only) - size of kernel. \n",
        " - Learning rate $\\in [0.0001, 0.0025]$\n",
        " - weight_decay $\\in [0.0, 0.1]$\n",
        "\n",
        "We will conduct Bayesian optimization on these parameters. \n",
        "### Choice of optimization metric\n",
        "We want to avoid the model predicting all True or all False so will maximize the product of the f1 scores for each class (as this will be zero if either class is never correctly predicted by the model).  To avoid complications due to different combinations of parameters affecting the speed at which the network learns, we will use a large number of epochs but take the maximum value (not including the first five epochs). In an ideal world, we would conduct a full scale Bayesian optimization for all three subtasks but due to the constraints of Colab, it will not be possible in this case. Hence we will optimize the values for subtask A and then do some basic checks for the latter parts. \n",
        " \n",
        "Note: It was clear from preliminary investigation that our implementation of the Bidirectional_GRU network was not performing well. As such, it was not included in the Bayesian Optimization. \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "7uXTKBmINouS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Bayesian optimization for hyperparameters\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def hyperparam_tuning(x0=None, y0=None):\n",
        "    \"\"\"Run hyperparameter tuning routine using bayesian optimization.\n",
        "\n",
        "    The hyperparameters are as follows (in this order):\n",
        "         - Lemmatize words $\\in [True, False]$\n",
        "         - Remove punctuation $\\in [True, False]$\n",
        "         - Remove stopwords $\\in [True, False]$\n",
        "         - Add sentiment (and subjectivity) $\\in [True, False]$\n",
        "         - Type of model $\\in [$simple_CNN, Deep_CNN, Bidirectional_GRU]\n",
        "         - Learning rate $\\in [0.0001, 0.0025]$\n",
        "         - weight_decay $\\in [0.0, 0.1]$\n",
        "         - window_size $\\in [3, 5]$\n",
        "    \"\"\"\n",
        "\n",
        "    #define dimension lower/upper bound (or possible values) for each hyperparameter\n",
        "        #args are of the form:\n",
        "        #[lemma, rem_punct, rem_stopwords, sentiment, model_type, lr, weight_decay, window_size, dropout_on]\n",
        "\n",
        "    dimensions = [(True, False), (True, False), (True, False), (True, False),\n",
        "                  (\"simple_CNN\", \"Deep_CNN\"), (0.0001, 0.0015), (0.0, 0.1), \n",
        "                  (2, 3, 4, 5), (True, False)]\n",
        "                  \n",
        "\n",
        "    #It will check the initial points above and then use bayesian optimization\n",
        "    #to choose the next points to evaluate\n",
        "    #first define a function to minimize:\n",
        "    res = gp_minimize(fn_optim, dimensions, n_calls=20, n_random_starts=0,\n",
        "                acq_func='gp_hedge', x0=x0, y0=y0, \n",
        "                random_state=SEED, verbose=True, callback=None, n_points=1000,\n",
        "                n_restarts_optimizer=5, xi=0.01, kappa=1.96,\n",
        "                noise='gaussian')\n",
        "    print(\"Bayesian Optimization Results:\")\n",
        "    print(res)\n",
        "    return res\n",
        "\n",
        "#Define function which will call the training cycle with each set of parameters\n",
        "def fn_optim(args, epochs=25, verbose=False):\n",
        "    \"\"\"Helper function to run the hyperparameter optimisation. It takes the\n",
        "    hyperparameters as args and must return the quantity to be minimized\n",
        "    \"\"\"\n",
        "    [lemma, rem_punct, rem_stopwords, sentiment, model_type, lr, weight_decay, window_size, dropout_on] = args\n",
        "\n",
        "    out_channels = 128 #keep these fixed\n",
        "    embedding_dim = 200 #for glove\n",
        "    \n",
        "    if dropout_on == True:\n",
        "        dropout_rate = 0.5\n",
        "    else:\n",
        "        dropout_rate = 0\n",
        "\n",
        "    TOKENIZE_PARAMS_LCL = {\"lemmatize\": lemma, \n",
        "                       \"rem_punct\": rem_punct, #remove punctuation\n",
        "                       \"rem_stopwords\": rem_stopwords}\n",
        "    \n",
        "    #Initialize tokenizer function with these parameters \n",
        "    def tokenizer_local(text):\n",
        "        \"\"\"wrapper for tokenize so it can be used in torchtext Field creation. It takes the \n",
        "        current global varaiable TOKENIZE_PARAMS\"\"\"\n",
        "        try:\n",
        "            params = TOKENIZE_PARAMS_LCL\n",
        "        except NameError:\n",
        "            print(\"You must initialize the global variable TOKENIZE_PARAMS\")\n",
        "            raise NameError\n",
        "        return tokenize_params(text, params)\n",
        "    \n",
        "    #Initialize vocab with these parameters:\n",
        "\n",
        "    TEXT = data.Field(sequential=True, tokenize=tokenizer_local, lower=True, batch_first = True)\n",
        "    LABEL = data.LabelField(sequential=False, use_vocab=True, batch_first = True)\n",
        "    ID = data.LabelField(sequential=False, use_vocab=False, batch_first=True)\n",
        "\n",
        "    data_fields = [('id', ID), \n",
        "                   ('tweet', TEXT),\n",
        "                   ('subtask_a',LABEL),\n",
        "                   ('subtask_b',LABEL),\n",
        "                   ('subtask_c',LABEL)]\n",
        "\n",
        "    set_seed()\n",
        "    train = data.TabularDataset(train_fp, format='TSV', fields = \n",
        "                                data_fields, skip_header=True, filter_pred=None)\n",
        "\n",
        "    train, valid = train.split(split_ratio=0.8, random_state=random.seed(SEED))\n",
        "\n",
        "    #Now build vocab (using only the training set)\n",
        "    TEXT.build_vocab(train, vectors='glove.twitter.27B.200d') #USE \"glove.840B.300d\" or glove.twitter.27B.200d\n",
        "\n",
        "    LABEL.build_vocab(train.subtask_a)\n",
        "\n",
        "    output_dim = len(LABEL.vocab)\n",
        "\n",
        "    #Create iterators\n",
        "    train_iterator, valid_iterator = data.BucketIterator.splits((train, valid),\n",
        "                            batch_sizes=(BATCH_SIZE, len(valid)),  \n",
        "                            sort_key=lambda x: len(x.tweet), device=device)\n",
        "\n",
        "    # For retrieving tweet text later on\n",
        "    train_df = pd.read_csv(train_fp, delimiter=\"\\t\")\n",
        "\n",
        "    #define model\n",
        "    if model_type == \"simple_CNN\":\n",
        "        n_hidden = 64\n",
        "        model = SimpleClassifierGloVe(TEXT.vocab, embedding_dim, window_size, out_channels, \n",
        "                          dropout=True, sentiment = sentiment)\n",
        "    elif model_type == \"Deep_CNN\":\n",
        "        n_hidden = (64, 32, 16, 8, 4)\n",
        "        model = ClassifierGloVeDeep(TEXT.vocab, embedding_dim, window_size, out_channels, \n",
        "             dropout=dropout_rate, n_hidden = n_hidden, sentiment=sentiment)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid NN architecture. n_hidden must be either \\\"funnel\\\" or \\\"diamond\\\".\")\n",
        "\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr = lr, weight_decay = weight_decay)\n",
        "\n",
        "    pos_weight = torch.tensor([2.], device = device) #deals with unbalanced classes\n",
        "\n",
        "    loss_fn = nn.BCEWithLogitsLoss(pos_weight = pos_weight)\n",
        "\n",
        "    #train model\n",
        "    _, v_losses, optim_metric = train_helper('subtask_a', model, optimizer, loss_fn = loss_fn, \n",
        "                                      epochs = 25, train_loader=train_iterator, \n",
        "                                      valid_loader=valid_iterator, ret_optim_metric=True, verbose=verbose)\n",
        "\n",
        "    max_val = max(optim_metric[4:])\n",
        "\n",
        "    return - max_val #return negative value as this will minimize this\n",
        "\n",
        "#hyperparam_tuning()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EXxnttXR5TUN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Optimization conclusions \n",
        "\n",
        "* The Bayesian optimization suggested that, for our limited range of architectures, all the preprocessing steps we performed (lemmatization, removal of punctuation and removal of stopwords) had a positive impact on performance.\n",
        "* The addition of sentiment aided performance as expected. \n",
        "* The use of 7 fully connected layers was superior to 2 layers (suggesting overfitting was not a huge issue here).\n",
        "* We found that weight_decay in the optimizer was preferred to dropout as a method of regularization.\n",
        "* A window size of 5 tokens in the convolution was preferred. This is particularly interesting as it was the largest value we tried. This suggests we may have been searching in the wrong region and optimal window size is larger than five. This suggests that the model prefers to have a larger amount of context available when considering the importance of each individual token. We conducted another search on this hyperparameter with the other hyperparameters fixed. "
      ]
    },
    {
      "metadata": {
        "id": "9ryiD3YS5rYa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "            #[lemma, rem_punct, rem_stopwords, sentiment, model_type, lr, weight_decay, window_size, dropout_on]\n",
        "best_args = [True, True, True, True, 'Deep_CNN', 0.0001656545740852317, 0.0054326444080709255, 5, False]\n",
        "optim_metric_val = - fn_optim(best_args, epochs=25, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yipQs8Gn_bTC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Iterative improvement of model "
      ]
    },
    {
      "metadata": {
        "id": "SyHIrrGl8oXg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VgDEwQxh8JMj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1402
        },
        "outputId": "aae1df16-4fc4-414f-bebe-755eec55ae16"
      },
      "cell_type": "code",
      "source": [
        "# Quick line search for best window size (keeping other best_args) fixed\n",
        "# Hack to code this quickly:\n",
        "    #We use the fn_optim() function and just pass a different value of window size\n",
        "    #at index 7 of args:\n",
        "    \n",
        "          #[lemma, rem_punct, rem_stopwords, sentiment, model_type, lr, weight_decay, window_size, dropout_on]\n",
        "best_args = [True, True, True, True, 'Deep_CNN', 0.0001656545740852317, 0.0054326444080709255, 5, False]\n",
        "\n",
        "w_sizes = [5, 6, 7, 9, 12, 20, 30, 50]\n",
        "\n",
        "for size in w_sizes:\n",
        "    print(\"For window size:\", size)\n",
        "    args = best_args\n",
        "    args[7] = size\n",
        "    optim_metric_val = - fn_optim(args, epochs=25, verbose=False)\n",
        "    print(\"Square root of product of F1-scores: \", optim_metric_val)\n",
        "    print()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For window size: 5\n",
            "Square root of product of F1-scores:  0.743731033087325\n",
            "\n",
            "For window size: 6\n",
            "Square root of product of F1-scores:  0.7306656271243175\n",
            "\n",
            "For window size: 7\n",
            "Square root of product of F1-scores:  0.7322754673368059\n",
            "\n",
            "For window size: 9\n",
            "Square root of product of F1-scores:  0.7302783836834617\n",
            "\n",
            "For window size: 12\n",
            "Square root of product of F1-scores:  0.7369161985199361\n",
            "\n",
            "For window size: 20\n",
            "Square root of product of F1-scores:  0.7320976881905901\n",
            "\n",
            "For window size: 30\n",
            "Square root of product of F1-scores:  0.733712883624817\n",
            "\n",
            "For window size: 50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-9a771799cc9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0moptim_metric_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfn_optim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Square root of product of F1-scores: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim_metric_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-8b64de8459f1>\u001b[0m in \u001b[0;36mfn_optim\u001b[0;34m(args, epochs, verbose)\u001b[0m\n\u001b[1;32m    124\u001b[0m     _, v_losses, optim_metric = train_helper('subtask_a', model, optimizer, loss_fn = loss_fn, \n\u001b[1;32m    125\u001b[0m                                       \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                                       valid_loader=valid_iterator, ret_optim_metric=True, verbose=verbose)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mmax_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim_metric\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-6d1f6e0add18>\u001b[0m in \u001b[0;36mtrain_helper\u001b[0;34m(task_header, model, optimizer, train_loader, valid_loader, epochs, RNN, loss_fn, print_every, verbose, ret_optim_metric)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m#Attempt to prevent GPU memory overflow by transferring model back to cpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;31m#model = model.to(device=\"cpu\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-6d1f6e0add18>\u001b[0m in \u001b[0;36mtrain_helper\u001b[0;34m(task_header, model, optimizer, train_loader, valid_loader, epochs, RNN, loss_fn, print_every, verbose, ret_optim_metric)\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0;31m#x = inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0;31m#y = targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0;31m# When using cross_entropy the targets need to have a shape (N,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-6fc0aa38ce57>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, ids)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mfeature_maps\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 17179869184.00 GiB (GPU 0; 11.17 GiB total capacity; 123.41 MiB already allocated; 10.63 GiB free; 105.47 MiB cached)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "PG0Qy_gVC-EJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This analysis suggests that window_size = 5 was the optimal value (though the distribution is relatively flat). \n",
        "\n",
        "In imaging, is often better to have a series of smaller convolutions than a single large convolution as this reduces the number of parameters and, in theory, allows the learning of more abstract features. In our case, an single CNN layer with a window size of 5 will have a simialr effect to 2 convolutional layers with window size 3. Therefore, we created a final candidate model: a 2-layer CNN with 7 dense layers and compared this to our single-layer CNN with 7-dense layers. In an ideal world, we would optimize hyperparameters for this new network but in practice, we did not have sufficient time to do this. "
      ]
    },
    {
      "metadata": {
        "id": "N5FsTsPpBI7H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Final candidate model:\n",
        "#embedding (lookup layer) layer\n",
        "class ClassifierGloVeDeepMultiConv(nn.Module):\n",
        "    \"\"\"Glove w. multiple 2d conv, 7 layers fc\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab, embedding_dim, window_size, n_hidden = (64, 32, 16, 8, 4), out_channels = (128, 256), #out_channels must now be an iterable\n",
        "                 dropout=0.5,  num_classes=2, sentiment=False):\n",
        "        \n",
        "        super(ClassifierGloVeDeepMultiConv, self).__init__()\n",
        "        self.sentiment = sentiment\n",
        "        \n",
        "        if sentiment == True:\n",
        "            added_features = 2 #This will update number of fully connected neurons\n",
        "        else:\n",
        "            added_features = 0\n",
        "            \n",
        "        self.vocab = vocab\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
        "        self.embedding.weight.data.copy_(vocab.vectors) # copies pre-trained word vectors\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=out_channels[0], kernel_size=(window_size, embedding_dim))\n",
        "        self.conv2 = nn.Conv2d(in_channels=out_channels[0], out_channels=out_channels[1], kernel_size=(window_size, 1))\n",
        "        \n",
        "        self.fc1 = nn.Linear(out_channels[1] + added_features, n_hidden[0])\n",
        "        self.fc2 = nn.Linear(n_hidden[0], n_hidden[1])\n",
        "        self.fc3 = nn.Linear(n_hidden[1], n_hidden[2])\n",
        "        self.fc4 = nn.Linear(n_hidden[2], n_hidden[3])\n",
        "        self.fc5 = nn.Linear(n_hidden[3], n_hidden[4])\n",
        "        self.fc6 = nn.Linear(n_hidden[4], 1 if num_classes == 2 else num_classes)\n",
        "        \n",
        "        #Activation: use leaky relu\n",
        "        self.lReLU = nn.LeakyReLU(negative_slope=0.05, inplace=False)\n",
        "        \n",
        "        #Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        #Kaming normalization\n",
        "        nn.init.kaiming_normal_(self.conv1.weight)\n",
        "        nn.init.kaiming_normal_(self.conv2.weight)\n",
        "        nn.init.kaiming_normal_(self.fc1.weight)\n",
        "        nn.init.kaiming_normal_(self.fc2.weight)\n",
        "        nn.init.kaiming_normal_(self.fc3.weight)\n",
        "        nn.init.kaiming_normal_(self.fc4.weight)\n",
        "        nn.init.kaiming_normal_(self.fc5.weight)\n",
        "        nn.init.kaiming_normal_(self.fc6.weight)\n",
        "\n",
        "        \n",
        "    def forward(self, x, ids=None):\n",
        "        \n",
        "        embedded = self.embedding(x)\n",
        "                \n",
        "       \n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        print(\"embedded\", embedded.shape)\n",
        "        \n",
        "        h = self.lReLU(self.conv1(embedded))\n",
        "        h = self.lReLU(self.conv2(h).squeeze(3))\n",
        "        print(\"h\", h.shape)\n",
        "        h = F.max_pool1d(h, h.shape[2]).squeeze(2)\n",
        "        print(\"h2\", h.shape)\n",
        "\n",
        "        if self.sentiment: #then add sentiment\n",
        "            h = add_sentiment(h, ids)\n",
        "        \n",
        "        h = self.lReLU(self.fc1( self.dropout(h)))\n",
        "        h = self.lReLU(self.fc2( self.dropout(h)))\n",
        "        h = self.lReLU(self.fc3( self.dropout(h)))\n",
        "        h = self.lReLU(self.fc4( self.dropout(h)))\n",
        "        h = self.lReLU(self.fc5( self.dropout(h)))\n",
        "        h = self.fc6( self.dropout(h))\n",
        "        \n",
        "        \n",
        "        return h\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-j6j0YYYHhqb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Define function which will call the training cycle with each set of parameters\n",
        "#It is similar to fn_optim but more general purpose as it takes an initialized \n",
        "#model as input\n",
        "\n",
        "def run_model(args, model, epochs=25, verbose=True, pos_wg = 2., subtask='subtask_a'):\n",
        "    \"\"\":args of form: [lemma, rem_punct, rem_stopwords, sentiment,\n",
        "                                lr, weight_decay]\n",
        "       :pos_wg - ratio of positive to negative weights in subtask\n",
        "    \"\"\"\n",
        "    [lemma, rem_punct, rem_stopwords, lr, weight_decay] = args\n",
        "\n",
        "    out_channels = 128 #keep these fixed\n",
        "    embedding_dim = 200 #for glove\n",
        "    \n",
        "    dropout_rate = 0\n",
        "\n",
        "    TOKENIZE_PARAMS_LCL = {\"lemmatize\": lemma, \n",
        "                       \"rem_punct\": rem_punct, #remove punctuation\n",
        "                       \"rem_stopwords\": rem_stopwords}\n",
        "    \n",
        "    #Initialize tokenizer function with these parameters \n",
        "    def tokenizer_local(text):\n",
        "        \"\"\"wrapper for tokenize so it can be used in torchtext Field creation. It takes the \n",
        "        current global varaiable TOKENIZE_PARAMS_LCL\"\"\"\n",
        "        try:\n",
        "            params = TOKENIZE_PARAMS_LCL\n",
        "        except NameError:\n",
        "            print(\"You must initialize the global variable TOKENIZE_PARAMS_LCL\")\n",
        "            raise NameError\n",
        "        return tokenize_params(text, params)\n",
        "    \n",
        "    #Initialize vocab with these parameters:\n",
        "\n",
        "    TEXT = data.Field(sequential=True, tokenize=tokenizer_local, lower=True, batch_first = True)\n",
        "    LABEL = data.LabelField(sequential=False, use_vocab=True, batch_first = True)\n",
        "    ID = data.LabelField(sequential=False, use_vocab=False, batch_first=True)\n",
        "\n",
        "    data_fields = [('id', ID), \n",
        "                   ('tweet', TEXT),\n",
        "                   ('subtask_a',LABEL),\n",
        "                   ('subtask_b',LABEL),\n",
        "                   ('subtask_c',LABEL)]\n",
        "\n",
        "    set_seed()\n",
        "    train = data.TabularDataset(train_fp, format='TSV', fields = \n",
        "                                data_fields, skip_header=True, filter_pred=None)\n",
        "\n",
        "    train, valid = train.split(split_ratio=0.8, random_state=random.seed(SEED))\n",
        "\n",
        "    #Now build vocab (using only the training set)\n",
        "    TEXT.build_vocab(train, vectors='glove.twitter.27B.200d') #USE \"glove.840B.300d\" or glove.twitter.27B.200d\n",
        "\n",
        "    LABEL.build_vocab(train.subtask_a)\n",
        "\n",
        "    output_dim = len(LABEL.vocab)\n",
        "\n",
        "    #Create iterators\n",
        "    train_iterator, valid_iterator = data.BucketIterator.splits((train, valid),\n",
        "                            batch_sizes=(BATCH_SIZE, len(valid)),  \n",
        "                            sort_key=lambda x: len(x.tweet), device=device)\n",
        "\n",
        "    # For retrieving tweet text later on\n",
        "    train_df = pd.read_csv(train_fp, delimiter=\"\\t\")\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr = lr, weight_decay = weight_decay)\n",
        "\n",
        "    pos_weight = torch.tensor([pos_wg], device = device) #deals with unbalanced classes\n",
        "\n",
        "    loss_fn = nn.BCEWithLogitsLoss(pos_weight = pos_weight)\n",
        "\n",
        "    #train model\n",
        "    _, v_losses, optim_metric = train_helper(subtask, model, optimizer, loss_fn = loss_fn, \n",
        "                                      epochs = epochs, train_loader=train_iterator, \n",
        "                                      valid_loader=valid_iterator, ret_optim_metric=True, verbose=verbose)\n",
        "\n",
        "    max_val = max(optim_metric[4:])\n",
        "\n",
        "    return - max_val #return negative value as this will minimize this"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0HuMlNzvLPPw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TOKENIZE_PARAMS_new = {\"lemmatize\": True, \n",
        "                   \"rem_punct\": True, #remove punctuation\n",
        "                   \"rem_stopwords\": True}\n",
        "\n",
        "#Initialize tokenizer function with these parameters \n",
        "def tokenizer_local(text):\n",
        "    \"\"\"wrapper for tokenize so it can be used in torchtext Field creation. It takes the \n",
        "    current global varaiable TOKENIZE_PARAMS_LCL\"\"\"\n",
        "    try:\n",
        "        params = TOKENIZE_PARAMS_new\n",
        "    except NameError:\n",
        "        print(\"You must initialize the global variable TOKENIZE_PARAMS_LCL\")\n",
        "        raise NameError\n",
        "    return tokenize_params(text, params)\n",
        "\n",
        "#Initialize vocab with these parameters:\n",
        "\n",
        "TEXT = data.Field(sequential=True, tokenize=tokenizer_local, lower=True, batch_first = True)\n",
        "LABEL = data.LabelField(sequential=False, use_vocab=True, batch_first = True)\n",
        "ID = data.LabelField(sequential=False, use_vocab=False, batch_first=True)\n",
        "\n",
        "data_fields = [('id', ID), \n",
        "               ('tweet', TEXT),\n",
        "               ('subtask_a',LABEL),\n",
        "               ('subtask_b',LABEL),\n",
        "               ('subtask_c',LABEL)]\n",
        "\n",
        "set_seed()\n",
        "train = data.TabularDataset(train_fp, format='TSV', fields = \n",
        "                            data_fields, skip_header=True, filter_pred=None)\n",
        "\n",
        "train, valid = train.split(split_ratio=0.8, random_state=random.seed(SEED))\n",
        "\n",
        "#Now build vocab (using only the training set)\n",
        "TEXT.build_vocab(train, vectors='glove.twitter.27B.200d') #USE \"glove.840B.300d\" or glove.twitter.27B.200d\n",
        "\n",
        "LABEL.build_vocab(train.subtask_a)\n",
        "\n",
        "#Create iterators\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid),\n",
        "                        batch_sizes=(BATCH_SIZE, len(valid)),  \n",
        "                        sort_key=lambda x: len(x.tweet), device=device)\n",
        "\n",
        "# For retrieving tweet text later on\n",
        "train_df = pd.read_csv(train_fp, delimiter=\"\\t\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fKPuKJabJV7a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## run ClassifierGloVeDeepMultiConv() and compare with ClassifierGloVeDeep()\n",
        "\n",
        "#Shared parameters:\n",
        "n_hidden = (64, 32, 16, 8, 4)\n",
        "embedding_dim = 200\n",
        "dropout_rate = 0\n",
        "sentiment = True\n",
        "args_fix = [True, True, True, 0.0001656545740852317, 0.0054326444080709255]\n",
        "             #[lemma, rem_punct, rem_stopwords, lr, weight_decay] \n",
        "\n",
        "#ClassifierGloVeDeepMultiConv params - 77.3% accuracy\n",
        "window_size = 3\n",
        "out_channels = (128, 256)\n",
        "\n",
        "model_1 = ClassifierGloVeDeepMultiConv(TEXT.vocab, embedding_dim, window_size, out_channels, \n",
        "     dropout=dropout_rate, n_hidden = n_hidden, sentiment=sentiment)\n",
        "\n",
        "#ClassifierGloVeDeep params\n",
        "window_size = 5\n",
        "out_channels = 128\n",
        "model_2 = ClassifierGloVeDeep(TEXT.vocab, embedding_dim, window_size, out_channels, \n",
        "     dropout=dropout_rate, n_hidden = n_hidden, sentiment=sentiment)\n",
        "\n",
        "#metric1 = run_model(args_fix, model_1)\n",
        "metric2 = run_model(args_fix, model_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MVRjOO2zlIfG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6038
        },
        "outputId": "61c4b763-1a07-4f01-a06b-716cee7f66b3"
      },
      "cell_type": "code",
      "source": [
        "#Conv with Glove Deep\n",
        "embedding_dim = 200\n",
        "window_size = 3\n",
        "lr = 0.0001656545740852317\n",
        "weight_decay = 0.0054326444080709255\n",
        "out_channels = (128, 256)\n",
        "dropout = 0\n",
        "pos_weight = torch.tensor([2.], device = device) #deals with unbalanced classes\n",
        "n_hidden = (64, 32, 16, 8, 4)\n",
        "sentiment = True\n",
        "\n",
        "set_seed()\n",
        "\n",
        "model = ClassifierGloVeDeepMultiConv(TEXT.vocab, embedding_dim, window_size, \n",
        "                            out_channels, dropout, n_hidden = n_hidden, sentiment=sentiment)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr, weight_decay=weight_decay)\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight = pos_weight)\n",
        "\n",
        "\n",
        "t_losses, v_losses = train_helper('subtask_a', model, optimizer, loss_fn = loss_fn,\n",
        "                                  epochs = 20, train_loader=train_iterator, valid_loader=valid_iterator)\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Iteration 0, loss = 0.9840\n",
            "Iteration 50, loss = 0.7898\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1731 / 2648 correct (65.37)\n",
            "[[1057  716]\n",
            " [ 201  674]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.60      0.70      1773\n",
            "           1       0.48      0.77      0.60       875\n",
            "\n",
            "   micro avg       0.65      0.65      0.65      2648\n",
            "   macro avg       0.66      0.68      0.65      2648\n",
            "weighted avg       0.72      0.65      0.66      2648\n",
            "\n",
            "Kappa: 0.3189\n",
            "Epoch: 1\n",
            "Iteration 0, loss = 0.7839\n",
            "Iteration 50, loss = 0.7657\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1880 / 2648 correct (71.00)\n",
            "[[1220  553]\n",
            " [ 215  660]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.69      0.76      1773\n",
            "           1       0.54      0.75      0.63       875\n",
            "\n",
            "   micro avg       0.71      0.71      0.71      2648\n",
            "   macro avg       0.70      0.72      0.70      2648\n",
            "weighted avg       0.75      0.71      0.72      2648\n",
            "\n",
            "Kappa: 0.4030\n",
            "Epoch: 2\n",
            "Iteration 0, loss = 0.6703\n",
            "Iteration 50, loss = 0.6633\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1942 / 2648 correct (73.34)\n",
            "[[1309  464]\n",
            " [ 242  633]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.74      0.79      1773\n",
            "           1       0.58      0.72      0.64       875\n",
            "\n",
            "   micro avg       0.73      0.73      0.73      2648\n",
            "   macro avg       0.71      0.73      0.71      2648\n",
            "weighted avg       0.76      0.73      0.74      2648\n",
            "\n",
            "Kappa: 0.4338\n",
            "Epoch: 3\n",
            "Iteration 0, loss = 0.6764\n",
            "Iteration 50, loss = 0.6241\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1949 / 2648 correct (73.60)\n",
            "[[1308  465]\n",
            " [ 234  641]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.74      0.79      1773\n",
            "           1       0.58      0.73      0.65       875\n",
            "\n",
            "   micro avg       0.74      0.74      0.74      2648\n",
            "   macro avg       0.71      0.74      0.72      2648\n",
            "weighted avg       0.76      0.74      0.74      2648\n",
            "\n",
            "Kappa: 0.4408\n",
            "Epoch: 4\n",
            "Iteration 0, loss = 0.5584\n",
            "Iteration 50, loss = 0.5949\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1899 / 2648 correct (71.71)\n",
            "[[1227  546]\n",
            " [ 203  672]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.69      0.77      1773\n",
            "           1       0.55      0.77      0.64       875\n",
            "\n",
            "   micro avg       0.72      0.72      0.72      2648\n",
            "   macro avg       0.70      0.73      0.70      2648\n",
            "weighted avg       0.76      0.72      0.73      2648\n",
            "\n",
            "Kappa: 0.4185\n",
            "Epoch: 5\n",
            "Iteration 0, loss = 0.4959\n",
            "Iteration 50, loss = 0.4930\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2052 / 2648 correct (77.49)\n",
            "[[1500  273]\n",
            " [ 323  552]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.85      0.83      1773\n",
            "           1       0.67      0.63      0.65       875\n",
            "\n",
            "   micro avg       0.77      0.77      0.77      2648\n",
            "   macro avg       0.75      0.74      0.74      2648\n",
            "weighted avg       0.77      0.77      0.77      2648\n",
            "\n",
            "Kappa: 0.4839\n",
            "Epoch: 6\n",
            "Iteration 0, loss = 0.4350\n",
            "Iteration 50, loss = 0.6141\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2030 / 2648 correct (76.66)\n",
            "[[1428  345]\n",
            " [ 273  602]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.81      0.82      1773\n",
            "           1       0.64      0.69      0.66       875\n",
            "\n",
            "   micro avg       0.77      0.77      0.77      2648\n",
            "   macro avg       0.74      0.75      0.74      2648\n",
            "weighted avg       0.77      0.77      0.77      2648\n",
            "\n",
            "Kappa: 0.4833\n",
            "Epoch: 7\n",
            "Iteration 0, loss = 0.3767\n",
            "Iteration 50, loss = 0.4519\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1909 / 2648 correct (72.09)\n",
            "[[1239  534]\n",
            " [ 205  670]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.70      0.77      1773\n",
            "           1       0.56      0.77      0.64       875\n",
            "\n",
            "   micro avg       0.72      0.72      0.72      2648\n",
            "   macro avg       0.71      0.73      0.71      2648\n",
            "weighted avg       0.76      0.72      0.73      2648\n",
            "\n",
            "Kappa: 0.4241\n",
            "Epoch: 8\n",
            "Iteration 0, loss = 0.4443\n",
            "Iteration 50, loss = 0.3677\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2003 / 2648 correct (75.64)\n",
            "[[1392  381]\n",
            " [ 264  611]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.79      0.81      1773\n",
            "           1       0.62      0.70      0.65       875\n",
            "\n",
            "   micro avg       0.76      0.76      0.76      2648\n",
            "   macro avg       0.73      0.74      0.73      2648\n",
            "weighted avg       0.77      0.76      0.76      2648\n",
            "\n",
            "Kappa: 0.4676\n",
            "Epoch: 9\n",
            "Iteration 0, loss = 0.4038\n",
            "Iteration 50, loss = 0.3234\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2020 / 2648 correct (76.28)\n",
            "[[1441  332]\n",
            " [ 296  579]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.81      0.82      1773\n",
            "           1       0.64      0.66      0.65       875\n",
            "\n",
            "   micro avg       0.76      0.76      0.76      2648\n",
            "   macro avg       0.73      0.74      0.73      2648\n",
            "weighted avg       0.77      0.76      0.76      2648\n",
            "\n",
            "Kappa: 0.4696\n",
            "Epoch: 10\n",
            "Iteration 0, loss = 0.2927\n",
            "Iteration 50, loss = 0.3956\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2017 / 2648 correct (76.17)\n",
            "[[1452  321]\n",
            " [ 310  565]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.82      0.82      1773\n",
            "           1       0.64      0.65      0.64       875\n",
            "\n",
            "   micro avg       0.76      0.76      0.76      2648\n",
            "   macro avg       0.73      0.73      0.73      2648\n",
            "weighted avg       0.76      0.76      0.76      2648\n",
            "\n",
            "Kappa: 0.4632\n",
            "Epoch: 11\n",
            "Iteration 0, loss = 0.2040\n",
            "Iteration 50, loss = 0.3175\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2049 / 2648 correct (77.38)\n",
            "[[1610  163]\n",
            " [ 436  439]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.91      0.84      1773\n",
            "           1       0.73      0.50      0.59       875\n",
            "\n",
            "   micro avg       0.77      0.77      0.77      2648\n",
            "   macro avg       0.76      0.70      0.72      2648\n",
            "weighted avg       0.77      0.77      0.76      2648\n",
            "\n",
            "Kappa: 0.4449\n",
            "Epoch: 12\n",
            "Iteration 0, loss = 0.2410\n",
            "Iteration 50, loss = 0.2669\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2047 / 2648 correct (77.30)\n",
            "[[1543  230]\n",
            " [ 371  504]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84      1773\n",
            "           1       0.69      0.58      0.63       875\n",
            "\n",
            "   micro avg       0.77      0.77      0.77      2648\n",
            "   macro avg       0.75      0.72      0.73      2648\n",
            "weighted avg       0.77      0.77      0.77      2648\n",
            "\n",
            "Kappa: 0.4653\n",
            "Epoch: 13\n",
            "Iteration 0, loss = 0.1569\n",
            "Iteration 50, loss = 0.1702\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1997 / 2648 correct (75.42)\n",
            "[[1420  353]\n",
            " [ 298  577]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.80      0.81      1773\n",
            "           1       0.62      0.66      0.64       875\n",
            "\n",
            "   micro avg       0.75      0.75      0.75      2648\n",
            "   macro avg       0.72      0.73      0.73      2648\n",
            "weighted avg       0.76      0.75      0.76      2648\n",
            "\n",
            "Kappa: 0.4531\n",
            "Epoch: 14\n",
            "Iteration 0, loss = 0.1246\n",
            "Iteration 50, loss = 0.1631\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2017 / 2648 correct (76.17)\n",
            "[[1472  301]\n",
            " [ 330  545]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.83      0.82      1773\n",
            "           1       0.64      0.62      0.63       875\n",
            "\n",
            "   micro avg       0.76      0.76      0.76      2648\n",
            "   macro avg       0.73      0.73      0.73      2648\n",
            "weighted avg       0.76      0.76      0.76      2648\n",
            "\n",
            "Kappa: 0.4569\n",
            "Epoch: 15\n",
            "Iteration 0, loss = 0.1696\n",
            "Iteration 50, loss = 0.2423\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1962 / 2648 correct (74.09)\n",
            "[[1377  396]\n",
            " [ 290  585]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.78      0.80      1773\n",
            "           1       0.60      0.67      0.63       875\n",
            "\n",
            "   micro avg       0.74      0.74      0.74      2648\n",
            "   macro avg       0.71      0.72      0.72      2648\n",
            "weighted avg       0.75      0.74      0.74      2648\n",
            "\n",
            "Kappa: 0.4320\n",
            "Epoch: 16\n",
            "Iteration 0, loss = 0.2211\n",
            "Iteration 50, loss = 0.1118\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1943 / 2648 correct (73.38)\n",
            "[[1347  426]\n",
            " [ 279  596]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.76      0.79      1773\n",
            "           1       0.58      0.68      0.63       875\n",
            "\n",
            "   micro avg       0.73      0.73      0.73      2648\n",
            "   macro avg       0.71      0.72      0.71      2648\n",
            "weighted avg       0.75      0.73      0.74      2648\n",
            "\n",
            "Kappa: 0.4229\n",
            "Epoch: 17\n",
            "Iteration 0, loss = 0.1926\n",
            "Iteration 50, loss = 0.2278\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1974 / 2648 correct (74.55)\n",
            "[[1401  372]\n",
            " [ 302  573]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.79      0.81      1773\n",
            "           1       0.61      0.65      0.63       875\n",
            "\n",
            "   micro avg       0.75      0.75      0.75      2648\n",
            "   macro avg       0.71      0.72      0.72      2648\n",
            "weighted avg       0.75      0.75      0.75      2648\n",
            "\n",
            "Kappa: 0.4362\n",
            "Epoch: 18\n",
            "Iteration 0, loss = 0.0804\n",
            "Iteration 50, loss = 0.0921\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 2001 / 2648 correct (75.57)\n",
            "[[1455  318]\n",
            " [ 329  546]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.82      0.82      1773\n",
            "           1       0.63      0.62      0.63       875\n",
            "\n",
            "   micro avg       0.76      0.76      0.76      2648\n",
            "   macro avg       0.72      0.72      0.72      2648\n",
            "weighted avg       0.75      0.76      0.76      2648\n",
            "\n",
            "Kappa: 0.4461\n",
            "Epoch: 19\n",
            "Iteration 0, loss = 0.1604\n",
            "Iteration 50, loss = 0.0718\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 1996 / 2648 correct (75.38)\n",
            "[[1458  315]\n",
            " [ 337  538]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.82      0.82      1773\n",
            "           1       0.63      0.61      0.62       875\n",
            "\n",
            "   micro avg       0.75      0.75      0.75      2648\n",
            "   macro avg       0.72      0.72      0.72      2648\n",
            "weighted avg       0.75      0.75      0.75      2648\n",
            "\n",
            "Kappa: 0.4400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-vCaaHLajE-0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2138
        },
        "outputId": "b3ccd6c1-bd2d-4d91-b65d-966c78f36ed8"
      },
      "cell_type": "code",
      "source": [
        "#Conv with Glove Deep\n",
        "embedding_dim = 200\n",
        "window_size = 3\n",
        "lr = 0.0001656545740852317\n",
        "weight_decay = 0.0054326444080709255\n",
        "out_channels = 128\n",
        "dropout = 0\n",
        "pos_weight = torch.tensor([1.5], device = device) #deals with unbalanced classes\n",
        "n_hidden = (64, 32, 16, 8, 4)\n",
        "sentiment = True\n",
        "\n",
        "set_seed()\n",
        "\n",
        "model = ClassifierGloVeDeep(TEXT.vocab, embedding_dim, window_size, \n",
        "                            out_channels, dropout, n_hidden = n_hidden, sentiment=sentiment)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr, weight_decay=weight_decay)\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight = pos_weight)\n",
        "\n",
        "\n",
        "t_losses, v_losses = train_helper('subtask_a', model, optimizer, loss_fn = loss_fn, epochs = 20, train_loader=train_iterator, valid_loader=valid_iterator)\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Iteration 0, loss = 0.8517\n",
            "Iteration 50, loss = 0.8314\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 875 / 2648 correct (33.04)\n",
            "[[   0 1773]\n",
            " [   0  875]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1773\n",
            "           1       0.33      1.00      0.50       875\n",
            "\n",
            "   micro avg       0.33      0.33      0.33      2648\n",
            "   macro avg       0.17      0.50      0.25      2648\n",
            "weighted avg       0.11      0.33      0.16      2648\n",
            "\n",
            "Kappa: 0.0000\n",
            "Epoch: 1\n",
            "Iteration 0, loss = 0.8323\n",
            "Iteration 50, loss = 0.8346\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 875 / 2648 correct (33.04)\n",
            "[[   0 1773]\n",
            " [   0  875]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1773\n",
            "           1       0.33      1.00      0.50       875\n",
            "\n",
            "   micro avg       0.33      0.33      0.33      2648\n",
            "   macro avg       0.17      0.50      0.25      2648\n",
            "weighted avg       0.11      0.33      0.16      2648\n",
            "\n",
            "Kappa: 0.0000\n",
            "Epoch: 2\n",
            "Iteration 0, loss = 0.8253\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-10de75bcdc97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mt_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'subtask_a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-6d1f6e0add18>\u001b[0m in \u001b[0;36mtrain_helper\u001b[0;34m(task_header, model, optimizer, train_loader, valid_loader, epochs, RNN, loss_fn, print_every, verbose, ret_optim_metric)\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0;31m#x = inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0;31m#y = targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0;31m# When using cross_entropy the targets need to have a shape (N,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-6fc0aa38ce57>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, ids)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#then add sentiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-b6005adaccfe>\u001b[0m in \u001b[0;36madd_sentiment\u001b[0;34m(h, ids)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#extract \"sentiment\" and \"subjectivity\" according to TextBlob:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0msentiments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubjectivities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sentiment_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tweet\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0msentiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2753\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mHanning\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2755\u001b[0;31m     \u001b[0mThe\u001b[0m \u001b[0mHanning\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtaper\u001b[0m \u001b[0mformed\u001b[0m \u001b[0mby\u001b[0m \u001b[0musing\u001b[0m \u001b[0ma\u001b[0m \u001b[0mweighted\u001b[0m \u001b[0mcosine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2757\u001b[0m     \u001b[0mParameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_vectorize_call\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2829\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2830\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2831\u001b[0;31m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2832\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2833\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Frequency response of the Hann window\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-b6005adaccfe>\u001b[0m in \u001b[0;36mget_sentiment\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;34m\"\"\"Gets sentiment and subjectivity of text\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0msentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0msubjectivity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubjectivity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubjectivity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/textblob/decorators.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/textblob/blob.py\u001b[0m in \u001b[0;36msentiment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnamedtuple\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mform\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mSentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolarity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubjectivity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \"\"\"\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcached_property\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/textblob/en/sentiments.py\u001b[0m in \u001b[0;36manalyze\u001b[0;34m(self, text, keep_assessments)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mSentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'polarity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'subjectivity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpattern_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/collections/__init__.py\u001b[0m in \u001b[0;36mnamedtuple\u001b[0;34m(typename, field_names, verbose, rename, module)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mfield_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0mnum_fields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0marg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m         repr_fmt = ', '.join(_repr_template.format(name=name)\n\u001b[1;32m    421\u001b[0m                              for name in field_names),\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oiXbnMDeath_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "fig1, ax1 = plt.subplots()\n",
        "\n",
        "ax1.plot(t_losses, label='Training')\n",
        "ax1.plot(v_losses, label='Validation')\n",
        "\n",
        "ax1.set_title('Losses')\n",
        "ax1.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mt1eluBC8rTo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part B"
      ]
    },
    {
      "metadata": {
        "id": "ljbIv5fj82GA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "7cfd1697-c27c-43ae-d41e-01d380e5a6cc"
      },
      "cell_type": "code",
      "source": [
        "TOKENIZE_PARAMS_new = {\"lemmatize\": True, \n",
        "                   \"rem_punct\": True, #remove punctuation\n",
        "                   \"rem_stopwords\": False}\n",
        "\n",
        "#Initialize tokenizer function with these parameters \n",
        "def tokenizer_local(text):\n",
        "    \"\"\"wrapper for tokenize so it can be used in torchtext Field creation. It takes the \n",
        "    current global varaiable TOKENIZE_PARAMS_LCL\"\"\"\n",
        "    try:\n",
        "        params = TOKENIZE_PARAMS_new\n",
        "    except NameError:\n",
        "        print(\"You must initialize the global variable TOKENIZE_PARAMS_LCL\")\n",
        "        raise NameError\n",
        "    return tokenize_params(text, params)\n",
        "\n",
        "TEXT = data.Field(sequential=True, tokenize=tokenizer_local, lower=True, batch_first = True)\n",
        "LABEL = data.LabelField(sequential=False, use_vocab=True, batch_first = True)\n",
        "ID = data.LabelField(sequential=False, use_vocab=False, batch_first=True)\n",
        "\n",
        "data_fields = [('id', ID), \n",
        "               ('tweet', TEXT),\n",
        "               ('subtask_a',LABEL),\n",
        "               ('subtask_b',LABEL),\n",
        "               ('subtask_c',LABEL)]\n",
        "\n",
        "set_seed()\n",
        "\n",
        "#Select data that does not have subtask_a == \"OFF\":\n",
        "train = data.TabularDataset(train_fp, format='TSV', fields = \n",
        "                            data_fields, skip_header=True, filter_pred=lambda d: d.subtask_a == 'OFF')\n",
        "\n",
        "train, valid = train.split(split_ratio=0.8, random_state=random.seed(SEED))\n",
        "\n",
        "print(f'Train size: {len(train)}')\n",
        "print(f'Validation size: {len(valid)}')\n",
        "\n",
        "#Now build vocab (using only the training set)\n",
        "TEXT.build_vocab(train, vectors='glove.twitter.27B.200d') #USE \"glove.840B.300d\" or glove.twitter.27B.200d\n",
        "\n",
        "LABEL.build_vocab(train.subtask_b)\n",
        "\n",
        "output_dim = len(LABEL.vocab)\n",
        "\n",
        "print(LABEL.vocab.stoi)\n",
        "\n",
        "#Create iterators\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid),\n",
        "                        batch_sizes=(BATCH_SIZE, len(valid)),  \n",
        "                        sort_key=lambda x: len(x.tweet), device=device)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 3520\n",
            "Validation size: 880\n",
            "defaultdict(<function _default_unk_index at 0x7f2156c0d8c8>, {'TIN': 0, 'UNT': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9KV2Yzy28xpD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17115
        },
        "outputId": "52536667-4c2f-4810-d7da-8acf9607d96b"
      },
      "cell_type": "code",
      "source": [
        "#Conv with Glove Deep\n",
        "embedding_dim = 200\n",
        "window_size = 3\n",
        "lr = 0.0001656545740852317\n",
        "weight_decay = 0.0054326444080709255\n",
        "out_channels = (128, 256)\n",
        "dropout = 0\n",
        "pos_weight = torch.tensor([6.8], device = device) #deals with unbalanced classes\n",
        "n_hidden = (64, 32, 16, 8, 4)\n",
        "sentiment = True\n",
        "\n",
        "set_seed()\n",
        "\n",
        "model_B = ClassifierGloVeDeepMultiConv(TEXT.vocab, embedding_dim, window_size = window_size, \n",
        "                            out_channels=out_channels, dropout=dropout, n_hidden = n_hidden, sentiment=sentiment)\n",
        "\n",
        "optimizer = optim.Adam(model_B.parameters(), lr, weight_decay=weight_decay)\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight = pos_weight)\n",
        "\n",
        "\n",
        "t_losses, v_losses = train_helper('subtask_b', model_B, optimizer, loss_fn = loss_fn, epochs = 10, \n",
        "                                  train_loader=train_iterator, valid_loader=valid_iterator)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "embedded torch.Size([128, 1, 70, 200])\n",
            "h torch.Size([128, 256, 66])\n",
            "h2 torch.Size([128, 256])\n",
            "Iteration 0, loss = 1.6851\n",
            "embedded torch.Size([128, 1, 83, 200])\n",
            "h torch.Size([128, 256, 79])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 103, 200])\n",
            "h torch.Size([128, 256, 99])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 61, 200])\n",
            "h torch.Size([128, 256, 57])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 74, 200])\n",
            "h torch.Size([128, 256, 70])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 59, 200])\n",
            "h torch.Size([128, 256, 55])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 62, 200])\n",
            "h torch.Size([128, 256, 58])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 107, 200])\n",
            "h torch.Size([128, 256, 103])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 61, 200])\n",
            "h torch.Size([128, 256, 57])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 58, 200])\n",
            "h torch.Size([128, 256, 54])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([64, 1, 56, 200])\n",
            "h torch.Size([64, 256, 52])\n",
            "h2 torch.Size([64, 256])\n",
            "embedded torch.Size([128, 1, 58, 200])\n",
            "h torch.Size([128, 256, 54])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 61, 200])\n",
            "h torch.Size([128, 256, 57])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 69, 200])\n",
            "h torch.Size([128, 256, 65])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 68, 200])\n",
            "h torch.Size([128, 256, 64])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 97, 200])\n",
            "h torch.Size([128, 256, 93])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 70, 200])\n",
            "h torch.Size([128, 256, 66])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 58, 200])\n",
            "h torch.Size([128, 256, 54])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 65, 200])\n",
            "h torch.Size([128, 256, 61])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 85, 200])\n",
            "h torch.Size([128, 256, 81])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 65, 200])\n",
            "h torch.Size([128, 256, 61])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 63, 200])\n",
            "h torch.Size([128, 256, 59])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 70, 200])\n",
            "h torch.Size([128, 256, 66])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 97, 200])\n",
            "h torch.Size([128, 256, 93])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 58, 200])\n",
            "h torch.Size([128, 256, 54])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 58, 200])\n",
            "h torch.Size([128, 256, 54])\n",
            "h2 torch.Size([128, 256])\n",
            "\n",
            "Validation Accuracy:\n",
            "embedded torch.Size([880, 1, 83, 200])\n",
            "h torch.Size([880, 256, 79])\n",
            "h2 torch.Size([880, 256])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 768 / 880 correct (87.27)\n",
            "[[768   0]\n",
            " [112   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      1.00      0.93       768\n",
            "           1       0.00      0.00      0.00       112\n",
            "\n",
            "   micro avg       0.87      0.87      0.87       880\n",
            "   macro avg       0.44      0.50      0.47       880\n",
            "weighted avg       0.76      0.87      0.81       880\n",
            "\n",
            "Kappa: 0.0000\n",
            "embedded torch.Size([880, 1, 83, 200])\n",
            "h torch.Size([880, 256, 79])\n",
            "h2 torch.Size([880, 256])\n",
            "Epoch: 1\n",
            "embedded torch.Size([128, 1, 59, 200])\n",
            "h torch.Size([128, 256, 55])\n",
            "h2 torch.Size([128, 256])\n",
            "Iteration 0, loss = 1.3418\n",
            "embedded torch.Size([128, 1, 61, 200])\n",
            "h torch.Size([128, 256, 57])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 63, 200])\n",
            "h torch.Size([128, 256, 59])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 73, 200])\n",
            "h torch.Size([128, 256, 69])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 79, 200])\n",
            "h torch.Size([128, 256, 75])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 97, 200])\n",
            "h torch.Size([128, 256, 93])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 107, 200])\n",
            "h torch.Size([128, 256, 103])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 62, 200])\n",
            "h torch.Size([128, 256, 58])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 65, 200])\n",
            "h torch.Size([128, 256, 61])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 70, 200])\n",
            "h torch.Size([128, 256, 66])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 60, 200])\n",
            "h torch.Size([128, 256, 56])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 70, 200])\n",
            "h torch.Size([128, 256, 66])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 63, 200])\n",
            "h torch.Size([128, 256, 59])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 103, 200])\n",
            "h torch.Size([128, 256, 99])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 83, 200])\n",
            "h torch.Size([128, 256, 79])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 63, 200])\n",
            "h torch.Size([128, 256, 59])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 58, 200])\n",
            "h torch.Size([128, 256, 54])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 57, 200])\n",
            "h torch.Size([128, 256, 53])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 70, 200])\n",
            "h torch.Size([128, 256, 66])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 69, 200])\n",
            "h torch.Size([128, 256, 65])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 85, 200])\n",
            "h torch.Size([128, 256, 81])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 61, 200])\n",
            "h torch.Size([128, 256, 57])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([64, 1, 97, 200])\n",
            "h torch.Size([64, 256, 93])\n",
            "h2 torch.Size([64, 256])\n",
            "embedded torch.Size([128, 1, 65, 200])\n",
            "h torch.Size([128, 256, 61])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 97, 200])\n",
            "h torch.Size([128, 256, 93])\n",
            "h2 torch.Size([128, 256])\n",
            "\n",
            "Validation Accuracy:\n",
            "embedded torch.Size([880, 1, 83, 200])\n",
            "h torch.Size([880, 256, 79])\n",
            "h2 torch.Size([880, 256])\n",
            "Got 767 / 880 correct (87.16)\n",
            "[[766   2]\n",
            " [111   1]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      1.00      0.93       768\n",
            "           1       0.33      0.01      0.02       112\n",
            "\n",
            "   micro avg       0.87      0.87      0.87       880\n",
            "   macro avg       0.60      0.50      0.47       880\n",
            "weighted avg       0.80      0.87      0.81       880\n",
            "\n",
            "Kappa: 0.0108\n",
            "embedded torch.Size([880, 1, 83, 200])\n",
            "h torch.Size([880, 256, 79])\n",
            "h2 torch.Size([880, 256])\n",
            "Epoch: 2\n",
            "embedded torch.Size([128, 1, 97, 200])\n",
            "h torch.Size([128, 256, 93])\n",
            "h2 torch.Size([128, 256])\n",
            "Iteration 0, loss = 1.2065\n",
            "embedded torch.Size([128, 1, 65, 200])\n",
            "h torch.Size([128, 256, 61])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 70, 200])\n",
            "h torch.Size([128, 256, 66])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 59, 200])\n",
            "h torch.Size([128, 256, 55])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 97, 200])\n",
            "h torch.Size([128, 256, 93])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 65, 200])\n",
            "h torch.Size([128, 256, 61])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 63, 200])\n",
            "h torch.Size([128, 256, 59])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 62, 200])\n",
            "h torch.Size([128, 256, 58])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([64, 1, 70, 200])\n",
            "h torch.Size([64, 256, 66])\n",
            "h2 torch.Size([64, 256])\n",
            "embedded torch.Size([128, 1, 103, 200])\n",
            "h torch.Size([128, 256, 99])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 83, 200])\n",
            "h torch.Size([128, 256, 79])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 97, 200])\n",
            "h torch.Size([128, 256, 93])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 59, 200])\n",
            "h torch.Size([128, 256, 55])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 85, 200])\n",
            "h torch.Size([128, 256, 81])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 73, 200])\n",
            "h torch.Size([128, 256, 69])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 69, 200])\n",
            "h torch.Size([128, 256, 65])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 107, 200])\n",
            "h torch.Size([128, 256, 103])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 58, 200])\n",
            "h torch.Size([128, 256, 54])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 68, 200])\n",
            "h torch.Size([128, 256, 64])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 60, 200])\n",
            "h torch.Size([128, 256, 56])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 79, 200])\n",
            "h torch.Size([128, 256, 75])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 61, 200])\n",
            "h torch.Size([128, 256, 57])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 58, 200])\n",
            "h torch.Size([128, 256, 54])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 74, 200])\n",
            "h torch.Size([128, 256, 70])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 60, 200])\n",
            "h torch.Size([128, 256, 56])\n",
            "h2 torch.Size([128, 256])\n",
            "\n",
            "Validation Accuracy:\n",
            "embedded torch.Size([880, 1, 83, 200])\n",
            "h torch.Size([880, 256, 79])\n",
            "h2 torch.Size([880, 256])\n",
            "Got 768 / 880 correct (87.27)\n",
            "[[766   2]\n",
            " [110   2]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      1.00      0.93       768\n",
            "           1       0.50      0.02      0.03       112\n",
            "\n",
            "   micro avg       0.87      0.87      0.87       880\n",
            "   macro avg       0.69      0.51      0.48       880\n",
            "weighted avg       0.83      0.87      0.82       880\n",
            "\n",
            "Kappa: 0.0259\n",
            "embedded torch.Size([880, 1, 83, 200])\n",
            "h torch.Size([880, 256, 79])\n",
            "h2 torch.Size([880, 256])\n",
            "Epoch: 3\n",
            "embedded torch.Size([64, 1, 59, 200])\n",
            "h torch.Size([64, 256, 55])\n",
            "h2 torch.Size([64, 256])\n",
            "Iteration 0, loss = 0.8116\n",
            "embedded torch.Size([128, 1, 63, 200])\n",
            "h torch.Size([128, 256, 59])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 107, 200])\n",
            "h torch.Size([128, 256, 103])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 97, 200])\n",
            "h torch.Size([128, 256, 93])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 63, 200])\n",
            "h torch.Size([128, 256, 59])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 103, 200])\n",
            "h torch.Size([128, 256, 99])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 70, 200])\n",
            "h torch.Size([128, 256, 66])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 65, 200])\n",
            "h torch.Size([128, 256, 61])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 65, 200])\n",
            "h torch.Size([128, 256, 61])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 83, 200])\n",
            "h torch.Size([128, 256, 79])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 97, 200])\n",
            "h torch.Size([128, 256, 93])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 63, 200])\n",
            "h torch.Size([128, 256, 59])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 74, 200])\n",
            "h torch.Size([128, 256, 70])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 85, 200])\n",
            "h torch.Size([128, 256, 81])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 62, 200])\n",
            "h torch.Size([128, 256, 58])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 60, 200])\n",
            "h torch.Size([128, 256, 56])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 79, 200])\n",
            "h torch.Size([128, 256, 75])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 59, 200])\n",
            "h torch.Size([128, 256, 55])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 62, 200])\n",
            "h torch.Size([128, 256, 58])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 97, 200])\n",
            "h torch.Size([128, 256, 93])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 59, 200])\n",
            "h torch.Size([128, 256, 55])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 70, 200])\n",
            "h torch.Size([128, 256, 66])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 73, 200])\n",
            "h torch.Size([128, 256, 69])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 59, 200])\n",
            "h torch.Size([128, 256, 55])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 57, 200])\n",
            "h torch.Size([128, 256, 53])\n",
            "h2 torch.Size([128, 256])\n",
            "\n",
            "Validation Accuracy:\n",
            "embedded torch.Size([880, 1, 83, 200])\n",
            "h torch.Size([880, 256, 79])\n",
            "h2 torch.Size([880, 256])\n",
            "Got 743 / 880 correct (84.43)\n",
            "[[718  50]\n",
            " [ 87  25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.93      0.91       768\n",
            "           1       0.33      0.22      0.27       112\n",
            "\n",
            "   micro avg       0.84      0.84      0.84       880\n",
            "   macro avg       0.61      0.58      0.59       880\n",
            "weighted avg       0.82      0.84      0.83       880\n",
            "\n",
            "Kappa: 0.1841\n",
            "embedded torch.Size([880, 1, 83, 200])\n",
            "h torch.Size([880, 256, 79])\n",
            "h2 torch.Size([880, 256])\n",
            "Epoch: 4\n",
            "embedded torch.Size([128, 1, 58, 200])\n",
            "h torch.Size([128, 256, 54])\n",
            "h2 torch.Size([128, 256])\n",
            "Iteration 0, loss = 1.0714\n",
            "embedded torch.Size([128, 1, 61, 200])\n",
            "h torch.Size([128, 256, 57])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 56, 200])\n",
            "h torch.Size([128, 256, 52])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 69, 200])\n",
            "h torch.Size([128, 256, 65])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 59, 200])\n",
            "h torch.Size([128, 256, 55])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 63, 200])\n",
            "h torch.Size([128, 256, 59])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 59, 200])\n",
            "h torch.Size([128, 256, 55])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 70, 200])\n",
            "h torch.Size([128, 256, 66])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 57, 200])\n",
            "h torch.Size([128, 256, 53])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 57, 200])\n",
            "h torch.Size([128, 256, 53])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 65, 200])\n",
            "h torch.Size([128, 256, 61])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 70, 200])\n",
            "h torch.Size([128, 256, 66])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 63, 200])\n",
            "h torch.Size([128, 256, 59])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 85, 200])\n",
            "h torch.Size([128, 256, 81])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 59, 200])\n",
            "h torch.Size([128, 256, 55])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 73, 200])\n",
            "h torch.Size([128, 256, 69])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 74, 200])\n",
            "h torch.Size([128, 256, 70])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 79, 200])\n",
            "h torch.Size([128, 256, 75])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 97, 200])\n",
            "h torch.Size([128, 256, 93])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 83, 200])\n",
            "h torch.Size([128, 256, 79])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([64, 1, 58, 200])\n",
            "h torch.Size([64, 256, 54])\n",
            "h2 torch.Size([64, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 97, 200])\n",
            "h torch.Size([128, 256, 93])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 107, 200])\n",
            "h torch.Size([128, 256, 103])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 103, 200])\n",
            "h torch.Size([128, 256, 99])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 62, 200])\n",
            "h torch.Size([128, 256, 58])\n",
            "h2 torch.Size([128, 256])\n",
            "\n",
            "Validation Accuracy:\n",
            "embedded torch.Size([880, 1, 83, 200])\n",
            "h torch.Size([880, 256, 79])\n",
            "h2 torch.Size([880, 256])\n",
            "Got 735 / 880 correct (83.52)\n",
            "[[700  68]\n",
            " [ 77  35]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.91      0.91       768\n",
            "           1       0.34      0.31      0.33       112\n",
            "\n",
            "   micro avg       0.84      0.84      0.84       880\n",
            "   macro avg       0.62      0.61      0.62       880\n",
            "weighted avg       0.83      0.84      0.83       880\n",
            "\n",
            "Kappa: 0.2319\n",
            "embedded torch.Size([880, 1, 83, 200])\n",
            "h torch.Size([880, 256, 79])\n",
            "h2 torch.Size([880, 256])\n",
            "Epoch: 5\n",
            "embedded torch.Size([128, 1, 85, 200])\n",
            "h torch.Size([128, 256, 81])\n",
            "h2 torch.Size([128, 256])\n",
            "Iteration 0, loss = 1.0100\n",
            "embedded torch.Size([128, 1, 63, 200])\n",
            "h torch.Size([128, 256, 59])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 69, 200])\n",
            "h torch.Size([128, 256, 65])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 107, 200])\n",
            "h torch.Size([128, 256, 103])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 73, 200])\n",
            "h torch.Size([128, 256, 69])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 63, 200])\n",
            "h torch.Size([128, 256, 59])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 58, 200])\n",
            "h torch.Size([128, 256, 54])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 70, 200])\n",
            "h torch.Size([128, 256, 66])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 79, 200])\n",
            "h torch.Size([128, 256, 75])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 97, 200])\n",
            "h torch.Size([128, 256, 93])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 58, 200])\n",
            "h torch.Size([128, 256, 54])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 103, 200])\n",
            "h torch.Size([128, 256, 99])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 61, 200])\n",
            "h torch.Size([128, 256, 57])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 62, 200])\n",
            "h torch.Size([128, 256, 58])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 65, 200])\n",
            "h torch.Size([128, 256, 61])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 83, 200])\n",
            "h torch.Size([128, 256, 79])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 97, 200])\n",
            "h torch.Size([128, 256, 93])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 58, 200])\n",
            "h torch.Size([128, 256, 54])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 97, 200])\n",
            "h torch.Size([128, 256, 93])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 68, 200])\n",
            "h torch.Size([128, 256, 64])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 63, 200])\n",
            "h torch.Size([128, 256, 59])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 63, 200])\n",
            "h torch.Size([128, 256, 59])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 60, 200])\n",
            "h torch.Size([128, 256, 56])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 65, 200])\n",
            "h torch.Size([128, 256, 61])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([64, 1, 62, 200])\n",
            "h torch.Size([64, 256, 58])\n",
            "h2 torch.Size([64, 256])\n",
            "\n",
            "Validation Accuracy:\n",
            "embedded torch.Size([880, 1, 83, 200])\n",
            "h torch.Size([880, 256, 79])\n",
            "h2 torch.Size([880, 256])\n",
            "Got 661 / 880 correct (75.11)\n",
            "[[613 155]\n",
            " [ 64  48]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.80      0.85       768\n",
            "           1       0.24      0.43      0.30       112\n",
            "\n",
            "   micro avg       0.75      0.75      0.75       880\n",
            "   macro avg       0.57      0.61      0.58       880\n",
            "weighted avg       0.82      0.75      0.78       880\n",
            "\n",
            "Kappa: 0.1683\n",
            "embedded torch.Size([880, 1, 83, 200])\n",
            "h torch.Size([880, 256, 79])\n",
            "h2 torch.Size([880, 256])\n",
            "Epoch: 6\n",
            "embedded torch.Size([128, 1, 68, 200])\n",
            "h torch.Size([128, 256, 64])\n",
            "h2 torch.Size([128, 256])\n",
            "Iteration 0, loss = 1.0969\n",
            "embedded torch.Size([128, 1, 83, 200])\n",
            "h torch.Size([128, 256, 79])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 59, 200])\n",
            "h torch.Size([128, 256, 55])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 65, 200])\n",
            "h torch.Size([128, 256, 61])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 79, 200])\n",
            "h torch.Size([128, 256, 75])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 58, 200])\n",
            "h torch.Size([128, 256, 54])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 70, 200])\n",
            "h torch.Size([128, 256, 66])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 60, 200])\n",
            "h torch.Size([128, 256, 56])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 59, 200])\n",
            "h torch.Size([128, 256, 55])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 107, 200])\n",
            "h torch.Size([128, 256, 103])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 61, 200])\n",
            "h torch.Size([128, 256, 57])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 60, 200])\n",
            "h torch.Size([128, 256, 56])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 73, 200])\n",
            "h torch.Size([128, 256, 69])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 85, 200])\n",
            "h torch.Size([128, 256, 81])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 97, 200])\n",
            "h torch.Size([128, 256, 93])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 103, 200])\n",
            "h torch.Size([128, 256, 99])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([64, 1, 69, 200])\n",
            "h torch.Size([64, 256, 65])\n",
            "h2 torch.Size([64, 256])\n",
            "embedded torch.Size([128, 1, 63, 200])\n",
            "h torch.Size([128, 256, 59])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 97, 200])\n",
            "h torch.Size([128, 256, 93])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 60, 200])\n",
            "h torch.Size([128, 256, 56])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 70, 200])\n",
            "h torch.Size([128, 256, 66])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 60, 200])\n",
            "h torch.Size([128, 256, 56])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 63, 200])\n",
            "h torch.Size([128, 256, 59])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 58, 200])\n",
            "h torch.Size([128, 256, 54])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 74, 200])\n",
            "h torch.Size([128, 256, 70])\n",
            "h2 torch.Size([128, 256])\n",
            "\n",
            "Validation Accuracy:\n",
            "embedded torch.Size([880, 1, 83, 200])\n",
            "h torch.Size([880, 256, 79])\n",
            "h2 torch.Size([880, 256])\n",
            "Got 688 / 880 correct (78.18)\n",
            "[[638 130]\n",
            " [ 62  50]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.83      0.87       768\n",
            "           1       0.28      0.45      0.34       112\n",
            "\n",
            "   micro avg       0.78      0.78      0.78       880\n",
            "   macro avg       0.59      0.64      0.61       880\n",
            "weighted avg       0.83      0.78      0.80       880\n",
            "\n",
            "Kappa: 0.2201\n",
            "embedded torch.Size([880, 1, 83, 200])\n",
            "h torch.Size([880, 256, 79])\n",
            "h2 torch.Size([880, 256])\n",
            "Epoch: 7\n",
            "embedded torch.Size([128, 1, 70, 200])\n",
            "h torch.Size([128, 256, 66])\n",
            "h2 torch.Size([128, 256])\n",
            "Iteration 0, loss = 0.9519\n",
            "embedded torch.Size([64, 1, 73, 200])\n",
            "h torch.Size([64, 256, 69])\n",
            "h2 torch.Size([64, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 60, 200])\n",
            "h torch.Size([128, 256, 56])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 70, 200])\n",
            "h torch.Size([128, 256, 66])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 83, 200])\n",
            "h torch.Size([128, 256, 79])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 65, 200])\n",
            "h torch.Size([128, 256, 61])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 79, 200])\n",
            "h torch.Size([128, 256, 75])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 58, 200])\n",
            "h torch.Size([128, 256, 54])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 61, 200])\n",
            "h torch.Size([128, 256, 57])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 63, 200])\n",
            "h torch.Size([128, 256, 59])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 58, 200])\n",
            "h torch.Size([128, 256, 54])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 59, 200])\n",
            "h torch.Size([128, 256, 55])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 63, 200])\n",
            "h torch.Size([128, 256, 59])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 97, 200])\n",
            "h torch.Size([128, 256, 93])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 63, 200])\n",
            "h torch.Size([128, 256, 59])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 58, 200])\n",
            "h torch.Size([128, 256, 54])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 107, 200])\n",
            "h torch.Size([128, 256, 103])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 61, 200])\n",
            "h torch.Size([128, 256, 57])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 97, 200])\n",
            "h torch.Size([128, 256, 93])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 74, 200])\n",
            "h torch.Size([128, 256, 70])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 62, 200])\n",
            "h torch.Size([128, 256, 58])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 103, 200])\n",
            "h torch.Size([128, 256, 99])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 60, 200])\n",
            "h torch.Size([128, 256, 56])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 65, 200])\n",
            "h torch.Size([128, 256, 61])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 61, 200])\n",
            "h torch.Size([128, 256, 57])\n",
            "h2 torch.Size([128, 256])\n",
            "\n",
            "Validation Accuracy:\n",
            "embedded torch.Size([880, 1, 83, 200])\n",
            "h torch.Size([880, 256, 79])\n",
            "h2 torch.Size([880, 256])\n",
            "Got 527 / 880 correct (59.89)\n",
            "[[448 320]\n",
            " [ 33  79]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.58      0.72       768\n",
            "           1       0.20      0.71      0.31       112\n",
            "\n",
            "   micro avg       0.60      0.60      0.60       880\n",
            "   macro avg       0.56      0.64      0.51       880\n",
            "weighted avg       0.84      0.60      0.67       880\n",
            "\n",
            "Kappa: 0.1378\n",
            "embedded torch.Size([880, 1, 83, 200])\n",
            "h torch.Size([880, 256, 79])\n",
            "h2 torch.Size([880, 256])\n",
            "Epoch: 8\n",
            "embedded torch.Size([128, 1, 63, 200])\n",
            "h torch.Size([128, 256, 59])\n",
            "h2 torch.Size([128, 256])\n",
            "Iteration 0, loss = 1.0473\n",
            "embedded torch.Size([128, 1, 63, 200])\n",
            "h torch.Size([128, 256, 59])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 63, 200])\n",
            "h torch.Size([128, 256, 59])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([64, 1, 64, 200])\n",
            "h torch.Size([64, 256, 60])\n",
            "h2 torch.Size([64, 256])\n",
            "embedded torch.Size([128, 1, 59, 200])\n",
            "h torch.Size([128, 256, 55])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 58, 200])\n",
            "h torch.Size([128, 256, 54])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 107, 200])\n",
            "h torch.Size([128, 256, 103])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 60, 200])\n",
            "h torch.Size([128, 256, 56])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 59, 200])\n",
            "h torch.Size([128, 256, 55])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 65, 200])\n",
            "h torch.Size([128, 256, 61])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 97, 200])\n",
            "h torch.Size([128, 256, 93])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 103, 200])\n",
            "h torch.Size([128, 256, 99])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 74, 200])\n",
            "h torch.Size([128, 256, 70])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 68, 200])\n",
            "h torch.Size([128, 256, 64])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 68, 200])\n",
            "h torch.Size([128, 256, 64])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 85, 200])\n",
            "h torch.Size([128, 256, 81])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 61, 200])\n",
            "h torch.Size([128, 256, 57])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 63, 200])\n",
            "h torch.Size([128, 256, 59])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 83, 200])\n",
            "h torch.Size([128, 256, 79])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 60, 200])\n",
            "h torch.Size([128, 256, 56])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 97, 200])\n",
            "h torch.Size([128, 256, 93])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 58, 200])\n",
            "h torch.Size([128, 256, 54])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 97, 200])\n",
            "h torch.Size([128, 256, 93])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 58, 200])\n",
            "h torch.Size([128, 256, 54])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 70, 200])\n",
            "h torch.Size([128, 256, 66])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 73, 200])\n",
            "h torch.Size([128, 256, 69])\n",
            "h2 torch.Size([128, 256])\n",
            "\n",
            "Validation Accuracy:\n",
            "embedded torch.Size([880, 1, 83, 200])\n",
            "h torch.Size([880, 256, 79])\n",
            "h2 torch.Size([880, 256])\n",
            "Got 665 / 880 correct (75.57)\n",
            "[[610 158]\n",
            " [ 57  55]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.79      0.85       768\n",
            "           1       0.26      0.49      0.34       112\n",
            "\n",
            "   micro avg       0.76      0.76      0.76       880\n",
            "   macro avg       0.59      0.64      0.59       880\n",
            "weighted avg       0.83      0.76      0.79       880\n",
            "\n",
            "Kappa: 0.2060\n",
            "embedded torch.Size([880, 1, 83, 200])\n",
            "h torch.Size([880, 256, 79])\n",
            "h2 torch.Size([880, 256])\n",
            "Epoch: 9\n",
            "embedded torch.Size([128, 1, 83, 200])\n",
            "h torch.Size([128, 256, 79])\n",
            "h2 torch.Size([128, 256])\n",
            "Iteration 0, loss = 0.9556\n",
            "embedded torch.Size([128, 1, 56, 200])\n",
            "h torch.Size([128, 256, 52])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 56, 200])\n",
            "h torch.Size([128, 256, 52])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 97, 200])\n",
            "h torch.Size([128, 256, 93])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 103, 200])\n",
            "h torch.Size([128, 256, 99])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 68, 200])\n",
            "h torch.Size([128, 256, 64])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 63, 200])\n",
            "h torch.Size([128, 256, 59])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 73, 200])\n",
            "h torch.Size([128, 256, 69])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 60, 200])\n",
            "h torch.Size([128, 256, 56])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 97, 200])\n",
            "h torch.Size([128, 256, 93])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 74, 200])\n",
            "h torch.Size([128, 256, 70])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 70, 200])\n",
            "h torch.Size([128, 256, 66])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 60, 200])\n",
            "h torch.Size([128, 256, 56])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 61, 200])\n",
            "h torch.Size([128, 256, 57])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 60, 200])\n",
            "h torch.Size([128, 256, 56])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 64, 200])\n",
            "h torch.Size([128, 256, 60])\n",
            "h2 torch.Size([128, 256])\n",
            "embedded torch.Size([128, 1, 97, 200])\n",
            "h torch.Size([128, 256, 93])\n",
            "h2 torch.Size([128, 256])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kgzCmb7O9LmJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "0514707b-8827-460c-f625-e92bd895d632"
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "fig1, ax1 = plt.subplots()\n",
        "\n",
        "ax1.plot(t_losses, label='Training')\n",
        "ax1.plot(v_losses, label='Validation')\n",
        "\n",
        "ax1.set_title('Losses')\n",
        "ax1.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFZCAYAAACizedRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIjFqJxFREQsRuUsIiJiMSpnERERi/lf+6q9145r72YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "HzDUdTT8Agxo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Task C"
      ]
    },
    {
      "metadata": {
        "id": "Wf47-PIDAlIw",
        "colab_type": "code",
        "outputId": "8dc2a284-eb2a-4096-cd99-a9a3c13d8b2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "TOKENIZE_PARAMS_new = {\"lemmatize\": True, \n",
        "                   \"rem_punct\": True, #remove punctuation\n",
        "                   \"rem_stopwords\": False}\n",
        "\n",
        "#Initialize tokenizer function with these parameters \n",
        "def tokenizer_local(text):\n",
        "    \"\"\"wrapper for tokenize so it can be used in torchtext Field creation. It takes the \n",
        "    current global varaiable TOKENIZE_PARAMS_LCL\"\"\"\n",
        "    try:\n",
        "        params = TOKENIZE_PARAMS_new\n",
        "    except NameError:\n",
        "        print(\"You must initialize the global variable TOKENIZE_PARAMS_LCL\")\n",
        "        raise NameError\n",
        "    return tokenize_params(text, params)\n",
        "\n",
        "TEXT = data.Field(sequential=True, tokenize=tokenizer_local, lower=True, batch_first = True)\n",
        "LABEL = data.LabelField(sequential=False, use_vocab=True, batch_first = True)\n",
        "ID = data.LabelField(sequential=False, use_vocab=False, batch_first=True)\n",
        "\n",
        "data_fields = [('id', ID), \n",
        "               ('tweet', TEXT),\n",
        "               ('subtask_a',LABEL),\n",
        "               ('subtask_b',LABEL),\n",
        "               ('subtask_c',LABEL)]\n",
        "\n",
        "set_seed()\n",
        "\n",
        "\n",
        "train = data.TabularDataset(train_fp, format='TSV', fields = \n",
        "                            data_fields, skip_header=True, filter_pred=lambda d: d.subtask_a == 'OFF' and d.subtask_b == 'TIN')\n",
        "\n",
        "train, valid = train.split(split_ratio=0.8, random_state=random.seed(SEED))\n",
        "\n",
        "print(f'Train size: {len(train)}')\n",
        "print(f'Validation size: {len(valid)}')\n",
        "\n",
        "#Now build vocab (using only the training set)\n",
        "TEXT.build_vocab(train, vectors='glove.twitter.27B.200d') #USE \"glove.840B.300d\" or glove.twitter.27B.200d\n",
        "\n",
        "LABEL.build_vocab(train.subtask_c)\n",
        "\n",
        "output_dim = len(LABEL.vocab)\n",
        "\n",
        "print(LABEL.vocab.stoi)\n",
        "\n",
        "#Create iterators\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid),\n",
        "                        batch_sizes=(BATCH_SIZE, len(valid)),  \n",
        "                        sort_key=lambda x: len(x.tweet), device=device)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 3101\n",
            "Validation size: 775\n",
            "defaultdict(<function _default_unk_index at 0x7f2156c0d8c8>, {'IND': 0, 'GRP': 1, 'OTH': 2})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UTUbzJZBBBMr",
        "colab_type": "code",
        "outputId": "4ffdad62-fa93-4205-f8b1-2f00fa7b35f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3493
        }
      },
      "cell_type": "code",
      "source": [
        "#Conv with Glove Deep\n",
        "embedding_dim = 200\n",
        "window_size = 3\n",
        "num_classes = 3\n",
        "lr = 0.0001656545740852317\n",
        "weight_decay = 0.0054326444080709255\n",
        "out_channels = (128, 256)\n",
        "dropout = 0.5\n",
        "weight = torch.tensor([1.6, 3.7 ,8.4], device = device) #deals with unbalanced classes\n",
        "n_hidden = (64, 32, 16, 8, 4)\n",
        "\n",
        "sentiment = False\n",
        "\n",
        "set_seed()\n",
        "#vocab, embedding_dim, window_size, out_channels, dropout, num_classes=2, sentiment=False, n_hidden = 64\n",
        "model_C = ClassifierGloVeDeepMultiConv(TEXT.vocab, embedding_dim, window_size,\n",
        "                            out_channels= out_channels, dropout=dropout, num_classes= num_classes )\n",
        "\n",
        "optimizer = optim.Adam(model_C.parameters(), lr, weight_decay=weight_decay)\n",
        "loss_fn = nn.CrossEntropyLoss(weight = weight)\n",
        "\n",
        "\n",
        "t_losses, v_losses = train_helper('subtask_c', model_C, optimizer, loss_fn = loss_fn, epochs = 30, \n",
        "                                  train_loader=train_iterator, valid_loader=valid_iterator)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Iteration 0, loss = 4.4505\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 207 / 775 correct (26.71)\n",
            "[[  0 476   0]\n",
            " [  0 207   0]\n",
            " [  0  92   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       476\n",
            "           1       0.27      1.00      0.42       207\n",
            "           2       0.00      0.00      0.00        92\n",
            "\n",
            "   micro avg       0.27      0.27      0.27       775\n",
            "   macro avg       0.09      0.33      0.14       775\n",
            "weighted avg       0.07      0.27      0.11       775\n",
            "\n",
            "Kappa: 0.0000\n",
            "Epoch: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 0, loss = 2.5405\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 207 / 775 correct (26.71)\n",
            "[[  0 476   0]\n",
            " [  0 207   0]\n",
            " [  0  92   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       476\n",
            "           1       0.27      1.00      0.42       207\n",
            "           2       0.00      0.00      0.00        92\n",
            "\n",
            "   micro avg       0.27      0.27      0.27       775\n",
            "   macro avg       0.09      0.33      0.14       775\n",
            "weighted avg       0.07      0.27      0.11       775\n",
            "\n",
            "Kappa: 0.0000\n",
            "Epoch: 2\n",
            "Iteration 0, loss = 2.9536\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 207 / 775 correct (26.71)\n",
            "[[  0 476   0]\n",
            " [  0 207   0]\n",
            " [  0  92   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       476\n",
            "           1       0.27      1.00      0.42       207\n",
            "           2       0.00      0.00      0.00        92\n",
            "\n",
            "   micro avg       0.27      0.27      0.27       775\n",
            "   macro avg       0.09      0.33      0.14       775\n",
            "weighted avg       0.07      0.27      0.11       775\n",
            "\n",
            "Kappa: 0.0000\n",
            "Epoch: 3\n",
            "Iteration 0, loss = 2.0293\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 207 / 775 correct (26.71)\n",
            "[[  0 476   0]\n",
            " [  0 207   0]\n",
            " [  0  92   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       476\n",
            "           1       0.27      1.00      0.42       207\n",
            "           2       0.00      0.00      0.00        92\n",
            "\n",
            "   micro avg       0.27      0.27      0.27       775\n",
            "   macro avg       0.09      0.33      0.14       775\n",
            "weighted avg       0.07      0.27      0.11       775\n",
            "\n",
            "Kappa: 0.0000\n",
            "Epoch: 4\n",
            "Iteration 0, loss = 2.0435\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 207 / 775 correct (26.71)\n",
            "[[  0 476   0]\n",
            " [  0 207   0]\n",
            " [  0  92   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       476\n",
            "           1       0.27      1.00      0.42       207\n",
            "           2       0.00      0.00      0.00        92\n",
            "\n",
            "   micro avg       0.27      0.27      0.27       775\n",
            "   macro avg       0.09      0.33      0.14       775\n",
            "weighted avg       0.07      0.27      0.11       775\n",
            "\n",
            "Kappa: 0.0000\n",
            "Epoch: 5\n",
            "Iteration 0, loss = 1.9965\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 207 / 775 correct (26.71)\n",
            "[[  0 476   0]\n",
            " [  0 207   0]\n",
            " [  0  92   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       476\n",
            "           1       0.27      1.00      0.42       207\n",
            "           2       0.00      0.00      0.00        92\n",
            "\n",
            "   micro avg       0.27      0.27      0.27       775\n",
            "   macro avg       0.09      0.33      0.14       775\n",
            "weighted avg       0.07      0.27      0.11       775\n",
            "\n",
            "Kappa: 0.0000\n",
            "Epoch: 6\n",
            "Iteration 0, loss = 1.6126\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 207 / 775 correct (26.71)\n",
            "[[  0 476   0]\n",
            " [  0 207   0]\n",
            " [  0  92   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       476\n",
            "           1       0.27      1.00      0.42       207\n",
            "           2       0.00      0.00      0.00        92\n",
            "\n",
            "   micro avg       0.27      0.27      0.27       775\n",
            "   macro avg       0.09      0.33      0.14       775\n",
            "weighted avg       0.07      0.27      0.11       775\n",
            "\n",
            "Kappa: 0.0000\n",
            "Epoch: 7\n",
            "Iteration 0, loss = 1.0671\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 207 / 775 correct (26.71)\n",
            "[[  0 476   0]\n",
            " [  0 207   0]\n",
            " [  0  92   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       476\n",
            "           1       0.27      1.00      0.42       207\n",
            "           2       0.00      0.00      0.00        92\n",
            "\n",
            "   micro avg       0.27      0.27      0.27       775\n",
            "   macro avg       0.09      0.33      0.14       775\n",
            "weighted avg       0.07      0.27      0.11       775\n",
            "\n",
            "Kappa: 0.0000\n",
            "Epoch: 8\n",
            "Iteration 0, loss = 1.4385\n",
            "\n",
            "Validation Accuracy:\n",
            "Got 207 / 775 correct (26.71)\n",
            "[[  0 476   0]\n",
            " [  0 207   0]\n",
            " [  0  92   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       476\n",
            "           1       0.27      1.00      0.42       207\n",
            "           2       0.00      0.00      0.00        92\n",
            "\n",
            "   micro avg       0.27      0.27      0.27       775\n",
            "   macro avg       0.09      0.33      0.14       775\n",
            "weighted avg       0.07      0.27      0.11       775\n",
            "\n",
            "Kappa: 0.0000\n",
            "Epoch: 9\n",
            "Iteration 0, loss = 1.5190\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-fb6be7fd9942>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m t_losses, v_losses = train_helper('subtask_c', model_C, optimizer, loss_fn = loss_fn, epochs = 30, \n\u001b[0;32m---> 23\u001b[0;31m                                   train_loader=train_iterator, valid_loader=valid_iterator)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-6d1f6e0add18>\u001b[0m in \u001b[0;36mtrain_helper\u001b[0;34m(task_header, model, optimizer, train_loader, valid_loader, epochs, RNN, loss_fn, print_every, verbose, ret_optim_metric)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;31m# This is the backwards pass: compute the gradient of the loss with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# respect to each  parameter of the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0;31m# Actually update the parameters of the model using the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "TSbEtokDP1Dy",
        "colab_type": "code",
        "outputId": "10f39def-a9c1-4cd6-8cfa-1b3d5c781414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "fig1, ax1 = plt.subplots()\n",
        "\n",
        "ax1.plot(t_losses, label='Training')\n",
        "ax1.plot(v_losses, label='Validation')\n",
        "\n",
        "ax1.set_title('Losses')\n",
        "ax1.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFZCAYAAACv05cWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl8VPW9//HXrNlmss9km2yEJJCE\nACEgCLIEoqAoahFxAS1aa9Vqr+21an8Vq1WxVW+1y70t7oIrIq4FFYMCgkjYEiA7CUnIvpB9mcz8\n/ggEKAkBMslkJp+n8shkzpkzn08mmfecc77nHIXVarUihBBCiCGntHcBQgghxEglISyEEELYiYSw\nEEIIYScSwkIIIYSdSAgLIYQQdiIhLIQQQtiJhLAQw1xsbCzl5eX2LkMIMQgkhIUQQgg7Udu7ACHE\nxWlvb+epp57ihx9+QKlUMmvWLP77v/8blUrFmjVrWLt2LVarFZ1OxzPPPEN0dHSf9+fl5fH4449T\nVVWFVqvl6aefZty4cTQ3N/PQQw9RUFBAR0cH06ZNY+XKlWg0Gnu3L4RTkBAWwkG98cYblJeX8/nn\nn2M2m7n11lv57LPPmDt3Li+++CJpaWnodDr+/e9/s2XLFoKCgnq9PyoqinvvvZc777yTG264gfT0\ndO655x7S0tLYsGEDnp6e/Pvf/8ZsNvPkk0+Sl5fH2LFj7d2+EE5BQlgIB7VlyxZWrFiBWq1GrVZz\n9dVXs337dq688koUCgXr1q1j4cKFLFiwAIDOzs5e78/Ly6OmpobFixcDMGnSJHx9fdm7d2/P123b\ntjFlyhT+8Ic/2K1fIZyR7BMWwkHV1tbi5eXV872Xlxc1NTVoNBpef/119uzZwxVXXMHNN99MdnZ2\nn/c3NDTQ1tbGggULmD9/PvPnz6empob6+noWLFjA7bffzosvvsi0adP4wx/+QEdHhx27FsK5yJqw\nEA7K39+f+vr6nu/r6+vx9/cHIC4ujpdeeomOjg5efvllVq5cybvvvtvr/c899xweHh5s3Lix1+dZ\nunQpS5cupaKigl/+8pds2LCBJUuWDEmPQjg7WRMWwkHNnj2bdevW0dXVRUtLCx9//DGzZs0iOzub\n+++/n46ODrRaLQkJCSgUij7vDwkJITAwsCeEa2trefDBB2lpaeHvf/8769atAyAgIACTyYRCobBn\n20I4FVkTFsIBLFu2DJVK1fP9H//4R5YtW0ZxcTFXXXUVCoWC+fPn9+znNZlMLFy4EI1Gg4eHB489\n9hgxMTG93q9QKHjhhRd4/PHH+ctf/oJSqeSnP/0p7u7uLFq0iEceeYTVq1ejUCgYP348ixYtsteP\nQQino5DrCQshhBD2IZujhRBCCDuREBZCCCHsREJYCCGEsBMJYSGEEMJOJISFEEIIOxnyQ5Sqqhpt\nujwfH3fq6lpsuszhwBn7csaewDn7kp4chzP25Yw9GQz6Xu93+DVhtVrV/0wOyBn7csaewDn7kp4c\nhzP25Yw99cXhQ1gIIYRwVBLCQgghhJ1ICAshhBB2IiEshBBC2ImEsBBCCGEnEsJCCCGEnUgICyGE\nEHYi1xMWQggxrKxatYq9e/dTW1tDW1sbwcEheHp68fTTfz7n47744lM8PHTMmjWn1+kvvvg8N9yw\nlODgkMEo+6JICAshhBhWHn74YaqqGvnii08pKMjnvvt+dV6Pu/LKq885/YEHfm2L8mxKQlgIIcSw\nt2fPbt59dw0tLS3cd99/sXdvOlu2bMZisTBt2nRWrLiLV175J97e3kRGRrF+/fsoFEqKio4we/Zc\nVqy4i/vuu4sHH3yItLTNNDc3cfRoEaWlJdx//6+ZNm06a9a8ztdff0lwcAhms5mlS28hKSl5UPty\n6BDuNHfx9a6jxIV6oVHL7m0hhLC197/J48esSpsuc/IYI0tSRl/w4/Lz83jnnfVotVr27k3nH/94\nGaVSyZIli7jxxpvPmPfQoYO8/faHWCwWbrjhalasuOuM6ZWVFTz33Evs3Pk9H3/8IfHxCaxf/wHv\nvPMhzc3NLF16PUuX3jKgPs+HQ4dw5pFa/vphBtfPHMXCSyPsXY4QQohBNHp0NFqtFgBXV1fuu+8u\nVCoV9fX1NDQ0nDFvbOwYXF1d+1xWYuIEAIxGI01NTZSUFDNqVBQuLq64uLgydmz84DVyGocO4TFh\nPri5qEjbW8qCqWGolLI2LIQQtrQkZfRFrbUOBo1GA0B5eRnvvbeWV19di7u7O8uWLTlrXpXq3BeB\nOH261WrFagXlaRmiUNio6H44dGq5uahJSQ6jrrGdfbnV9i5HCCHEEKivr8fHxwd3d3eys7MoLy+n\ns7NzQMsMCgqioCAfs9lMXV0dWVmHbVTtuTl0CANcNT0SgM3pJXauRAghxFCIjo7Bzc2dX/xiBZs3\nf8miRdfz/PPPDmiZvr5+pKbO52c/W86LLz5HXFx8v2vTtqCwWq3WQX+W01RVNdp0eQaDnode+o7D\nRXU8cccUTAadTZdvLwaD3uY/K3tzxp7AOfuSnhyHM/Zlr56++OJTUlPno1KpWL58KS+88FeMxgCb\nLNtg0Pd6v8OvCQPMnWQC4Js9pXauRAghhKOqqanhrrtu4+67V3D55fNtFsDn4tADs04aP9oPP08X\nvs8sY/GsUbi7auxdkhBCCAezbNntLFt2+5A+p1OsCauUSuYkmejotLA9o9ze5QghhBDn5bxCOCcn\nh3nz5rFmzZqzppWVlXHTTTexePFiHnvsMZsXeL4uSwxCrVKyeU8JlqHdzS2EEEJclH5DuKWlhSef\nfJJp06b1On3VqlWsWLGCdevWoVKpOHbsmM2LPB96dy2XxBmprGvl4JFau9QghBBCXIh+Q1ir1bJ6\n9WqMRuNZ0ywWC+np6aSkpACwcuVKgoODbV/leTo5QEsOVxJCCOEI+g1htVrd56m/amtr8fDw4Jln\nnuGmm27i+eeft3mBFyIi0JOoYE8y8muorGuxay1CCCEuzo033njWyTL+7//+xjvvnL1LdM+e3fy/\n//cQAA8//OBZ0z/88D1eeeWffT5XXl4uR48WAbBy5SO0t7cNpPQLNqDR0VarlYqKCpYvX05ISAh3\n3XUXW7ZsYfbs2X0+xsfHHbXatgdAn3781bWzR/P823vYmVXFHdck2PR5hlpfx5U5MmfsCZyzL+nJ\ncThbXwsXLmTnzm+57LIpPfdt27aFN99886xevb3dcXHRYDDoeeWV1WctS6dzpbPTpc+f0bvvbich\nIQGDIYF//ONvtm3kPAwohH18fAgODiYsLAyAadOmkZube84QrrPxGup/HtQdG+KJp4eWL3cWcUWy\nCRfN4J/xZDDIAfiOwxn7kp4chzP2deWVV7JkyY3cfvvdAGRlHcbHx4/09AxefvleNBoNer2eJ55Y\nRX19C+3tnVRVNXLVVXP5/PPN7N69i5deeh5fXz/8/PwJDg6hrKyOp556nKqqSlpbW1mx4i4CA4N4\n++138Pb2Rql05bHHHuHNN9+jqamRZ555gs7OTpRKJQ8//HsUCgVPPfU4wcEh5OXlEhMTy8MP//68\ne+rrQ8CAQlitVhMaGkphYSEREREcPHiQq666aiCLHDC1Ssms8cF8+n0hOw+WM2tCiF3rEUIIR7Y+\n7zP2VmbYdJkTjeO4fvTCPqf7+fkRHBzCoUOZxMUl8M03X5GaOp/GxkZWrvwjwcEhPPnkY/zwww7c\n3d3Pevw///k3fv/7J4mOjuE3v7mf4OAQGhsbmDJlKgsWLKS0tITf//5hXn11DZdcMo3Zs+cSF3dq\ny+nLL/8fCxcuYu7cy0lL+5pXX/0Xd9zxc7KzD/OHPzyNj48v1113JY2Njej1A9sK0W8IZ2Zm8uyz\nz1JaWoparWbTpk2kpKRgMplITU3l0Ucf5eGHH8ZqtRITE9MzSMueZk8M4fMdRWxOL2Xm+GAUQ3U5\nDCGEEDaRmjqfzZu/Ii4uge3bv+N///dV8vJyePbZP9LV1cWxY6VMmjS51xAuKysjOjoGgAkTkmhv\nb0ev9+Tw4YN88sl6FAolDQ3H+3zu7OzD3H33fQAkJSXz+usvAxASEoqfnz8A/v4GmpubBj+EExIS\neOutt/qcHh4ezjvvvDOgImzNR+9CUqyB3VmV5JYcJybU294lCSGEQ7p+9MJzrrUOllmz5vDmm6+S\nmnoFoaFheHp68swzT/LnP/+FiIhIXnih7ws2nH5JwpOXR/jqq400NDTw97+/TENDA3feuewcz67o\neVxnpxmFont5/3lBB1tcesEpzpjVm7lJ3Zuh5XAlIYRwPO7uHkRFRfPmm6+RmjofgObmJgICAmls\nbGTPnvQ+L1/o72/g6NFCrFYre/emA92XPwwKCkapVPLtt9/0PFahUNDV1XXG48eOjWPPnt0A7NuX\nzpgxYwerTecN4ZhQb0wGHXtyqqhrbLd3OUIIIS5Qaup8fvzxB2bMmAnA9dffwC9+cQd/+tNT3HLL\nctaseZ2amrOvJX/XXffw//7fb/ntb/+r5yIMs2en8P33W3nggV/g5uaG0WjktddWM378RP7ylz+z\ne/eunsffeefdbNz4BffffzdffPEZd9zx80Hr0SkuZdjXMr/dV8obG7O5+tIIrps5yqbPO9icccSj\nM/YEztmX9OQ4nLEvZ+2pN067JgwwNS4Qdxc13+4rpdNssXc5QgghxBmcOoRdtCpmJAbR0NJJenal\nvcsRQgghzuDUIQwwJykEBbB5jwzQEkIIMbw4fQgH+LgzLsqP/NIGCssb7F2OEEII0cPpQxjk6kpC\nCCGGpxERwvGRvhh93PjhUCWNLR32LkcIIYQARkgIKxUKUpJMmLssbD1QZu9yhBBCCGCEhDDAjHGB\naDVK0vaUYLEM6aHRQgghRK9GTAi7u2q4ND6QmoZ29uedfYYVIYQQYqiNmBAGSEk6MUBLDlcSQggx\nDIyoEDYZdYwJ8+ZQYR3HqpvtXY4QQogRbkSFMJxaG/5G1oaFEELY2YgL4Ykx/vjoXdieWU5ru9ne\n5QghhBjBRlwIq5RKZk8Mob2ji+8zy+1djhBCiBFsxIUwwKzxwahVCjanlzDEV3IUQggheozIEPb0\n0DJ5TADltS0cKqyzdzlCCCFGqBEZwiDnkxZCCGF/IzaERwV7EhmkZ39eNdX1rfYuRwghxAg0YkMY\nug9XsgJpe0vtXYoQQogRaESH8JSxRnRuGr7bf4yOzi57lyOEEGKEGdEhrFGrmDUhmOY2Mz8crrB3\nOUIIIUaYER3CAHMmhqBQIIcrCSGEGHIjPoR9PV1JijZwtKKJ/NIGe5cjhBBiBBnxIQyQMkmuriSE\nEGLoSQgDY8K8Cfb3YHdWJfVN7fYuRwghxAghIQwoFArmJoXQZbHy3b5j9i5HCCHECCEhfMK0hEDc\nXFSk7SvF3GWxdzlCCCFGAAnhE1y1aqaPC+J4Uwd7cqrsXY4QQogRQEL4NClJcj5pIYQQQ0dC+DSB\nvu4kRPqSW3KcoxWN9i5HCCGEk5MQ/g8pcnUlIYQQQ0RC+D8kjvLD4O3KzkMVNLV22rscIYQQTuy8\nQjgnJ4d58+axZs2aPud5/vnnWbZsmc0KsxelUsGciSY6zRa27pfDlYQQQgyefkO4paWFJ598kmnT\npvU5T15eHj/++KNNC7Ony8YHodUo+WZPKRaLnE9aCCHE4Og3hLVaLatXr8ZoNPY5z6pVq/iv//ov\nmxZmTx6uGqbFB1LT0Mb+vGp7lyOEEMJJ9RvCarUaV1fXPqevX7+eKVOmEBISYtPC7G3uicOVvpYB\nWkIIIQaJeiAPrq+vZ/369bz22mtUVJzf9Xh9fNxRq1UDedqzGAx6my7v5DLHRfmTkV9Na5eVsEBP\nmz/H+dTgbJyxJ3DOvqQnx+GMfTljT70ZUAjv3LmT2tpabrnlFjo6Ojh69ChPP/00jz76aJ+Pqatr\nGchTnsVg0FNVNTjH9M5MDCQjv5p1X+ew7IrYQXmOvgxmX/bijD2Bc/YlPTkOZ+zLWXvqzYBCeP78\n+cyfPx+AkpISHnnkkXMGsKOZEO2Pr6cL32eW85NZUbi7DujHJYQQQpyh31TJzMzk2WefpbS0FLVa\nzaZNm0hJScFkMpGamjoUNdqNSqlkzsQQPvy2gG0ZZVw+OdTeJQkhhHAi/YZwQkICb731Vr8LMplM\n5zWfo5k5PpiPtxXyzZ4S5iWbUCoU9i5JCCGEk5AzZvVD765lalwAlXWtZBbU2LscIYQQTkRC+DzM\nnSSHKwkhhLA9CeHzEB6oZ7TJi8yCWsprbTu6WwghxMglIXye5p1YG/5G1oaFEELYiITweUqKMeCt\n07Ito4zWdrO9yxFCCOEEJITPk1qlZPbEENo6uvg+s9ze5QghhHACEsIXYNaEENQqBd/sKcFqlasr\nCSGEGBgJ4Qvg5aFl8hgjZTUtHCqss3c5QgghHJyE8AWaO6n7rFmbZYCWEEKIAZIQvkCjgj2JDPJk\nf141lfWt9i5HCCGEA5MQvgjzJpmwAml7ZG1YCCHExZMQvgjJY4x4umvYur+M9o4ue5cjhBDCQUkI\nXwSNWsnMCSG0tJvZcUgOVxJCCHFxJIQv0pyJIaiUCjany+FKQgghLo6E8EXy0buQFGOgtKqZnOJ6\ne5cjhBDCAUkID4BcXUkIIcRASAgPQLTJizCjjr051dQ2tNm7HCGEEA5GQngAFAoFcyeZsFitpO0t\ntXc5QgghHIyE8ABdEheAzk3Dt/uO0WmWw5WEEEKcPwnhAdJqVFw2Poim1k5+OFRp73KEEEI4EAlh\nG5gzMQSFAjlcSQghxAWRELYBfy83JkYbKKpoJL+0wd7lCCGEcBASwjZy6nClYjtXIoQQwlFICNvI\nmDBvQgwepGdXUdfYbu9yhBBCOAAJYRtRKBTMTTLRZbHy7T45XEkIIUT/JIRtaFp8IO4uarbsO4a5\ny2LvcoQQQgxzEsI25KJVMSMxiIbmDn7MksOVhBBCnJuEsI2lTDKhoPtwJSGEEOJcJIRtzOjtRmKU\nHwXHGjhSJocrCSGE6JuE8CCYm3zicKXdsjYshBCibxLCgyAuwpdAX3d+zKqgobnD3uUIIYQYpiSE\nB4HyxNWVzF1yuJIQQoi+SQgPkksTAnHVqkjbWyqHKwkhhOiVhPAgcXNRM31cEPVNHezJqbJ3OUII\nIYYhCeFBdPJ80nK4khBCiN5ICA+iQF93EiJ9yS05ztGKRnuXI4QQYpg5rxDOyclh3rx5rFmz5qxp\nO3fuZMmSJSxdupRHHnkEi0X2f55O1oaFEEL0pd8Qbmlp4cknn2TatGm9Tn/sscd46aWXePfdd2lu\nbmbr1q02L9KRjYvyw+jtxs5DFTS1dtq7HCGEEMNIvyGs1WpZvXo1RqOx1+nr168nMDAQAF9fX+rq\n6mxboYNTKhSkJIXQabawdf8xe5cjhBBiGFH3O4NajVrd92w6nQ6AyspKtm/fzgMPPHDO5fn4uKNW\nqy6wzHMzGPQ2XZ6tLUqJ4aNtR9iy/xiLU2Nxd9Wc1+OGe18Xwxl7AufsS3pyHM7YlzP21Jt+Q/h8\n1NTUcPfdd7Ny5Up8fHzOOW9dXYstnrKHwaCnqmr4D3qaMyGEjbuO8tu/buXBGyegczt3EDtKXxfC\nGXsC5+xLenIcztiXs/bUmwGPjm5qauJnP/sZv/rVr5gxY8ZAF+e0Fs+OYkZiEIXljTy7dg/1Te32\nLkkIIYSdDTiEV61axW233cbMmTNtUY/TUioV3L5gDPMmmSitbmbVmj1U17fauywhhBB21O/m6MzM\nTJ599llKS0tRq9Vs2rSJlJQUTCYTM2bMYMOGDRQVFbFu3ToAFi5cyI033jjohTsipULBTfOicXNR\n8+n3hTyzdg+/WTqBID8Pe5cmhBDCDvoN4YSEBN56660+p2dmZtq0IGenUCi4buYoXF1UfJCWz6q1\ne/j1jRMICxgZgxCEEEKcImfMspMFl4Sz7IpYmlo6+dPbe8krPW7vkoQQQgwxCWE7mjMxhDsXxtHW\n0cXz7+7jcGGtvUsSQggxhCSE7WxaQiD3XJdAl8XC/3xwgH251fYuSQghxBCREB4GkmIMPLB4PEol\n/P2jDH44VGHvkoQQQgwBCeFhIj7Sl1/fOAGtRsm/PjnIpp1F9i5JCCHEIJMQHkaiTd48dFMSHm4a\n/vbBPr7cddTeJQkhhBhEEsLDTHignt/ekoSvpyvvfpPHx9uOYLVa7V2WEEKIQSAhPAyF+Hvw7H0z\n8Pdy5eNtR3g/LU+CWAghnJCE8DAV6OfBI7dOIsjPnU27inljYzYWiwSxEEI4EwnhYcxH78Jvb0ki\nLEDHd/uPsfqzQ5i7LPYuSwghhI1ICA9znu5aHrppIqNNXvxwqIJ/fJRJp7nL3mUJIYSwAQlhB+Du\nquHXSyYQF+HDvrxq/vLBAdo6zPYuSwghxABJCDsIF62KBxYnMjHan8NFdTz/3j5a2jrtXZYQQogB\nkBB2IBq1il9cm8DU+ADySxv409t7aWjusHdZQgghLpKEsINRq5TcuTCO2ROCOVrZxKq1e6htaLN3\nWUIIIS6ChLADUioULLsilvmXhFFe28KTb+6mqLzR3mUJIYS4QBLCDkqhUHDD7CiWzo2moamDZ9am\nyxWYhBDCwUgIOzCFQsHlk0O57/pxAPz1wwN8tbvYzlUJIYQ4XxLCTmBijIHf3pyEp4eWd77OZe2X\nOXRZ5KQeQggx3Dl0CBc1FHPb+v/itYNvU9pUZu9y7CoyyJPfLZ9EiMGDzXtK+OuHGXIssRBCDHMO\nHcKeWj1Gdz92V+zj6V3/wz/2v0pe/RF7lzUkqlpq2Fi4mZf2/ouM6kMA+Hu58cgtk4iP9OVAfg2r\n1uyhrrHdzpUKIYToi+rxxx9/fCifsKXFdse1uqldWZQ4D4PKSF1bPdl1eews201WbS6eWh0GN38U\nCoXNnm8oeXi4nPWzqm8/zvfHdvFB7id8nP8FOXX51LTVkl6xH6VCRZRXBFqNiiljjTS0dHAgv4Yf\nsyoZG+6Dl87FTp2c0ltPzsAZ+5KeHIcz9uWsPfVGPcR12JxCoSDBfywJ/mPJqz/CV0VpZNZk8b8H\nXiPYI5DLw+eQZExEpVTZu9SL0tTZzN7KDNIr9pFXfwQrVpQKJWN9Y5gUMAF/V1/eOPQunxZspLTp\nGLeOXYKLSsvyK2IJ8HHn/bQ8nlm7h18siicxyt/e7QghxLDRam6ltq2e2rY6atrqqG2ro7atHrVC\nxS1jb0CjHPyIVFiH+EK1VVW2PZ7VYNCftczSpjK+LEojvWI/Vqz4ufoyL2wWU4OS0ao0Nn3+wdBm\nbuNIewFpeTs5XJuDxdo9yCrKK4LkgAlMNCai1+p65m/saGJ1xpvkHy/EpAvmrnG34efmA8DurMqe\nqy/dkhpDSpLJLj1B76+VM3DGvqQnx+GMfdmiJ6vVSlNn81kBW3va7VZza6+P9dLq+f3U3+CmdhtQ\nDaczGPS93u+UIXxSdWsNXx/9jh1lP2K2mNFrdaSYLuMy01Sb/nBtoaOrk4M1WaRX7COz5jCdlu5B\nVaH6EJIDJjDJOB4fV+8+H2+2mPkg52O2HfsBncaDOxOWEe0zCoD8Y8f567oDNLR0cvnkUJbMGY1S\nOfSb6Z3xzQKcsy/pyXE4Y1/n21NjRxOVLdXUtNWeFbC1bXV0Wno/v75WpcXX1QdfV298XX3wczlx\n280XX1dvPLV6lArbDpkakSF8UkNHI2nF2/iuZAdtXW24qlyZaZrGbNMMvFx6/8EMhS5LF1l1uaRX\n7Gd/VSZtXd2DqALcjcwaNYUxurEEuBsuaJlbS3fwfs7HANwQvYiZpmkAVNe38j8f7KespoWJ0f7c\ndXU8Ltqh2UTfZelia+lOSttKuCxwOmF6+62ND4aR/CboSJyxJ3DOvnrrqdXcRnFjCUUNJRQ1FFPY\nUExde32vj/dQu/cE7Olhe/Kfh8Z9yMcLjegQPqnV3MrWkp18U7yVxs4m1Eo104ImMy9sJv5ufjat\nqy8Wq4X8+iPsrtzPvsoMmjqbAfB19WGScTzJARMI0QVhNHpe9M8qt66AlzPfoqmzmRnBl3BDzCLU\nSjUtbZ38/aNMDhfVER6o54HFiXgP8oCtwzU5rMv9hPKWSgAUKJgalMzVo67Ay8VzUJ97qIyUN0FH\n54w9gXP25eXryr4j2d2B21hMUUMJlS1VWDkVVzqNB+GeoQR5BOB3WsD6unrjqna1Y/W9kxA+TUdX\nJzvLdvP10W+paatFqVCSZEzk8vA5hOiCBlyT2WKmxdxKS2fria8ttJhbKW4sZU/lAerbjwOg1+pI\nOhG8kZ5hZ3wyG+gfVk1rHf/MeJ3SpjKivCL42bjl6LU6zF0W3tyUzbYDZfh6uvCrxeMxGXX9L/AC\nVbZUsT7vMzKqD6NAwfTgKUwbNZG1ezdwrLkcF5WWy8NTSAm9zCH205+LM74JjuSealprOdpYipva\n9bR/bripXVEPwUCdC+Xor1WXpYvylkqKGoq7/zWWcKypjC7rqRMOuapcCNWHEO4Z2v1PH4qvq7dD\nHf0iIdyLLksXeyoP8GVRGseaywFI8BtDavgcIjxDew3SFnMrrZ2tNJtbTpvWSqv51HwdfeyHAHBT\nuzHRkMCkgAlEe4/qc9S2Lf6w2rs6WHP4ffZUHsDHxZu7EpcTpjdhtVr5YmcRH35bgKtWxT3XJpAw\nyjZbAlrNrfy7cDNbirfTZe0i2nsUi6OvwaQPxmDQU1HZfZjVpwWbaOpsxtfVh2ujriTJmOhQf1Cn\nO9drZbVaya7LY3PxdxTUFxLhGUacXyxxfrEEuhuHbc+O/sbem/56Ot7ewMbCzWw79kPPYMj/pFVq\nugNZ44abyhU3jSvuareekD4Z2O49wd192/XEV80gfOB0pNfKarVS1VrD0YZiCk+s4ZY0lp7xnqlW\nqon0NhHsHky4PpRwTxNGd4PN99EONQnhc7BarRysyeLLojTyjxde8OMVKHBVd/8xumvcur/23Hbv\n/iPVuOHr6kOMT9R5DXu31R+W1Wrly6I0Pi3YhFqp5taxN5AcMAGAXYcrePmzw1gsVm69IobZE0Iu\n+nksVgs7y3bzSf5GGjub8HP14brRC5lgSOgJmtN7ajW3srHwG9KKt9Fl7WKUVwSLo68m3DN0wD0P\ntd5eq06LmfSKfXxTvLXnbG7gwdasAAAgAElEQVQ+Lt5n7MPycfEmzi+GON9YYn1HD6vBgo70xn6+\n+uqppbOVr49+S1rxVjosnRjd/JkWPJkuSxct5lbazG20mNtoNZ/8sH3ydlufYd0XrUqLXuOBh8YD\nndYDvUaHh8a9+6vWHZ1Gh17bPV2v8cBV7dpv+FzIa2W1WmnvaqfV3EaruY22rrae263mNtpO/Gu3\ndGC1WrFixWq1Yjnx9fT7rFixWK1YsZx1/xmPOfG109JJaVMZLaeNSFagIMgj4NQarqeJYI9AggJ8\nnPL3rzcSwv8hr/4IW0q209LZ0muQnh60Hpru+8/nD+VC2bqvjOpDvH7wHdq62rk8fA5Xj7oCpUJJ\nXslxXvrwAE2tncy/JIzFs6NQXuDaWV79EdblfExx0zG0Ki1XhM8hJXTmWZuZe+upqqWGDfmfs68q\nE4BLAidxTdR8vF28BtbwEDq9r6bOZraV/sB3Jds53tGIUqFkomEcKWGXEeEZRn37cQ7X5nK4Jpus\n2lyazS0AKBVKIk+uJfvGYtIH2/WT/0gI4Y6uTr4t2c6XRWm0mFvx0npyVWQqU4OSz+u8AlarlQ5L\nZ3cwd7aeCLJTX88K7s5WmjubaexspqmzGbOl/9PKKhXKUyGtcUen1aHTeHT/03Z/1etdqayrOyNI\nzwjWrjND9vT9qkPN4OZ3YnOyiTDPUEL1IbiotGfP56S/f72REB6mBqOv8uYK/u/A61S11hDvN4af\nxt+Em9qNyroW/vLBAcprW5gUa+DOhXG4aPp/E6ptq2ND3hekV+4HYEpgEouiFvQZoOfqKacun3W5\nn1DaVIZWqeHy8DnMDZuJtpc/0OHGYNBzsKiAtOJt7CjbTaelE1eVK9ODpzDLNL3nmO3/ZLFaKGoo\n4VBtNodrsilsKO55g9RpPBjrG0ucXwxjfWPOOC58KDjj39XJnrosXews283nR77ieEcDbmo3rgif\nwyzTpUP2+9a9RtpBU2dzdzB3NNF0IpybOppPhfVpt/s6pvVcFChwUbn0bCp3PW0/t6vatXuT+lnT\n3NCqNChRolAoUKBAeeJr9/f/cf8Z07q/Ks/4Xtkz7/nuU3fW37/eSAgPU4PVV0tnC68efJvDtTkE\nuBv4eeLtBLgbaGrt5O/rM8guricyyJP7Fyfi5dH7G1J7VwdfFW3h66Nb6LSYCfcM5Yboa4j0Ch9Q\nTz2btAs20tjRhI+LN9dGLWBSwIRhue/UarWSV3+EbZU7SC89gBUrvq4+zDFNZ1rwFNwucIRmU2cz\n2bW5HKrJ4VBtNg0d3T8rBQpC9cHE+cYy1i+WSM+wQT8DnDP+Xfn5e/DVoR18WrCRypZqNEoNc0Jn\nkBo2C3eNu73L61eXpYumzhaaOptOBHd3aHvq3ehqBdf/CFk3tSsuKheH3JfqjL9/EsIOZjD7slgt\nbMj/gs1Hv8NN7cpP428m3m8M5i4Lr/87i+8zy/Hy0LLsiliSYk4dp2y1Wkmv2MdH+V9Q334cL62e\nRVFXMjlw4nn9oZ9vT63mNr4sSuObo99htnYR6RnGT6KvIdIrbEB920qXpYu9lQfYXLyVo40lAER4\nhjE3bCbj/eNtEpBWq5XSprITa8k55B8vpMvaBXSfMz3WJ7pnf7KXi+eg7w6xWq2YrV20d7XTbu6g\nvaudDkvHqdtdHbR3dd9u77l95jSL1UKkVzhjfKOH5IPE6bVn1eXyRdGXFNQdRalQMj34EhZEzHWK\nw+Sc8T3QWXvqjYTwMDUUfe0q38ParHV0WbpYFLWAeWGzANi0q5j13xVg7rIwZayRm1NjqDNXsC73\nEwqOF6FWqpkXOpPU8Dm4qs//OOML7am6tZYN+V+wt/IAAJMDJrIoasE5zxw2mFo6W/m+bBdbirdT\n116PAgXjDfH8JHE+PhbDoK6tt5nbyKnL51BtDodqsqhpqztrHuVpm/0UCiVKlCgVirPvVyhRcuL+\nnnm7v1ee2NyoVENLe9sZwXqhg5DOxVXlQrTPKMb4xDDGN5oA98H5+RU2HOXj/I3k1OUBkBwwgasi\nL8fo7jznUXfG90Bn7ak35xXCOTk53HPPPdx+++3ceuutZ0z7/vvveeGFF1CpVMycOZN77733nMuS\nED4/Q9VXUUMx/8p4k/r24yQHTOCWMTegVWkoq2nm1S8Ok19ZhXt4HlbfYgAmGMZx3eir8HfzveDn\nutie8uqPsC73E4obS9EoNaSGzWJe+OxeB3QMhurWWrYUb+P7sl20d3WgVWm5NGgys00zMLj7Dfnv\noNVqpbK1mkM12eTU5dNqbu0ZpWqxnhyZasFi7R61arFaToxk7Z5+8nvrye97Hmc5MaLVglKpQqvU\n4KJywUWlPfHPBe1pt11U2rO+P3vaqfvNli5y6/PJqs0lqzaXytbqnp68XbwY4xvNWJ9oYn2jB7wP\nvLy5gk8LNvUM+Ivzi+X2ST/Bw2yfD3CDyRnfA521p970G8ItLS38/Oc/JyIigtjY2LNC+Morr+SV\nV14hICCAW2+9lSeeeILRo0f3uTwJ4fMzlH0db29gdcZbHGkoIkwfwl3jbkOn1fHN0a18XvA1XXRi\nadET3nUJd8+dddFn2RpITxarhR/K9/BJ/r9p6GjE28WLRVELSA6YMGj7vI4cL2Jz8Vb2VWZgxYq3\nixezTdOZHjzljH2Izvg7OBQ91bTWkVWXQ1ZtLtm1eT0jxQFMumDG+EYzxjeaKK/I8z6hS11bPZ8f\n+YqdZbuxYiXSM5xFUfOJ9olyytcJ5PfPUVx0CJvNZsxmM6tXr8bHx+eMEC4uLuahhx7inXfeAeCf\n//wn7u7uLFu2rM/lSQifn6Huq9Ni5r3sj9hR9iN6jQ4XtQvVrTXoNB7MCpzDgV3u5BQ34O6i5qZ5\n0VyaEHjBmw9t0VObuZ2vitLYXPxd96AwfSgRXqHQPWaT7v+7/zv99slau+dRoIAT85y8raR7lu75\nMqsPc6ShCIBQXTApYTOZZBzf635MZ/wdHOqeLFYLJY3HOFzbHcoFxwsxn9gHrlGqifKKPBHKMYTo\nAs/64NXU0cymom/4rnQHZouZII8Arhk1n3H+cb0ep+5MnLEvZ+2pN/2OF1er1ajVvc9WVVWFr++p\nzZK+vr4UFxefc3k+Pu6o1bYdkNFXc45uqPv6lfGnjMmN5I1962gxt3BVzFwWx1+Jh9Ydy1QrG3cW\n8vpnB3nl88Psy6/h3sUTMPhc2AkmBt6TnhVBN3D1uBTWHtjA90d3U9R47t+5izUpeBwLY+cRZ4ju\n9wOHM/4ODnVPAUYvJkWNBaDd3MHhqlwOlB/mQEUWWXW5ZNXlQv4XeLroGBcwhsSAscQaovj+aDqf\nZn1Fq7kNg7svSxKu5rLwKSiVZ28hccbXCZyzL2fsqTdDfiLUurqW/me6AM74iQns11eyTzLBk01o\nVRr83fxoOd5FC911TI72J3LFFN7YmE16ViX3/GkzS1JGM2t88HmtFdu2Jy23jF7CAlMqreY2ACxW\nK3DqDD2c/M7affTtybP3AFitlpO3TjzuzHmNbv4Y3LtP5Vld3TSEfQ0Pw6GnEHUYIaYwFpiu4Hh7\nI9l1uSf2J+ew/ehuth/d3TOvTuPB4uhrmBEyFY1STU1N81nLGw49DQZn7MtZe+rNgELYaDRSXX1q\ncEVFRQVGo3EgixTDQLAusM9p/l5uPLhkPNsOlPHuN3m8uTGbHw9XctuCMRi9h/60i76uvZ8IQzgX\nLxc9UwKTmBKYhNVqpay5gqy6XPLrCzHpgpgTOmNYXjlHiP4MKIRNJhNNTU2UlJQQGBhIWloazz33\nnK1qE8OUQqHgsvHBJIzy482NWezPr+GxV35g8awoUiaZLvi0l0JcCIVCQbAukGBdICmhl9m7HCEG\npN8QzszM5Nlnn6W0tBS1Ws2mTZtISUnBZDKRmprK448/zq9//Wuge6R0ZGTkoBcthgcfvQv3L05k\n56EK3v4qh7e/zuXHrEp+euVYAn2H/xmIhBDC3uRkHcOUo/V1vLmDtV9mszu7Co1aybWXRXLF5DCU\nSttdI3m4csa+pCfH4Yx9OWtPvXG8k4qKYcnLQ8s9143jnmsTcNOq+CAtn6feSqe06tyDmoQQYiST\nEBY2lTzGyJN3XsLU+ACOlDXw+Gs/8un2I5i7bHfKQyGEcBYSwsLm9O5a7ro6nvt/kojeXcNHW4/w\nxzd2U1B63N6lCSHEsCIhLAbNhGh//njnJcxIDOJoZRMP/uVbPt1+hC6LrBULIQRICItB5u6qYcWV\nY3lwyXi89S58tPUIq9bsocLGJ20RQghHJCEshkTCKD/+9ps5XBIXQP6xBla+uost+0oZ4sH5Qggx\nrEgIiyGjc9fy82vi+fk18aiVSt7cmM2L6w5wvKnd3qUJIYRdSAiLIXdJXABP3DGFuAgfDuTX8PtX\ndpGeXWXvsoQQYshJCAu78PV05cEbJ3DzvGjaO7v4+0cZvPr5YVrbzfYuTQghhsyQX0VJiJOUCgXz\nkkOJi/Bl9aeH2JZRRtbROu64aiyxYXJhBiGE85M1YWF3wf4e/G75JBZeGkFNQxt/ensvH6Tl0WmW\nQ5mEEM5NQlgMC2qVkutnjuKRWydh8Hbj3z8c5ck3dlNSKae9FEI4LwlhMayMDvHi8RWTmTUhmJKq\nJp5440c2/nAUixzKJIRwQhLCYthx1aq5bf4Y7l+ciLurhvfT8njunb1UH2+1d2lCCGFTEsJi2Jow\n2p8n7pjCxGh/so7Ws/LVXWzPKJMTfAghnIaEsBjWPN213Hf9OFZcORarFV75/DD/2JBJY0uHvUsT\nQogBk0OUxLCnUCiYkRhEbJg3r3x2iPTsKvJKjrPiqrGMG+Vn7/KEEOKiyZqwcBgGbzceujmJG2ZH\n0dTayf+8v5+3NmVTXtsiA7eEEA5J1oSFQ1EqFSyYGk58pC+rPztE2t5S0vaW4qpVEWbUERagJzxQ\nT1iAniA/d9Qq+ZwphBi+JISFQwoL0PPYbclsPVBGfmkDRysayS09Tk7J8Z551ColJoNHTyiHB+gx\nGTzQalR2rFwIIU6REBYOS6NWkZJkIiWp+/v2zi5Kqpo4Wt5IUUUTRRWNlFQ1UVje2PMYpUJBkL87\nYcbuNebwgO61ZzcX+VMQQgw9eecRTsNFoyIq2IuoYK+e+8xdFo5VN3P0RCgXVTRSXNFEaVUzOw6W\n98xn9HE7sbasIzxAT1SIlwSzEGLQybuMcGpqlZKwgO7N0TMIAsBitVJR29ITzEcrGikqb2R3ViW7\nsyoB8HTX8ItrE+RCEkKIQSUhLEYcpUJBkJ8HQX4eXBIXAIDVaqW2oZ2iikZyS+r5encJf35nH0vm\nRJE6ORSFQmHnqoUQzkhCWAi6j0X283LFz8uVpBgDE6MN/O+GTN79Jo+CsgZuXzAGV638uQghbEuO\n3xCiFzGh3qz86WRGm7zYdbiSp95Mp7y2xd5lCSGcjISwEH3w1rnw0E0TmTfJRGl1M0++8SN7c6rs\nXZYQwolICAtxDmqVkptTY/jZ1XF0dVn56/oMPvw2H4tFztAlhBg4CWEhzsO0+EB+tzwZo7cbn+8o\n4n/e3ycXkRBCDJiEsBDnKdSo4/e3JzM+yo+DhXU88fqPFJY32LssIYQDkxAW4gJ4uGr45eJErp0R\nSW1DO0+/tYevfiiyd1lCCAclISzEBVIqFFwzI5IHbhiPi0bJS+/v442NWXSaLfYuTQjhYCSEhbhI\niVF+/P72yYwK9uLbfcdYtTad2oY2e5clhHAgEsJCDIDR241nfzmDSxMCOVLWyOOv/cjhwlp7lyWE\ncBASwkIMkKtWzR1XjWXZ5TG0tpt57r19/HtnEVarHMYkhDi38zoP39NPP83+/ftRKBQ8+uijJCYm\n9kxbu3Ytn3zyCUqlkoSEBH73u98NWrFCDFcKhYI5SSZCA/T846MMPtiST8GxBlZcNVauxiSE6FO/\na8K7du2iqKiI9957j6eeeoqnnnqqZ1pTUxOvvPIKa9eu5Z133iE/P599+/YNasFCDGejQ7xY+dMp\nxIZ6k55TxR/f3M2x6mZ7lyWEGKb6DeEdO3Ywb948AKKiojh+/DhNTU0AaDQaNBoNLS0tmM1mWltb\n8fLyOtfihHB6Xh5afnPTBC6fHEpZTQtPvrm75xKJQghxun63k1VXVxMfH9/zva+vL1VVVeh0Olxc\nXLj33nuZN28eLi4uXHXVVURGRp5zeT4+7qjVqoFXfhqDQW/T5Q0XztiXM/YEvff1y6VJTIgN4KX3\n9/KPDZlcN3s0yxaMRaN2jKEYzvhaOWNP4Jx9OWNPvbngnVWnDzZpamrin//8Jxs3bkSn03HbbbeR\nlZXFmDFj+nx8XZ1tr0RjMOipqmq06TKHA2fsyxl7gnP3Ncbkye+WTeJvH2Xy0ZY8dmYc47b5Y4gJ\n9R7iKi+MM75WztgTOGdfztpTb/r9SG40Gqmuru75vrKyEoPBAEB+fj6hoaH4+vqi1WpJTk4mMzPT\nRiUL4RxCDDoeuy2ZOUkhlNe0sGrtHl774jBNrZ32Lk0IYWf9hvD06dPZtGkTAAcPHsRoNKLT6QAI\nCQkhPz+ftrbuExRkZmYSERExeNUK4aDcXNQsuzyWR5dPwmTQsfVAGb9bvZMdmeVyKJMQI1i/m6OT\nkpKIj49n6dKlKBQKVq5cyfr169Hr9aSmpnLHHXewfPlyVCoVEydOJDk5eSjqFsIhRQV78djtyXy9\nu4QN2wpY/dkhtmWUsfyKWAJ83e1dnhBiiCmsQ/wx3Nbb+Z1x3wE4Z1/O2BNcfF/V9a2s+SqHA/k1\nqFVKFl4azoJLwofFwC1nfK2csSdwzr6ctafe2P+vXYgRyt/bjQcWJ3LPtQl4uKnZsPUIj7+2i+yj\ndfYuTQgxRCSEhbAjhUJB8hgjT905lZQTA7eefXsvr8rALSFGBAlhIYYBd1c1t14ey++WJxNq1LHt\nQBmP/msn2zPKZOCWEE5MQliIYWRUsCeP3Z7Mkjmj6TB38crnh3nu3X2U19r2+HohxPAgISzEMKNS\nKpl/SRh/vPMSEqP8OFxUx2Ov/MAn247QabbYuzwhhA1JCAsxTPl7nRq4pXPTsGHbEVa+KgO3hHAm\nEsJCDGM9A7d+NpW5k0xU1HYP3Hrl80M0tnTYuzwhxABJCAvhANxc1NySGsPvlicTZtSxPaOc363+\nQQZuCeHgJISFcCCjgj35/e3J3JhyauDW71b/wOc7CqlrbLd3eUKIC3TBV1ESQtiXSqnkiilhJMca\n+fC7fHZnVfHhtwWs/66AhEg/po8LZGK0PxobXzJUCGF7EsJCOCg/L1fuujqeW1I72XWogm0Z5WQU\n1JBRUIOHq5opcQHMGBdERKAehUJh73KFEL2QEBbCwXm4apiTZGJOkonS6ma2Z5SxI7OctD2lpO0p\nJcTfg+njgpgWH4CXzsXe5QohTiMhLIQTCfH3YMmc0fxk1igyC2rZnlHGvrxq3k/LY92WfMaN8mVG\nYhDjR/ujVsmQECHsTUJYCCekUioZP9qf8aP9aWrt5IdDFWzLKGN/fg3782vQuWmYGhfA9HFBhAf2\nfnUXIcTgkxAWwsnp3DTMnWRi7iQTxZVNbM8oY+fBcr5OL+Hr9BJCjTqmjwtianwAnu5ae5crxIgi\nISzECBJq1LF0bjSLZ0eRUVDDtgNlHMiv4d3NuXyQlkdilB8zEoNI8fWwd6lCjAgSwkKMQGqVkonR\nBiZGG2ho6WDnwQq2HShjb241e3OrefWLLGJDvYmL8CEuwpcAHzcZYS3EIJAQFmKE83TXcvnkUC6f\nHMrRika2HSgj40gte3Kq2JNTBYCfpwtjI3y7QzncF08P2WwthC1ICAsheoQF6Lk5VY/BoOdgbiWH\nCms5VFjH4cJath0oY9uBMgBMBh1xET7ER/oSY/LGRSsnBhHiYkgICyF6ZfR2wzghhNkTQrBYrRRX\nNHGwsJZDhbXkFB+npKqJL38sRqVUMDrEq2fTdUSQHpVSDn8S4nxICAsh+qVUKAgP1BMeqOfKqeF0\ndHaRV3qcQ4V1HCysJae4nuziej7aegQ3FzVjwryJO7H5OtDXXfYnC9EHCWEhxAXTalQnQtaXxUTR\n1NpJVlFdz+brkwO8AHz0Lj2brifFGNGoZS1ZiJMkhIUQA6Zz05A8xkjyGCMAVfWtp/YnF9WxPaOc\n7Rnl+Hnms/DSCKaPC5IzdgmBhLAQYhAYvN2YNSGEWaftT95xsJy0vaW8sTGbz3cUcc30SKYlBMj+\nYzGiSQgLIQbV6fuTr5gSxhc7ivh2fymvfnGYz3cWsWh6BFPGBqBUyn5jMfLIR1AhxJDx0btwy+Ux\nrPr5NGZPCKa6vpV/fXqIx17dxY9ZlVisVnuXKMSQkjVhIcSQ8/V0Zfn8MVw5NZxPvi/k+4xy/ndD\nJiaDjmsvi2RitL+MqBYjgoSwEMJu/L3dWHHlWK6aGs4n2wvZeaicv63PIDxQz3WXRTJulJ+EsXBq\nEsJCCLsL8HXnZ1fHcdW0cD7ZfoRdhyv5ywcHiAr25NrLRhEX4SNhLJyShLAQYtgI9vfg7kUJLJzW\nxMfbjpCeU8Xz7+0jxuTFdTNHERvmY+8ShbApCWEhxLBjMuq49/pxFJU3smFrAfvza3j27b2MDffh\nustGMdrkZe8ShbAJCWEhxLAVHqjngRvGU3CsgQ1bC8g8UsvhonQSRvly7YxRjAr2tHeJQgyIhLAQ\nYtgbFezJgzdOILekng1bj5BZUEtmQS0TRvszMdofvbsWvbvmxD8trlqV7EMWDkFCWAjhMKJN3vz3\nTRPJKqrjo60F7MurZl9e9VnzqVUKdG6a08JZi9HPAzXWM+47+dXdVY1SQlvYgYSwEMLhjAn34eGw\nJPKPNVBe00JjaweNLZ00tpz82n27sr6V4sqmfpenVCjQual7gjnIz4PLJ4cS4Os+BN2Ikey8Qvjp\np59m//79KBQKHn30URITE3umlZWV8eCDD9LZ2UlcXBxPPPHEoBUrhBAnKRTd1zEeHXLuQVqd5i4a\nWzpRuWgoLq0/FdatnWfdrm9qp7S6mayj9WzZV8olcQEsnBZBsL/HEHUlRpp+Q3jXrl0UFRXx3nvv\nkZ+fz6OPPsp7773XM33VqlWsWLGC1NRU/vCHP3Ds2DGCg4MHtWghhDhfGrUKX08VBoMeLxdVv/Ob\nuyzsza3m0+1H2Hmwgh8OVjB5rJGFl0ZgMuiGoGIxkvQbwjt27GDevHkAREVFcfz4cZqamtDpdFgs\nFtLT03nhhRcAWLly5eBWK4QQg0ytUjJ5jJFJsQb25Vb3nDxk1+FKJsUYuHp6BGEBenuXKZxEvyFc\nXV1NfHx8z/e+vr5UVVWh0+mora3Fw8ODZ555hoMHD5KcnMyvf/3rQS1YCCGGglKhICnGwMRof/bn\n1/Dp9kLSc6pIz6liwmh/rp4eQWSQHCIlBuaCB2ZZT7vKidVqpaKiguXLlxMSEsJdd93Fli1bmD17\ndp+P9/FxR63uf5PQhTAYnPNTqTP25Yw9gXP2JT2dkmr0ZN7UCPZmV/HuV9k9o7InjTGyNDWWMRG+\nNq70wshr5bj6DWGj0Uh19alDACorKzEYDAD4+PgQHBxMWFgYANOmTSM3N/ecIVxX1zLAks9kMOip\nqmq06TKHA2fsyxl7AufsS3rqXaifG7+5cTxZRXV8sr2Q9KxK0rMqiYvw4epLI+xyWk15rRxDXx8q\n+r2e8PTp09m0aRMABw8exGg0otN1D05Qq9WEhoZSWFjYMz0yMtJGJQshxPCjUCgYG+HLb29J4rc3\nTyQuwodDhXU8+/Zenl27h8OFtWdsMRTiXPpdE05KSiI+Pp6lS5eiUChYuXIl69evR6/Xk5qayqOP\nPsrDDz+M1WolJiaGlJSUoahbCCHsLjbMh9gwH/JKj/Pp9kIyCmr487v7GG3y4ppLI4iP9JUzd4lz\nUliH+CObrTcxOONmC3DOvpyxJ3DOvqSni3OkrIFPtxf2nMUrMsiTa6ZHkBg1eNdFltfKMfS1OVrO\nmCWEEDYSGeTJ/YsTKSpv5LPvu0dTv7juAOEBehZeGsH40X6oVf3uBRQjiISwEELYWHignnuvH0dJ\nZROf7Sjkx8OV/P2jDNxc1Iwb5UtilB/jRvmhd9fau1RhZxLCQggxSExGHXcvSuCa6c18s6eE/Xk1\nPSf+UACjQjwZH+XP+NH+mAwesv94BJIQFkKIQRbs78Gtl8dyS6qV0upmDuTXsD+vmrzS4+SXNrD+\nuwJ89C6MH+1PYpQfY8N9cNHY9nwKYniSEBZCiCGiUCgwGXSYDDqunBpOU2snmQU17M+vIbOghi17\nS9mytxSNWsnYcB/GR/mRGOWPn5ervUsXg0RCWAgh7ETnpmFqfCBT4wPpsljIL21gf341B/JqOJDf\n/Q9yMBk8etaSo4K9UCpls7WzkBAWQohhQKVUEhPqTUyoNzfMHk11fSv7TwTx4aI6SnYU8fmOInRu\nGhJG+TI+yp+EUb4Y7F24GBAJYSGEGIb8vd2YO8nE3Ekm2ju6OFRU27MveefBCnYerECpUJAQ5cf4\nUb4kxRjw0rnYu2xxgSSEhRBimHPRqpgYbWBitAGr1UpxZRP786q715TzqjmQV82aL3OIDvUmOdbA\npFgjPnoJZEcgISyEEA5EoVAQFqAnLEDP1dMjUWjUfLnjCOlZleQW15NTXM/bX+cyOsSrJ5BlYNfw\nJSEshBAOzN/bjdTkUFKTQ6lvaic9u4r07Eqyi+vJKz3Ou9/kERnkSfIYA8mxRgzebvYuWZxGQlgI\nIZyEt86lZz/y8eYO9uZUsTu7kqyieo6UNfBBWj7hgXqSYw0kjzES4ONu75JHPAlhIYRwQl4eWmZP\nDGH2xBAaWzrYm1vN7uxKDhfWUVTeyIffFhBq1PUEcpCfh71LHpEkhIUQwsnp3bXMHB/MzPHBNLd1\nsi+3mt1ZlRwsrOWjrU18tPUIIf4eTDoRyCH+cgrNoSIhLIQQI4iHq4bp44KYPi6IljYz+/O7Azmj\noJZPthfyyfZCgv09mHcR/60AAA2uSURBVDU+mOnjAnF31di7ZKcmISyEECOUu6uaafGBTIsPpLXd\nTEZBDT9mVbI/r5p3Nufy4bf5XBIXQEqSifDA3q+HKwZGQlgIIQRuLmqmjA1gytgAGlo62H6gjLS9\npWw9UMbWA2VEBnmSkhTC5DFGtHJxCZuREBZCCHEGT3ctC6aGc8UlYWQW1LJlbyn786p55fMG3t2c\ny4zEIGZPDJHR1TYgISyEEKJXSoWCxCg/EqP8qD7eyrf7jrF1/zE27Spm065i4iN9SZkYQuJoP1RK\npb3LdUgSwkIIIfrl7+XGT2ZFsWjG/2/v/oOiqv89jj+XXX4toOwiu/gLNVJBrj8gQZEroCmpTVbf\n+U4jE9eaoalUpHE0RCbDmUb8hU0ONpVWVlpzLeI2VM4Xv03eO94CxB9hoomkXwN/8GuBREmCzv2D\n6+YKCJRw9mzvxwwznvM5B94fP+fw4pw9P8Zx7Gwdh45XU37BRvkFGyY/TxKmdV59Lc+v7h8JYSGE\nEH1m0LsxY5KVGZOsVNe1cOjEJYpOXeW/Dl+g4Jt/ETkhkLmRI5kw2l9uc+oDCWEhhBB/yKhAX/4j\ncSJ/jw+h+HQNh45XU/pDLaU/1DJimA9zIkYSEx6E0UuipifyPyOEEOJP8fY0MCdiJAnTRlB5qZlD\nxy9x9GwtH/6zgrz//pGZ4VYSpo0k2OorR8d3kBAWQghxT+h0OsaP8mf8KH+WXB/P4ZOX+Z/vfv/y\ndNdjNXsTZDZiNRkJMhsJCjBiNXn/ZR8KIiEshBDinhvi48HDMWNZOGMM359voKj8Kpfrb3C14QY/\n1bR0Xd7ojtVsxGo2EjLahK+HniCzNxaTN+4G170vWUJYCCHEgHFz0zH1/mFMvX8YAL8pCk3XbnLV\ndoMa2w2u2lqpaewM58pLzZyrbuZ/T16xr68DAoZ6YTUbCTIZ7UfSQWYj5iFeuLlp+/S2hLAQQohB\n46bTYR7ihXmIF5PGmh3a2jt+o66pldZ2hYp/2X4P6sYb9tuhbmfQuzF2uB9RoRaiQi34a/D2KAlh\nIYQQTsGgd2N4gA+BgX7cZ/V1aGu92U5tY6tDMF+pv8GP1c1UVjfzn1+dY8Jof6LDLDww0cIQHw+V\netE/EsJCCCGcnrengTFBfl1eJNHUcpOj/39b1NmqJs5WNbHvnxWEBpvsgezr7bwXfUkICyGE0Cx/\nX0/mTR/NvOmjsf38iz2Qz1xs5MzFRvYWVjBprImoMAuREwLxcbKrsCWEhRBCuATzEC8So4NJjA6m\nvrmV0h9qOXKmllMXbJy6YOODf5zl38aZiQ6zMm38MLw91Y9A9SsQQggh7rFhQ71ZOGMMC2eMobbx\nhj2Qy35soOzHBgx6Nybf1xnIU+8PwMtDnTiUEBZCCOHSLCYjD8eM5eGYsVxpuN75aM0ztZw4V8+J\nc/V4GNyYEhJAdJiVySEBeA7i+5IlhIUQQvxlDA/wYXHsOBbHjuNSXYv9CPno2TqOnq3D013PjEkW\nkhMnYtAP/OsZJYSFEEL8JY0M9GVkoC+P/vs4qmpvBXINR87U8veE+/H1dpIQzs7OpqysDJ1OR2Zm\nJlOmTOmyzPbt2/nuu+/Yu3fvPS9SCCGEGCg6nY5gqx/BVj/+FncfvykKereBD2CAXn/KkSNHuHjx\nIvv372fjxo1s3LixyzKVlZWUlpYOSIFCCCHEYNHpdIMWwNCHEC4qKmLevHkAhISE0NzcTEuL48O3\nN2/ezKpVqwamQiGEEMJF9RrC9fX1mEwm+7TZbKaurs4+nZ+fT3R0NCNHjhyYCoUQQggX1e8LsxRF\nsf+7qamJ/Px89uzZQ01NTZ/WN5mMGO7xa6kCA/16X0iDXLFfrtgncM1+SZ+0wxX75Yp96k6vIWyx\nWKivr7dP19bWEhgYCEBxcTE2m40nn3yStrY2fvrpJ7Kzs8nMzOzx+zU23rgHZf8uMNCPurpr9/R7\nOgNX7Jcr9glcs1/SJ+1wxX65ap+60+vp6NjYWAoLCwEoLy/HYrHg69v5dosFCxZw4MABPv74Y3bu\n3El4ePhdA1gIIYQQv+v1SDgyMpLw8HCWLFmCTqcjKyuL/Px8/Pz8mD9//mDUKIQQQrikPn0mvGbN\nGofp0NDQLsuMGjVK7hEWQggh+mHwboYSQgghhAMJYSGEEEIlEsJCCCGESiSEhRBCCJXolNufviGE\nEEKIQSNHwkIIIYRKJISFEEIIlUgICyGEECqREBZCCCFUIiEshBBCqERCWAghhFBJv98nrKbs7GzK\nysrQ6XRkZmYyZcoUe9u3337Lq6++il6vJy4ujhUrVqhYad9t3bqVY8eO0d7eznPPPUdiYqK9be7c\nuQQFBaHXd75/OScnB6vVqlapfVZSUsILL7zA+PHjAZgwYQLr16+3t2txrD755BMKCgrs06dOneLE\niRP26fDwcCIjI+3T7733nn3cnFFFRQXLly/n6aefJjk5mStXrpCenk5HRweBgYFs27YNDw8Ph3Xu\ntv85g+76tG7dOtrb2zEYDGzbts3+GlbofTt1Fnf2KyMjg/Lycvz9/QFISUkhISHBYR2tjVVaWhqN\njY1A53vqp02bxiuvvGJfPj8/nx07dhAcHAzArFmzWLZsmSq133OKRpSUlCjPPvusoiiKUllZqTzx\nxBMO7QsXLlQuX76sdHR0KElJScq5c+fUKLNfioqKlGeeeUZRFEWx2WxKfHy8Q/ucOXOUlpYWFSr7\nc4qLi5WVK1f22K7FsbpdSUmJsmHDBod50dHRKlXTf9evX1eSk5OVl156Sdm7d6+iKIqSkZGhHDhw\nQFEURdm+fbvy4YcfOqzT2/6ntu76lJ6ernz55ZeKoijKvn37lC1btjis09t26gy669fatWuVr7/+\nusd1tDhWt8vIyFDKysoc5n366afK5s2bB6vEQaWZ09FFRUXMmzcPgJCQEJqbm2lpaQGgqqqKoUOH\nMnz4cNzc3IiPj6eoqEjNcvskKiqKHTt2ADBkyBBaW1vp6OhQuaqBpdWxut3rr7/O8uXL1S7jD/Pw\n8GD37t1YLBb7vJKSEh588EEA5syZ02VM7rb/OYPu+pSVlcVDDz0EgMlkoqmpSa3y/rDu+tUbLY7V\nLefPn+fatWtOd+Q+kDQTwvX19ZhMJvu02Wymrq4OgLq6Osxmc7dtzkyv12M0GgHIy8sjLi6uyynM\nrKwskpKSyMnJQdHQw80qKyt5/vnnSUpK4ptvvrHP1+pY3XLy5EmGDx/ucFoToK2tjdWrV7NkyRL2\n7NmjUnV9YzAY8PLycpjX2tpqP/0cEBDQZUzutv85g+76ZDQa0ev1dHR08NFHH/HII490Wa+n7dRZ\ndNcvgH379rF06VJWrVqFzWZzaNPiWN3ywQcfkJyc3G3bkSNHSElJ4amnnuL06dMDWeKg0tRnwrfT\nUiD15quvviIvL493333XYX5aWhqzZ89m6NChrFixgsLCQhYsWKBSlX03duxYUlNTWbhwIVVVVSxd\nupSDBw92+YxRi/Ly8nj88ce7zE9PT2fx4sXodDqSk5OZPn06kydPVqHCP68v+5ZW9r+Ojg7S09OZ\nOXMmMTExDm1a3U4fffRR/P39CQsLY9euXezcuZOXX365x+W1MlZtbW0cO3aMDRs2dGmbOnUqZrOZ\nhIQETpw4wdq1a/n8888Hv8gBoJkjYYvFQn19vX26trbWfjRyZ1tNTU2/Tt+o6fDhw7z55pvs3r0b\nPz8/h7bHHnuMgIAADAYDcXFxVFRUqFRl/1itVhYtWoROpyM4OJhhw4ZRU1MDaHusoPO0bURERJf5\nSUlJ+Pj4YDQamTlzpmbG6haj0cgvv/wCdD8md9v/nNm6desYM2YMqampXdrutp06s5iYGMLCwoDO\nizfv3Na0OlalpaU9noYOCQmxX3wWERGBzWZzmY/uNBPCsbGxFBYWAlBeXo7FYsHX1xeAUaNG0dLS\nQnV1Ne3t7Rw6dIjY2Fg1y+2Ta9eusXXrVt566y37lY63t6WkpNDW1gZ0bqC3ruJ0dgUFBbzzzjtA\n5+nnhoYG+1XdWh0r6AwnHx+fLkdK58+fZ/Xq1SiKQnt7O8ePH9fMWN0ya9Ys+/518OBBZs+e7dB+\nt/3PWRUUFODu7k5aWlqP7T1tp85s5cqVVFVVAZ1/FN65rWlxrAC+//57QkNDu23bvXs3X3zxBdB5\nZbXZbHbquw/6Q1NvUcrJyeHo0aPodDqysrI4ffo0fn5+zJ8/n9LSUnJycgBITEwkJSVF5Wp7t3//\nfnJzcxk3bpx93owZM5g4cSLz58/n/fff57PPPsPT05NJkyaxfv16dDqdihX3TUtLC2vWrOHnn3/m\n119/JTU1lYaGBk2PFXTelvTaa6/x9ttvA7Br1y6ioqKIiIhg27ZtFBcX4+bmxty5c5369olTp06x\nZcsWLl26hMFgwGq1kpOTQ0ZGBjdv3mTEiBFs2rQJd3d3Vq1axaZNm/Dy8uqy//X0C1MN3fWpoaEB\nT09PewCFhISwYcMGe5/a29u7bKfx8fEq98RRd/1KTk5m165deHt7YzQa2bRpEwEBAZoeq9zcXHJz\nc3nggQdYtGiRfdlly5bxxhtvcPXqVV588UX7H7rOeNvVH6WpEBZCCCFciWZORwshhBCuRkJYCCGE\nUImEsBBCCKESCWEhhBBCJRLCQgghhEokhIUQQgiVSAgLIYQQKpEQFkIIIVTyf3/bXK1kLBoHAAAA\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "zza6aQ1QnMyy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def output_prediction_csv(fp, model, test_loader, task_header, RNN=False):\n",
        "    \"\"\"Outputs CSV to filepath fp\"\"\"\n",
        "    model.eval()  # set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(test_loader):\n",
        "            x = batch.tweet\n",
        "            ids = batch.id\n",
        "               \n",
        "            x = x.to(device=device, dtype=torch.long)  # move to  GPU\n",
        "            \n",
        "            if RNN:\n",
        "                #Must zero all of the accumalated hidden state for the RNN\n",
        "                model.hidden = model.init_hidden(batch.batch_size)\n",
        "   \n",
        "            logits = model(x, ids = batch.id)\n",
        "            if task_header == 'subtask_c':\n",
        "                pred_prob = F.softmax(logits, dim=1)\n",
        "                pred_1 = torch.argmax(pred_prob, dim=1).view(-1, 1)\n",
        "            else:\n",
        "                pred_prob = torch.sigmoid(logits)\n",
        "                print(pred_prob.shape)\n",
        "                pred_1 = (pred_prob > 0.5).type(torch.long)\n",
        "                print(pred_1.shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9v2KyvFUQlXi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "9c221699-dc12-42df-95dd-130a5dd1a1aa"
      },
      "cell_type": "code",
      "source": [
        "#create test loader:\n",
        "One_drive = \"\"\"/content/drive/My Drive/colab_data/\"\"\"\n",
        "test_fp_A = One_drive + \"testset-taska.tsv\"\n",
        "test_fp_B = One_drive + \"testset-taskb.tsv\"\n",
        "test_fp_C = One_drive + \"test_set_taskc.tsv\"\n",
        "\n",
        "\n",
        "data_fields = [('id', ID), \n",
        "               ('tweet', TEXT)]\n",
        "\n",
        "test_B = data.TabularDataset(test_fp_B, format='TSV', fields = \n",
        "                            data_fields, skip_header=True, filter_pred=None)\n",
        "#Create iterators\n",
        "test_B_iterator, = data.BucketIterator.splits((test_B,),\n",
        "                        batch_sizes=(len(test_B),))\n",
        "print(len(test_B))\n",
        "output_prediction_csv(\"predictions_B.csv\", model_B, test_B_iterator, \"subtask_b\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "240\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-39f4b2dd50be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                         batch_sizes=(len(test_B),))\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0moutput_prediction_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predictions_B.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_B_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"subtask_b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-39-c7b5a718b8eb>\u001b[0m in \u001b[0;36moutput_prediction_csv\u001b[0;34m(fp, model, test_loader, task_header, RNN)\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtask_header\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'subtask_c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mpred_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-64704ddab435>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, ids)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#then add sentiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-b6005adaccfe>\u001b[0m in \u001b[0;36madd_sentiment\u001b[0;34m(h, ids)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubjectivities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;31m#(batch size, out channels + 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 1. Got 0 and 240 in dimension 0 at /pytorch/aten/src/THC/generic/THCTensorMath.cu:83"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "AX9baJC4DQjm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cFuRQRbbEm8B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}